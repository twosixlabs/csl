{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_immediate_sensitivity(model, criterion, inputs, labels, epoch):\n",
    "    inp = Variable(inputs, requires_grad=True)\n",
    "    \n",
    "    outputs = model.forward(inp)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # (1) first-order gradient (wrt parameters)\n",
    "    first_order_grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    \n",
    "    # (2) L2 norm of the gradient from (1)\n",
    "    grad_l2_norm = torch.norm(torch.cat([x.view(-1) for x in first_order_grads]), p = 2)\n",
    "    \n",
    "    # (3) Gradient (wrt inputs) of the L2 norm of the gradient from (2)\n",
    "    sensitivity_vec = torch.autograd.grad(grad_l2_norm, inp, retain_graph=True)[0]\n",
    "    \n",
    "    # (4) L2 norm of (3) - \"immediate sensitivity\"\n",
    "    s = [torch.norm(v, p=2).numpy().item() for v in sensitivity_vec]\n",
    "    \n",
    "    '''\n",
    "    if epoch > 5:\n",
    "        print(f\"inputs: \",inp)\n",
    "        print(f\"outputs: \", outputs)\n",
    "        print(f\"loss: \", loss)\n",
    "        print(f\"first_order_grads: \", first_order_grads)\n",
    "        print(f\"grad_l2_norm:: \", grad_l2_norm)\n",
    "        print(f\"sensitivity_vec: \", sensitivity_vec)\n",
    "        print(f\"sensitivies: \", s)\n",
    "    '''\n",
    "\n",
    "    loss.backward()\n",
    "    return loss, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_accuracy(model, test_loader):\n",
    "    correct = 0\n",
    "    num_data = 0\n",
    "\n",
    "    #grab a batch from the test loader\n",
    "    for examples, labels in test_loader:\n",
    "        outputs = model.forward(examples)\n",
    "        \n",
    "        #for each output in the batch, check if the label is correct\n",
    "        for i, output in enumerate(outputs):\n",
    "            num_data += 1\n",
    "            \n",
    "            max_i = np.argmax(output.detach().numpy())\n",
    "            if max_i == labels[i]:\n",
    "                correct += 1\n",
    "\n",
    "    acc = float(correct)/num_data\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "print(len(mnist_trainset))\n",
    "print(len(mnist_testset))\n",
    "train_loader = DataLoader(mnist_trainset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_testset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# next(iter(train_loader))[0].shape --> torch.Size([16, 1, 28, 28])\n",
    "# This means we have 16 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mnist_Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=(5,5))\n",
    "        self.conv2 = nn.Conv2d(28, 32, kernel_size=(5,5))\n",
    "        self.fc1 = nn.Linear(32*20*20, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        # print(x.size()) --> torch.Size([16, 32, 20, 20])\n",
    "        x = x.view(-1, 32*20*20)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return torch.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mnist():\n",
    "    # reset the model\n",
    "    model = mnist_Classifier()\n",
    "    model_criterion = nn.CrossEntropyLoss()\n",
    "    model_optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "    # number of epochs and iterations\n",
    "    epochs = 30\n",
    "    iters = epochs * BATCH_SIZE\n",
    "    \n",
    "    # plotting criteria\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Start of epoch %d' % (epoch,))\n",
    "        all_sensitivities = []\n",
    "        sigmas = []\n",
    "\n",
    "        for batch_id, (x_batch_train, y_batch_train) in enumerate(train_loader):\n",
    "            \n",
    "            #zero out the gradients from the previous iteration\n",
    "            model_optimizer.zero_grad()\n",
    "            \n",
    "            #compute loss\n",
    "            outputs = model.forward(x_batch_train)\n",
    "            loss = model_criterion(outputs, y_batch_train)\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            \n",
    "            # perform the backpropagation\n",
    "            model_optimizer.step()\n",
    "\n",
    "        print(\"Average train loss:\", np.mean(train_losses))\n",
    "        test_accs.append(mnist_accuracy(model, test_loader))\n",
    "        print(\"Accuracy:\", test_accs[-1])\n",
    "    return mnist_accuracy(model, test_loader), (train_losses, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-62921ad0cd7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-166-e722ff138ea2>\u001b[0m in \u001b[0;36mrun_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# perform the backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average train loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = run_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Models at the Same Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_loader_generator(dataloader):\n",
    "    '''\n",
    "    Generates a function that infinitely samples a dataloader\n",
    "    '''\n",
    "    while True:\n",
    "        for x, y in dataloader:\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_accuracy_2(model, inf_gen, test_steps):\n",
    "    correct = 0\n",
    "    num_data = 0\n",
    "\n",
    "    #grab a batch from the test loader\n",
    "    for j in range(test_steps):\n",
    "        examples, labels = next(inf_gen)\n",
    "        outputs = model.forward(examples)\n",
    "\n",
    "        #for each output in the batch, check if the label is correct\n",
    "        for i, output in enumerate(outputs):\n",
    "            num_data += 1\n",
    "\n",
    "            max_i = np.argmax(output.detach().numpy())\n",
    "            if max_i == labels[i]:\n",
    "                correct += 1\n",
    "\n",
    "    acc = float(correct)/num_data\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_1 = 16\n",
    "BATCH_SIZE_2 = 16\n",
    "\n",
    "mnist_trainset_1 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_trainset_2 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_1 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_2 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "print(len(mnist_trainset_1))\n",
    "print(len(mnist_testset_1))\n",
    "print(len(mnist_trainset_2))\n",
    "print(len(mnist_testset_2))\n",
    "\n",
    "train_loader_1 = DataLoader(mnist_trainset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "test_loader_1 = DataLoader(mnist_testset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "train_loader_2 = DataLoader(mnist_trainset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "test_loader_2 = DataLoader(mnist_testset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "\n",
    "inf_train_1 = inf_loader_generator(train_loader_1)\n",
    "inf_test_1 = inf_loader_generator(test_loader_1)\n",
    "inf_train_2 = inf_loader_generator(train_loader_2)\n",
    "inf_test_2 = inf_loader_generator(test_loader_2)\n",
    "\n",
    "iters_per_epoch_train_1 = len(mnist_trainset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_test_1 = len(mnist_testset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_train_2 = len(mnist_trainset_2)//BATCH_SIZE_2\n",
    "iters_per_epoch_test_2 = len(mnist_testset_2)//BATCH_SIZE_2\n",
    "\n",
    "# next(iter(train_loader))[0].shape --> torch.Size([16, 1, 28, 28])\n",
    "# This means we have 16 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_gen_1 = inf_loader_generator(train_loader_1)\\nbatch_ids = []\\nfor i in range(10000):\\n    batch_id, (x_batch_train, y_batch_train) = next(train_gen_1)\\n    batch_ids.append(batch_id)\\nprint(batch_ids)\\n'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing out the infinite loader - the batch_ids reset when the dataloader reshuffles. \n",
    "'''\n",
    "train_gen_1 = inf_loader_generator(train_loader_1)\n",
    "batch_ids = []\n",
    "for i in range(10000):\n",
    "    batch_id, (x_batch_train, y_batch_train) = next(train_gen_1)\n",
    "    batch_ids.append(batch_id)\n",
    "print(batch_ids)\n",
    "'''\n",
    "#next(iter(train_loader_1))\n",
    "#next(iter(test_loader_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_two_mnist():\n",
    "    # reset the model 1\n",
    "    model_1 = mnist_Classifier()\n",
    "    model_criterion_1 = nn.CrossEntropyLoss()\n",
    "    model_optimizer_1 = optim.Adam(model_1.parameters(),lr=0.001)\n",
    "    \n",
    "    # reset the model 2\n",
    "    model_2 = mnist_Classifier()\n",
    "    model_criterion_2 = nn.CrossEntropyLoss()\n",
    "    model_optimizer_2 = optim.Adam(model_2.parameters(),lr=0.001)\n",
    "\n",
    "    # plotting criteria\n",
    "    train_losses_1 = []\n",
    "    test_accs_1 = []\n",
    "    train_losses_2 = []\n",
    "    test_accs_2 = []\n",
    "\n",
    "    training = True\n",
    "    model_steps_1 = 0\n",
    "    model_steps_2 = 0\n",
    "    train_steps_per_epoch_1 = 2000\n",
    "    test_steps_per_epoch_1 = 300\n",
    "    train_steps_per_epoch_2 = 2000\n",
    "    test_steps_per_epoch_2 = 300\n",
    "    print(\"Model 1: Start of Epoch %d\" % (model_steps_1/train_steps_per_epoch_1))\n",
    "    print(\"Model 2: Start of Epoch %d\" % (model_steps_2/train_steps_per_epoch_2))\n",
    "    while training:\n",
    "            \n",
    "        # model 1\n",
    "        x_batch_train_1, y_batch_train_1 = next(inf_train_1)\n",
    "        #zero out the gradients from the previous iteration\n",
    "        model_optimizer_1.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        outputs_1 = model_1.forward(x_batch_train_1)\n",
    "        loss_1 = model_criterion_1(outputs_1, y_batch_train_1)\n",
    "        loss_1.backward()\n",
    "        train_losses_1.append(loss_1.item())\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        model_optimizer_1.step()\n",
    "        model_steps_1 += 1\n",
    "        \n",
    "        if model_steps_1 % train_steps_per_epoch_1 == 0:\n",
    "            print(\"Model 1 Average Train Loss:\", np.mean(train_losses_1))\n",
    "            test_accs_1.append(mnist_accuracy_2(model_1, inf_test_1, test_steps_per_epoch_1))\n",
    "            print(\"Accuracy:\", test_accs_1[-1])\n",
    "            print(\"Model 1: Start of Epoch %d\" % (model_steps_1/train_steps_per_epoch_1))\n",
    "        \n",
    "        # model 2\n",
    "        x_batch_train_2, y_batch_train_2 = next(inf_train_2)\n",
    "        #zero out the gradients from the previous iteration\n",
    "        model_optimizer_2.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        outputs_2 = model_2.forward(x_batch_train_2)\n",
    "        loss_2 = model_criterion_2(outputs_2, y_batch_train_2)\n",
    "        loss_2.backward()\n",
    "        train_losses_2.append(loss_2.item())\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        model_optimizer_2.step()\n",
    "        model_steps_2 += 1\n",
    "        \n",
    "        if model_steps_2 % train_steps_per_epoch_2 == 0:\n",
    "            print(\"Model 2 Average Train Loss:\", np.mean(train_losses_2))\n",
    "            test_accs_2.append(mnist_accuracy_2(model_2, inf_test_2, test_steps_per_epoch_2))\n",
    "            print(\"Accuracy:\", test_accs_2[-1])\n",
    "            print(\"Model 2: Start of Epoch %d\" % (model_steps_2/train_steps_per_epoch_2))\n",
    "        \n",
    "        if model_steps_1 > (train_steps_per_epoch_1*50) + 1:\n",
    "            break\n",
    "        \n",
    "    return [mnist_accuracy_2(model_1, inf_test_1, test_steps_per_epoch_1*6), (train_losses_1, test_accs_1),\n",
    "            mnist_accuracy_2(model_1, inf_test_2, test_steps_per_epoch_2*6), (train_losses_2, test_accs_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Start of Epoch 0\n",
      "Model 2: Start of Epoch 0\n",
      "Model 1 Average Train Loss: 1.8432904768586158\n",
      "Accuracy: 0.6695833333333333\n",
      "Model 1: Start of Epoch 1\n",
      "Model 2 Average Train Loss: 1.9315890341997146\n",
      "Accuracy: 0.6008333333333333\n",
      "Model 2: Start of Epoch 1\n",
      "Model 1 Average Train Loss: 1.8133660935759544\n",
      "Accuracy: 0.684375\n",
      "Model 1: Start of Epoch 2\n",
      "Model 2 Average Train Loss: 1.9156818189024925\n",
      "Accuracy: 0.589375\n",
      "Model 2: Start of Epoch 2\n",
      "Model 1 Average Train Loss: 1.7992913405696551\n",
      "Accuracy: 0.7572916666666667\n",
      "Model 1: Start of Epoch 3\n",
      "Model 2 Average Train Loss: 1.9085230160752933\n",
      "Accuracy: 0.5945833333333334\n",
      "Model 2: Start of Epoch 3\n",
      "Model 1 Average Train Loss: 1.7700483030229808\n",
      "Accuracy: 0.7816666666666666\n",
      "Model 1: Start of Epoch 4\n",
      "Model 2 Average Train Loss: 1.905207740932703\n",
      "Accuracy: 0.600625\n",
      "Model 2: Start of Epoch 4\n",
      "Model 1 Average Train Loss: 1.7520967221140862\n",
      "Accuracy: 0.7829166666666667\n",
      "Model 1: Start of Epoch 5\n",
      "Model 2 Average Train Loss: 1.9025734634399414\n",
      "Accuracy: 0.5860416666666667\n",
      "Model 2: Start of Epoch 5\n",
      "Model 1 Average Train Loss: 1.7382667655746142\n",
      "Accuracy: 0.7858333333333334\n",
      "Model 1: Start of Epoch 6\n",
      "Model 2 Average Train Loss: 1.9009676310420036\n",
      "Accuracy: 0.595625\n",
      "Model 2: Start of Epoch 6\n",
      "Model 1 Average Train Loss: 1.7170934731704848\n",
      "Accuracy: 0.9635416666666666\n",
      "Model 1: Start of Epoch 7\n",
      "Model 2 Average Train Loss: 1.8998208712935447\n",
      "Accuracy: 0.5947916666666667\n",
      "Model 2: Start of Epoch 7\n",
      "Model 1 Average Train Loss: 1.6910692160576581\n",
      "Accuracy: 0.9627083333333334\n",
      "Model 1: Start of Epoch 8\n",
      "Model 2 Average Train Loss: 1.898857570067048\n",
      "Accuracy: 0.5979166666666667\n",
      "Model 2: Start of Epoch 8\n",
      "Model 1 Average Train Loss: 1.6694544690052668\n",
      "Accuracy: 0.96875\n",
      "Model 1: Start of Epoch 9\n",
      "Model 2 Average Train Loss: 1.8977586660385133\n",
      "Accuracy: 0.5902083333333333\n",
      "Model 2: Start of Epoch 9\n",
      "Model 1 Average Train Loss: 1.6517986259579658\n",
      "Accuracy: 0.9725\n",
      "Model 1: Start of Epoch 10\n",
      "Model 2 Average Train Loss: 1.8973365762054921\n",
      "Accuracy: 0.5895833333333333\n",
      "Model 2: Start of Epoch 10\n"
     ]
    }
   ],
   "source": [
    "two_mnist_results = run_two_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing Gradients from One Model To the Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_1 = 16\n",
    "BATCH_SIZE_2 = 16\n",
    "\n",
    "mnist_trainset_1 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_trainset_2 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_1 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_2 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "print(len(mnist_trainset_1))\n",
    "print(len(mnist_testset_1))\n",
    "print(len(mnist_trainset_2))\n",
    "print(len(mnist_testset_2))\n",
    "\n",
    "train_loader_1 = DataLoader(mnist_trainset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "test_loader_1 = DataLoader(mnist_testset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "train_loader_2 = DataLoader(mnist_trainset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "test_loader_2 = DataLoader(mnist_testset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "\n",
    "inf_train_1 = inf_loader_generator(train_loader_1)\n",
    "inf_test_1 = inf_loader_generator(test_loader_1)\n",
    "inf_train_2 = inf_loader_generator(train_loader_2)\n",
    "inf_test_2 = inf_loader_generator(test_loader_2)\n",
    "\n",
    "iters_per_epoch_train_1 = len(mnist_trainset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_test_1 = len(mnist_testset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_train_2 = len(mnist_trainset_2)//BATCH_SIZE_2\n",
    "iters_per_epoch_test_2 = len(mnist_testset_2)//BATCH_SIZE_2\n",
    "\n",
    "# next(iter(train_loader))[0].shape --> torch.Size([16, 1, 28, 28])\n",
    "# This means we have 16 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share_gradient_mnist():\n",
    "    # reset the model 1\n",
    "    model_1 = mnist_Classifier()\n",
    "    model_criterion_1 = nn.CrossEntropyLoss()\n",
    "    model_optimizer_1 = optim.Adam(model_1.parameters(),lr=0.001)\n",
    "    \n",
    "    # reset the model 2\n",
    "    model_2 = mnist_Classifier()\n",
    "    model_criterion_2 = nn.CrossEntropyLoss()\n",
    "    model_optimizer_2 = optim.Adam(model_2.parameters(),lr=0.001)\n",
    "\n",
    "    # plotting criteria\n",
    "    train_losses_1 = []\n",
    "    test_accs_1 = []\n",
    "    train_losses_2 = []\n",
    "    test_accs_2 = []\n",
    "\n",
    "    training = True\n",
    "    model_steps_1 = 0\n",
    "    model_steps_2 = 0\n",
    "    train_steps_per_epoch_1 = 2000\n",
    "    test_steps_per_epoch_1 = 300\n",
    "    train_steps_per_epoch_2 = 2000\n",
    "    test_steps_per_epoch_2 = 300\n",
    "    print(\"Model 1: Start of Epoch %d\" % (model_steps_1/train_steps_per_epoch_1))\n",
    "    print(\"Model 2: Start of Epoch %d\" % (model_steps_2/train_steps_per_epoch_2))\n",
    "    while training:\n",
    "            \n",
    "        # model 1\n",
    "        x_batch_train_1, y_batch_train_1 = next(inf_train_1)\n",
    "        #zero out the gradients from the previous iteration\n",
    "        model_optimizer_1.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        outputs_1 = model_1.forward(x_batch_train_1)\n",
    "        loss_1 = model_criterion_1(outputs_1, y_batch_train_1)\n",
    "        loss_1.backward()\n",
    "        train_losses_1.append(loss_1.item())\n",
    "        \n",
    "        model_1_params = list(model_1.parameters())\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        model_optimizer_1.step()\n",
    "        model_steps_1 += 1\n",
    "        \n",
    "        if model_steps_1 % train_steps_per_epoch_1 == 0:\n",
    "            print(\"Model 1 Average Train Loss:\", np.mean(train_losses_1))\n",
    "            test_accs_1.append(mnist_accuracy_2(model_1, inf_test_1, test_steps_per_epoch_1))\n",
    "            print(\"Accuracy:\", test_accs_1[-1])\n",
    "            print(\"Model 1: Start of Epoch %d\" % (model_steps_1/train_steps_per_epoch_1))\n",
    "        \n",
    "        # model 2\n",
    "        x_batch_train_2, y_batch_train_2 = next(inf_train_2)\n",
    "        #zero out the gradients from the previous iteration\n",
    "        model_optimizer_2.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        outputs_2 = model_2.forward(x_batch_train_2)\n",
    "        loss_2 = model_criterion_2(outputs_2, y_batch_train_2)\n",
    "        loss_2.backward()\n",
    "        train_losses_2.append(loss_2.item())\n",
    "        \n",
    "        # Add model 1's gradient to model 2\n",
    "        model_2_params = list(model_2.parameters())\n",
    "        for p,q in zip(model_1_params, model_2_params):\n",
    "            q.grad += p.grad\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        model_optimizer_2.step()\n",
    "        model_steps_2 += 1\n",
    "        \n",
    "        if model_steps_2 % train_steps_per_epoch_2 == 0:\n",
    "            print(\"Model 2 Average Train Loss:\", np.mean(train_losses_2))\n",
    "            test_accs_2.append(mnist_accuracy_2(model_2, inf_test_2, test_steps_per_epoch_2))\n",
    "            print(\"Accuracy:\", test_accs_2[-1])\n",
    "            print(\"Model 2: Start of Epoch %d\" % (model_steps_2/train_steps_per_epoch_2))\n",
    "            \n",
    "        if model_steps_1 > (train_steps_per_epoch_1*50) + 1:\n",
    "            break\n",
    "        \n",
    "    return [mnist_accuracy_2(model_1, inf_test_1, test_steps_per_epoch_1*6), (train_losses_1, test_accs_1),\n",
    "            mnist_accuracy_2(model_2, inf_test_2, test_steps_per_epoch_2*6), (train_losses_2, test_accs_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "share_gradient_results = share_gradient_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "-----------\n",
      "28\n",
      "28\n",
      "32\n",
      "32\n",
      "16\n",
      "16\n",
      "10\n",
      "10\n",
      "-----------\n",
      "28\n",
      "1\n",
      "5\n",
      "tensor([[[ 0.1573,  0.0239, -0.1093,  0.1152, -0.0214],\n",
      "         [-0.0398,  0.0588,  0.0438,  0.0889, -0.1593],\n",
      "         [ 0.0337, -0.0114, -0.0363,  0.0549,  0.0545],\n",
      "         [-0.0629, -0.0394, -0.0910, -0.0025, -0.0461],\n",
      "         [ 0.0345, -0.1686, -0.0637, -0.1188, -0.0762]]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[ 0.1573,  0.0239, -0.1093,  0.1152, -0.0214],\n",
      "        [-0.0398,  0.0588,  0.0438,  0.0889, -0.1593],\n",
      "        [ 0.0337, -0.0114, -0.0363,  0.0549,  0.0545],\n",
      "        [-0.0629, -0.0394, -0.0910, -0.0025, -0.0461],\n",
      "        [ 0.0345, -0.1686, -0.0637, -0.1188, -0.0762]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "Parameter containing:\n",
      "tensor([ 0.1244, -0.0783,  0.1221,  0.1507, -0.0294, -0.0566,  0.1969,  0.1550,\n",
      "         0.0330, -0.1946,  0.0554, -0.0257, -0.1056, -0.1283, -0.1869,  0.1958,\n",
      "         0.1546, -0.1363,  0.0688, -0.0438, -0.0415, -0.1582, -0.1455,  0.1552,\n",
      "         0.0025,  0.1939,  0.0101, -0.0620], requires_grad=True)\n",
      "------------\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(example_params))\n",
    "print(\"-----------\")\n",
    "\n",
    "'''\n",
    "class mnist_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mnist_Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=(5,5))\n",
    "        self.conv2 = nn.Conv2d(28, 32, kernel_size=(5,5))\n",
    "        self.fc1 = nn.Linear(32*20*20, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        # print(x.size()) --> torch.Size([16, 32, 20, 20])\n",
    "        x = x.view(-1, 32*20*20)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return torch.softmax(x,dim=1)\n",
    "'''\n",
    "\n",
    "for i in range(len(example_params)):\n",
    "    print(len(example_params[i]))\n",
    "          \n",
    "print('-----------')\n",
    "    \n",
    "print(len(example_params[0]))\n",
    "print(len(example_params[0][0]))\n",
    "print(len(example_params[0][0][0]))\n",
    "\n",
    "#print(example_params[0])\n",
    "print(example_params[0][0])\n",
    "print(example_params[0][0][0])\n",
    "print(example_params[1])\n",
    "#print(example_params[1][0])\n",
    "#print(example_params[1][0][0])\n",
    "\n",
    "print(\"------------\")\n",
    "print(len(example_params[0].grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Both models sharing with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_1 = 16\n",
    "BATCH_SIZE_2 = 16\n",
    "\n",
    "mnist_trainset_1 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_trainset_2 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_1 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_2 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "print(len(mnist_trainset_1))\n",
    "print(len(mnist_testset_1))\n",
    "print(len(mnist_trainset_2))\n",
    "print(len(mnist_testset_2))\n",
    "\n",
    "train_loader_1 = DataLoader(mnist_trainset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "test_loader_1 = DataLoader(mnist_testset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "train_loader_2 = DataLoader(mnist_trainset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "test_loader_2 = DataLoader(mnist_testset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "\n",
    "inf_train_1 = inf_loader_generator(train_loader_1)\n",
    "inf_test_1 = inf_loader_generator(test_loader_1)\n",
    "inf_train_2 = inf_loader_generator(train_loader_2)\n",
    "inf_test_2 = inf_loader_generator(test_loader_2)\n",
    "\n",
    "iters_per_epoch_train_1 = len(mnist_trainset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_test_1 = len(mnist_testset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_train_2 = len(mnist_trainset_2)//BATCH_SIZE_2\n",
    "iters_per_epoch_test_2 = len(mnist_testset_2)//BATCH_SIZE_2\n",
    "\n",
    "# next(iter(train_loader))[0].shape --> torch.Size([16, 1, 28, 28])\n",
    "# This means we have 16 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share_both_mnist():\n",
    "    # reset the model 1\n",
    "    model_1 = mnist_Classifier()\n",
    "    model_criterion_1 = nn.CrossEntropyLoss()\n",
    "    model_optimizer_1 = optim.Adam(model_1.parameters(),lr=0.001)\n",
    "    \n",
    "    # reset the model 2\n",
    "    model_2 = mnist_Classifier()\n",
    "    model_criterion_2 = nn.CrossEntropyLoss()\n",
    "    model_optimizer_2 = optim.Adam(model_2.parameters(),lr=0.001)\n",
    "\n",
    "    # plotting criteria\n",
    "    train_losses_1 = []\n",
    "    test_accs_1 = []\n",
    "    train_losses_2 = []\n",
    "    test_accs_2 = []\n",
    "\n",
    "    training = True\n",
    "    model_steps_1 = 0\n",
    "    model_steps_2 = 0\n",
    "    train_steps_per_epoch_1 = 2000\n",
    "    test_steps_per_epoch_1 = 300\n",
    "    train_steps_per_epoch_2 = 2000\n",
    "    test_steps_per_epoch_2 = 300\n",
    "    print(\"Model 1: Start of Epoch %d\" % (model_steps_1/train_steps_per_epoch_1))\n",
    "    print(\"Model 2: Start of Epoch %d\" % (model_steps_2/train_steps_per_epoch_2))\n",
    "    while training:\n",
    "            \n",
    "        # model 1\n",
    "        x_batch_train_1, y_batch_train_1 = next(inf_train_1)\n",
    "        #zero out the gradients from the previous iteration\n",
    "        model_optimizer_1.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        outputs_1 = model_1.forward(x_batch_train_1)\n",
    "        loss_1 = model_criterion_1(outputs_1, y_batch_train_1)\n",
    "        loss_1.backward()\n",
    "        train_losses_1.append(loss_1.item())\n",
    "        \n",
    "        model_1_params = list(model_1.parameters())\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        \n",
    "        if model_steps_1 % train_steps_per_epoch_1 == 0:\n",
    "            print(\"Model 1 Average Train Loss:\", np.mean(train_losses_1))\n",
    "            test_accs_1.append(mnist_accuracy_2(model_1, inf_test_1, test_steps_per_epoch_1))\n",
    "            print(\"Accuracy:\", test_accs_1[-1])\n",
    "            print(\"Model 1: Start of Epoch %d\" % (model_steps_1/train_steps_per_epoch_1))\n",
    "        \n",
    "        # model 2\n",
    "        x_batch_train_2, y_batch_train_2 = next(inf_train_2)\n",
    "        #zero out the gradients from the previous iteration\n",
    "        model_optimizer_2.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        outputs_2 = model_2.forward(x_batch_train_2)\n",
    "        loss_2 = model_criterion_2(outputs_2, y_batch_train_2)\n",
    "        loss_2.backward()\n",
    "        train_losses_2.append(loss_2.item())\n",
    "        \n",
    "        # Add model 1's gradient to model 2\n",
    "        model_2_params = list(model_2.parameters())\n",
    "        for p,q in zip(model_1_params, model_2_params):\n",
    "            q.grad += p.grad\n",
    "        \n",
    "        #perform the backpropagation\n",
    "        # NEED TO MAKE A DEEP COPY HERE?~?\n",
    "        model_optimizer_1.step()\n",
    "        model_steps_1 += 1\n",
    "        model_optimizer_2.step()\n",
    "        model_steps_2 += 1\n",
    "        \n",
    "        if model_steps_2 % train_steps_per_epoch_2 == 0:\n",
    "            print(\"Model 2 Average Train Loss:\", np.mean(train_losses_2))\n",
    "            test_accs_2.append(mnist_accuracy_2(model_2, inf_test_2, test_steps_per_epoch_2))\n",
    "            print(\"Accuracy:\", test_accs_2[-1])\n",
    "            print(\"Model 2: Start of Epoch %d\" % (model_steps_2/train_steps_per_epoch_2))\n",
    "            \n",
    "        if model_steps_1 > (train_steps_per_epoch_1*20) + 1:\n",
    "            break\n",
    "        \n",
    "    return [mnist_accuracy_2(model_1, inf_test_1, test_steps_per_epoch_1*6), (train_losses_1, test_accs_1),\n",
    "            mnist_accuracy_2(model_2, inf_test_2, test_steps_per_epoch_2*6), (train_losses_2, test_accs_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_both_results = share_both_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No sharing vs 1-Sharing vs 2-Sharing Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.304023265838623,\n",
       " 2.2921273708343506,\n",
       " 2.3246054649353027,\n",
       " 2.307582378387451,\n",
       " 2.3011348247528076,\n",
       " 2.310779333114624,\n",
       " 2.29919171333313,\n",
       " 2.3031556606292725,\n",
       " 2.2981133460998535,\n",
       " 2.3027493953704834,\n",
       " 2.3015964031219482,\n",
       " 2.303175449371338,\n",
       " 2.298534870147705,\n",
       " 2.294466018676758,\n",
       " 2.296642780303955,\n",
       " 2.3009636402130127,\n",
       " 2.292402505874634,\n",
       " 2.301198959350586,\n",
       " 2.2488698959350586,\n",
       " 2.273270606994629,\n",
       " 2.2430574893951416,\n",
       " 2.309187412261963,\n",
       " 2.3498151302337646,\n",
       " 2.3289361000061035,\n",
       " 2.2262306213378906,\n",
       " 2.2701902389526367,\n",
       " 2.2388293743133545,\n",
       " 2.199167490005493,\n",
       " 2.249704360961914,\n",
       " 2.238769769668579,\n",
       " 2.218247652053833,\n",
       " 2.2530252933502197,\n",
       " 2.276012420654297,\n",
       " 2.285748243331909,\n",
       " 2.1479697227478027,\n",
       " 2.1857051849365234,\n",
       " 2.1023106575012207,\n",
       " 2.128092050552368,\n",
       " 2.264326810836792,\n",
       " 2.2410314083099365,\n",
       " 2.2151641845703125,\n",
       " 2.2920875549316406,\n",
       " 2.214625835418701,\n",
       " 2.22855281829834,\n",
       " 2.1520938873291016,\n",
       " 2.134685516357422,\n",
       " 2.246134042739868,\n",
       " 2.177459955215454,\n",
       " 2.1898415088653564,\n",
       " 2.0606744289398193,\n",
       " 2.0412580966949463,\n",
       " 2.1442925930023193,\n",
       " 2.237525463104248,\n",
       " 1.9445335865020752,\n",
       " 2.138751745223999,\n",
       " 2.2832000255584717,\n",
       " 2.1092417240142822,\n",
       " 2.2817070484161377,\n",
       " 1.9788715839385986,\n",
       " 2.0434539318084717,\n",
       " 2.0665624141693115,\n",
       " 2.026714563369751,\n",
       " 2.0805060863494873,\n",
       " 2.1811978816986084,\n",
       " 2.0621836185455322,\n",
       " 2.2480204105377197,\n",
       " 2.076443910598755,\n",
       " 2.213104009628296,\n",
       " 2.2028145790100098,\n",
       " 2.1799919605255127,\n",
       " 2.135887622833252,\n",
       " 2.117910623550415,\n",
       " 2.252277135848999,\n",
       " 2.115389347076416,\n",
       " 2.084200859069824,\n",
       " 2.052764415740967,\n",
       " 2.017744541168213,\n",
       " 1.9385569095611572,\n",
       " 2.0223402976989746,\n",
       " 2.0866334438323975,\n",
       " 2.093947172164917,\n",
       " 2.0956168174743652,\n",
       " 2.1663317680358887,\n",
       " 1.9263383150100708,\n",
       " 2.166358470916748,\n",
       " 1.8466469049453735,\n",
       " 2.0091118812561035,\n",
       " 1.8444647789001465,\n",
       " 1.9771604537963867,\n",
       " 1.9837092161178589,\n",
       " 1.96170973777771,\n",
       " 1.72145414352417,\n",
       " 2.0727407932281494,\n",
       " 1.78030264377594,\n",
       " 2.0925941467285156,\n",
       " 2.148613691329956,\n",
       " 1.9662748575210571,\n",
       " 2.0485270023345947,\n",
       " 2.0239193439483643,\n",
       " 2.141127109527588,\n",
       " 1.914821982383728,\n",
       " 1.803045392036438,\n",
       " 2.1267709732055664,\n",
       " 1.9316781759262085,\n",
       " 2.027350664138794,\n",
       " 2.1199581623077393,\n",
       " 2.0290772914886475,\n",
       " 2.009894609451294,\n",
       " 1.8788084983825684,\n",
       " 2.011996269226074,\n",
       " 1.9391834735870361,\n",
       " 1.9074454307556152,\n",
       " 1.8150866031646729,\n",
       " 1.9483139514923096,\n",
       " 2.0040314197540283,\n",
       " 2.0168232917785645,\n",
       " 1.8977882862091064,\n",
       " 2.0895540714263916,\n",
       " 1.9466698169708252,\n",
       " 1.771715521812439,\n",
       " 2.0183663368225098,\n",
       " 2.023432731628418,\n",
       " 2.198845863342285,\n",
       " 2.0796244144439697,\n",
       " 1.8931758403778076,\n",
       " 2.079622983932495,\n",
       " 2.045780658721924,\n",
       " 1.8946584463119507,\n",
       " 2.01344633102417,\n",
       " 2.0358221530914307,\n",
       " 1.8967227935791016,\n",
       " 2.003598690032959,\n",
       " 1.7274383306503296,\n",
       " 1.8466088771820068,\n",
       " 2.0159718990325928,\n",
       " 2.147311210632324,\n",
       " 2.0540363788604736,\n",
       " 1.7976136207580566,\n",
       " 1.961319923400879,\n",
       " 1.9753354787826538,\n",
       " 2.0198493003845215,\n",
       " 1.8977810144424438,\n",
       " 1.8880845308303833,\n",
       " 2.059061050415039,\n",
       " 1.935206651687622,\n",
       " 2.2096107006073,\n",
       " 1.922867774963379,\n",
       " 2.266331434249878,\n",
       " 1.745910882949829,\n",
       " 1.8788024187088013,\n",
       " 2.0432276725769043,\n",
       " 1.8817687034606934,\n",
       " 1.7819445133209229,\n",
       " 2.0157506465911865,\n",
       " 2.01157808303833,\n",
       " 2.0220985412597656,\n",
       " 1.9058936834335327,\n",
       " 1.8694487810134888,\n",
       " 2.079423427581787,\n",
       " 1.939332127571106,\n",
       " 2.0943825244903564,\n",
       " 2.0262765884399414,\n",
       " 2.1363742351531982,\n",
       " 1.8695002794265747,\n",
       " 1.8359578847885132,\n",
       " 2.074141025543213,\n",
       " 1.8272145986557007,\n",
       " 2.1823015213012695,\n",
       " 2.2659716606140137,\n",
       " 2.064917802810669,\n",
       " 2.2057788372039795,\n",
       " 2.157768964767456,\n",
       " 2.108684778213501,\n",
       " 1.8348419666290283,\n",
       " 2.011463165283203,\n",
       " 2.0430891513824463,\n",
       " 2.2268528938293457,\n",
       " 1.8088542222976685,\n",
       " 1.8476102352142334,\n",
       " 2.057971477508545,\n",
       " 1.835123896598816,\n",
       " 2.0230486392974854,\n",
       " 1.9609860181808472,\n",
       " 1.9513336420059204,\n",
       " 1.9441964626312256,\n",
       " 2.096027135848999,\n",
       " 1.8940939903259277,\n",
       " 1.741072177886963,\n",
       " 1.8289457559585571,\n",
       " 1.93338942527771,\n",
       " 1.8572356700897217,\n",
       " 1.7179216146469116,\n",
       " 2.111319065093994,\n",
       " 2.0159802436828613,\n",
       " 1.9756487607955933,\n",
       " 1.9496920108795166,\n",
       " 1.9471925497055054,\n",
       " 2.169938087463379,\n",
       " 2.141436815261841,\n",
       " 1.7186039686203003,\n",
       " 1.8837963342666626,\n",
       " 2.142073631286621,\n",
       " 1.8921319246292114,\n",
       " 1.8484543561935425,\n",
       " 1.9035494327545166,\n",
       " 1.957188606262207,\n",
       " 1.9034607410430908,\n",
       " 2.083817481994629,\n",
       " 1.8567183017730713,\n",
       " 1.8281443119049072,\n",
       " 1.9317258596420288,\n",
       " 2.019101142883301,\n",
       " 2.0000877380371094,\n",
       " 2.081580400466919,\n",
       " 2.0211338996887207,\n",
       " 2.0102274417877197,\n",
       " 1.9570354223251343,\n",
       " 1.8947031497955322,\n",
       " 1.892716884613037,\n",
       " 1.80684494972229,\n",
       " 2.2499496936798096,\n",
       " 1.97088623046875,\n",
       " 2.133953094482422,\n",
       " 2.0228493213653564,\n",
       " 1.959259271621704,\n",
       " 1.9444643259048462,\n",
       " 2.0485613346099854,\n",
       " 2.005051612854004,\n",
       " 2.0173871517181396,\n",
       " 1.7504700422286987,\n",
       " 1.838011384010315,\n",
       " 1.9724394083023071,\n",
       " 2.0395901203155518,\n",
       " 2.0942418575286865,\n",
       " 1.7266416549682617,\n",
       " 2.129401683807373,\n",
       " 2.0461912155151367,\n",
       " 1.9143272638320923,\n",
       " 1.915727972984314,\n",
       " 1.7773776054382324,\n",
       " 1.899300217628479,\n",
       " 2.1196322441101074,\n",
       " 2.0256361961364746,\n",
       " 2.1875228881835938,\n",
       " 1.783510446548462,\n",
       " 2.052816390991211,\n",
       " 2.017496347427368,\n",
       " 1.7247477769851685,\n",
       " 1.8194384574890137,\n",
       " 1.897318720817566,\n",
       " 2.0200159549713135,\n",
       " 2.0745182037353516,\n",
       " 1.954721212387085,\n",
       " 1.983530879020691,\n",
       " 1.727613925933838,\n",
       " 2.1429295539855957,\n",
       " 2.079324960708618,\n",
       " 1.8885409832000732,\n",
       " 1.9542853832244873,\n",
       " 2.112858533859253,\n",
       " 2.0156497955322266,\n",
       " 2.0502169132232666,\n",
       " 1.9518992900848389,\n",
       " 1.9812017679214478,\n",
       " 1.9775431156158447,\n",
       " 1.9443708658218384,\n",
       " 2.1247997283935547,\n",
       " 1.9270098209381104,\n",
       " 2.1452789306640625,\n",
       " 2.0361595153808594,\n",
       " 2.340726375579834,\n",
       " 2.198941230773926,\n",
       " 2.0066287517547607,\n",
       " 1.7624943256378174,\n",
       " 2.103461980819702,\n",
       " 1.8069067001342773,\n",
       " 1.9276814460754395,\n",
       " 1.8329334259033203,\n",
       " 1.9004838466644287,\n",
       " 2.0022952556610107,\n",
       " 2.1083834171295166,\n",
       " 2.0232183933258057,\n",
       " 1.8753769397735596,\n",
       " 1.9681309461593628,\n",
       " 1.9150490760803223,\n",
       " 1.9648183584213257,\n",
       " 1.7808269262313843,\n",
       " 1.8068060874938965,\n",
       " 1.8366739749908447,\n",
       " 1.9127906560897827,\n",
       " 2.0771241188049316,\n",
       " 1.8821007013320923,\n",
       " 2.149199962615967,\n",
       " 1.8031456470489502,\n",
       " 1.5845229625701904,\n",
       " 1.9166234731674194,\n",
       " 2.173675537109375,\n",
       " 1.7859011888504028,\n",
       " 1.9084535837173462,\n",
       " 1.824084758758545,\n",
       " 1.8915690183639526,\n",
       " 2.018448829650879,\n",
       " 2.073305368423462,\n",
       " 1.8333810567855835,\n",
       " 1.8968327045440674,\n",
       " 2.0261871814727783,\n",
       " 1.932489037513733,\n",
       " 1.9648759365081787,\n",
       " 1.9932451248168945,\n",
       " 1.849921464920044,\n",
       " 2.183793783187866,\n",
       " 1.8601558208465576,\n",
       " 1.9439072608947754,\n",
       " 1.947643518447876,\n",
       " 2.015056610107422,\n",
       " 2.125195264816284,\n",
       " 2.0253489017486572,\n",
       " 1.9523760080337524,\n",
       " 2.13714861869812,\n",
       " 1.9567866325378418,\n",
       " 1.9018745422363281,\n",
       " 1.954325556755066,\n",
       " 1.7835536003112793,\n",
       " 1.9913110733032227,\n",
       " 1.9245610237121582,\n",
       " 1.8615138530731201,\n",
       " 2.1336658000946045,\n",
       " 1.7715449333190918,\n",
       " 2.0249412059783936,\n",
       " 2.116060495376587,\n",
       " 2.0429158210754395,\n",
       " 1.9015735387802124,\n",
       " 1.8947608470916748,\n",
       " 2.1416759490966797,\n",
       " 1.9542922973632812,\n",
       " 1.9602229595184326,\n",
       " 2.0240190029144287,\n",
       " 2.083921432495117,\n",
       " 1.7144972085952759,\n",
       " 1.853746771812439,\n",
       " 1.9655026197433472,\n",
       " 2.1397435665130615,\n",
       " 1.828084945678711,\n",
       " 1.9189375638961792,\n",
       " 1.9457757472991943,\n",
       " 2.021315336227417,\n",
       " 1.9261704683303833,\n",
       " 1.919022560119629,\n",
       " 2.013747453689575,\n",
       " 2.086134672164917,\n",
       " 1.8763465881347656,\n",
       " 2.128331184387207,\n",
       " 1.8898835182189941,\n",
       " 1.7744643688201904,\n",
       " 1.8974194526672363,\n",
       " 2.0242128372192383,\n",
       " 1.8966541290283203,\n",
       " 1.9383769035339355,\n",
       " 2.1934683322906494,\n",
       " 1.7207626104354858,\n",
       " 1.7759982347488403,\n",
       " 1.8423759937286377,\n",
       " 1.8953025341033936,\n",
       " 1.894988775253296,\n",
       " 1.9227774143218994,\n",
       " 1.903723120689392,\n",
       " 1.836587905883789,\n",
       " 1.8999648094177246,\n",
       " 1.8090603351593018,\n",
       " 1.645328164100647,\n",
       " 1.7800211906433105,\n",
       " 2.111884355545044,\n",
       " 1.8973637819290161,\n",
       " 2.0194454193115234,\n",
       " 1.7945187091827393,\n",
       " 2.0770263671875,\n",
       " 1.7708089351654053,\n",
       " 2.0759644508361816,\n",
       " 1.7798452377319336,\n",
       " 1.8899621963500977,\n",
       " 1.8297524452209473,\n",
       " 1.9567664861679077,\n",
       " 1.897201418876648,\n",
       " 1.8083778619766235,\n",
       " 1.710988163948059,\n",
       " 1.8337419033050537,\n",
       " 2.023681402206421,\n",
       " 1.7710175514221191,\n",
       " 1.9886826276779175,\n",
       " 1.9787745475769043,\n",
       " 1.8334486484527588,\n",
       " 1.8351233005523682,\n",
       " 1.798242449760437,\n",
       " 1.9427694082260132,\n",
       " 1.9653277397155762,\n",
       " 1.8812201023101807,\n",
       " 1.7769970893859863,\n",
       " 1.791783094406128,\n",
       " 2.071474075317383,\n",
       " 2.0137076377868652,\n",
       " 1.798433780670166,\n",
       " 1.8352863788604736,\n",
       " 1.539873480796814,\n",
       " 1.8309657573699951,\n",
       " 1.907604694366455,\n",
       " 1.7170227766036987,\n",
       " 1.9583771228790283,\n",
       " 1.9890165328979492,\n",
       " 2.0297348499298096,\n",
       " 2.0792765617370605,\n",
       " 2.2034823894500732,\n",
       " 1.8994874954223633,\n",
       " 1.9846351146697998,\n",
       " 2.104994773864746,\n",
       " 2.1048147678375244,\n",
       " 1.9551045894622803,\n",
       " 2.059678077697754,\n",
       " 1.9358625411987305,\n",
       " 2.0216357707977295,\n",
       " 2.0006515979766846,\n",
       " 1.8439455032348633,\n",
       " 1.9987140893936157,\n",
       " 2.01762056350708,\n",
       " 1.949825406074524,\n",
       " 2.131807327270508,\n",
       " 1.959524393081665,\n",
       " 1.75767982006073,\n",
       " 2.0190420150756836,\n",
       " 2.0866191387176514,\n",
       " 2.0373988151550293,\n",
       " 2.0706851482391357,\n",
       " 1.8543074131011963,\n",
       " 2.1405022144317627,\n",
       " 1.879378318786621,\n",
       " 1.9089736938476562,\n",
       " 1.9527068138122559,\n",
       " 2.144157886505127,\n",
       " 1.8399250507354736,\n",
       " 1.9582418203353882,\n",
       " 1.9658379554748535,\n",
       " 1.7896156311035156,\n",
       " 1.895260214805603,\n",
       " 1.8561961650848389,\n",
       " 1.8504235744476318,\n",
       " 2.0020852088928223,\n",
       " 1.9084581136703491,\n",
       " 2.0181713104248047,\n",
       " 1.9393188953399658,\n",
       " 1.9751157760620117,\n",
       " 1.8965909481048584,\n",
       " 1.8784369230270386,\n",
       " 1.9494359493255615,\n",
       " 2.154874801635742,\n",
       " 2.082857131958008,\n",
       " 2.0473082065582275,\n",
       " 1.771445870399475,\n",
       " 1.8575259447097778,\n",
       " 2.0732176303863525,\n",
       " 2.0462453365325928,\n",
       " 2.0374088287353516,\n",
       " 2.12398099899292,\n",
       " 1.7183302640914917,\n",
       " 1.95137619972229,\n",
       " 1.814764380455017,\n",
       " 1.835868239402771,\n",
       " 1.6497281789779663,\n",
       " 1.8480435609817505,\n",
       " 1.9082316160202026,\n",
       " 1.7591350078582764,\n",
       " 1.9093115329742432,\n",
       " 2.1330981254577637,\n",
       " 1.9491026401519775,\n",
       " 1.9570525884628296,\n",
       " 1.9634110927581787,\n",
       " 2.0204575061798096,\n",
       " 1.9250906705856323,\n",
       " 1.943427562713623,\n",
       " 1.900955319404602,\n",
       " 1.9047397375106812,\n",
       " 1.8978842496871948,\n",
       " 1.8693472146987915,\n",
       " 1.9385749101638794,\n",
       " 1.8436622619628906,\n",
       " 2.019385576248169,\n",
       " 1.7753756046295166,\n",
       " 1.965641736984253,\n",
       " 2.086221218109131,\n",
       " 1.881159782409668,\n",
       " 1.879318356513977,\n",
       " 2.0414793491363525,\n",
       " 2.0158703327178955,\n",
       " 1.9233022928237915,\n",
       " 1.6513619422912598,\n",
       " 1.947974443435669,\n",
       " 1.9644701480865479,\n",
       " 1.8887616395950317,\n",
       " 2.0166306495666504,\n",
       " 1.8973029851913452,\n",
       " 1.9041404724121094,\n",
       " 1.9455442428588867,\n",
       " 2.0214760303497314,\n",
       " 1.8991405963897705,\n",
       " 1.959226369857788,\n",
       " 1.9535881280899048,\n",
       " 1.8334269523620605,\n",
       " 1.907228708267212,\n",
       " 2.1021692752838135,\n",
       " 1.9746224880218506,\n",
       " 1.8164297342300415,\n",
       " 1.9538713693618774,\n",
       " 2.070537805557251,\n",
       " 2.20847749710083,\n",
       " 1.9584217071533203,\n",
       " 1.893632411956787,\n",
       " 1.7945942878723145,\n",
       " 1.772527813911438,\n",
       " 2.143855333328247,\n",
       " 1.7987595796585083,\n",
       " 1.844731330871582,\n",
       " 1.9569405317306519,\n",
       " 2.0474741458892822,\n",
       " 2.021436929702759,\n",
       " 1.854662299156189,\n",
       " 1.9597162008285522,\n",
       " 1.759053111076355,\n",
       " 1.7650442123413086,\n",
       " 2.1254844665527344,\n",
       " 1.9596810340881348,\n",
       " 1.775410532951355,\n",
       " 1.829048991203308,\n",
       " 1.919715404510498,\n",
       " 1.810223937034607,\n",
       " 1.9281301498413086,\n",
       " 1.7049628496170044,\n",
       " 2.068549156188965,\n",
       " 1.7191791534423828,\n",
       " 1.9594124555587769,\n",
       " 1.954637885093689,\n",
       " 1.8937503099441528,\n",
       " 1.9477488994598389,\n",
       " 1.908134937286377,\n",
       " 1.9495810270309448,\n",
       " 1.7291505336761475,\n",
       " 1.9837383031845093,\n",
       " 2.263765335083008,\n",
       " 2.0520355701446533,\n",
       " 1.8555495738983154,\n",
       " 1.8363994359970093,\n",
       " 1.722819447517395,\n",
       " 1.688513994216919,\n",
       " 1.9430978298187256,\n",
       " 1.8166965246200562,\n",
       " 1.7196565866470337,\n",
       " 1.7064573764801025,\n",
       " 1.83974027633667,\n",
       " 1.8502321243286133,\n",
       " 1.7753028869628906,\n",
       " 2.0074520111083984,\n",
       " 1.7107949256896973,\n",
       " 1.8271349668502808,\n",
       " 1.8472189903259277,\n",
       " 1.8725723028182983,\n",
       " 1.8946870565414429,\n",
       " 1.8341652154922485,\n",
       " 1.708936333656311,\n",
       " 1.8338844776153564,\n",
       " 1.7746139764785767,\n",
       " 1.9521797895431519,\n",
       " 1.9325231313705444,\n",
       " 1.8908121585845947,\n",
       " 1.8931162357330322,\n",
       " 1.8932653665542603,\n",
       " 1.7679555416107178,\n",
       " 1.9533627033233643,\n",
       " 1.834446668624878,\n",
       " 1.9525340795516968,\n",
       " 1.7903258800506592,\n",
       " 1.8892842531204224,\n",
       " 2.146226644515991,\n",
       " 2.1172661781311035,\n",
       " 2.138861894607544,\n",
       " 2.0319385528564453,\n",
       " 1.8915600776672363,\n",
       " 1.7062424421310425,\n",
       " 1.7714855670928955,\n",
       " 1.9562828540802002,\n",
       " 2.0093305110931396,\n",
       " 1.892170786857605,\n",
       " 1.8774394989013672,\n",
       " 1.8927159309387207,\n",
       " 1.9038825035095215,\n",
       " 1.9287760257720947,\n",
       " 1.7755858898162842,\n",
       " 2.06882643699646,\n",
       " 1.8002417087554932,\n",
       " 1.8966375589370728,\n",
       " 2.0235798358917236,\n",
       " 1.9462978839874268,\n",
       " 1.8287760019302368,\n",
       " 2.0855515003204346,\n",
       " 1.8865070343017578,\n",
       " 1.992817997932434,\n",
       " 2.2546534538269043,\n",
       " 1.8228051662445068,\n",
       " 1.8288922309875488,\n",
       " 1.894700527191162,\n",
       " 1.7430607080459595,\n",
       " 2.1336541175842285,\n",
       " 1.9477604627609253,\n",
       " 1.8182120323181152,\n",
       " 2.072705030441284,\n",
       " 1.7219346761703491,\n",
       " 1.940517783164978,\n",
       " 2.116849422454834,\n",
       " 1.8897135257720947,\n",
       " 1.922406792640686,\n",
       " 1.7805496454238892,\n",
       " 1.898604154586792,\n",
       " 1.9558337926864624,\n",
       " 2.140763282775879,\n",
       " 1.9408612251281738,\n",
       " 1.9116419553756714,\n",
       " 1.9713919162750244,\n",
       " 1.877021312713623,\n",
       " 1.758582592010498,\n",
       " 2.0216312408447266,\n",
       " 1.730269193649292,\n",
       " 1.7741951942443848,\n",
       " 1.8261842727661133,\n",
       " 1.7622840404510498,\n",
       " 1.8183518648147583,\n",
       " 1.6259480714797974,\n",
       " 2.0241446495056152,\n",
       " 1.778026819229126,\n",
       " 1.940420150756836,\n",
       " 1.7535836696624756,\n",
       " 1.735691785812378,\n",
       " 1.9570165872573853,\n",
       " 1.683680534362793,\n",
       " 1.9998009204864502,\n",
       " 1.9517310857772827,\n",
       " 1.762252688407898,\n",
       " 1.8912650346755981,\n",
       " 1.7520946264266968,\n",
       " 1.8974967002868652,\n",
       " 1.9592549800872803,\n",
       " 2.088300943374634,\n",
       " 1.9405437707901,\n",
       " 1.8348643779754639,\n",
       " 1.8969619274139404,\n",
       " 2.0009214878082275,\n",
       " 1.8218958377838135,\n",
       " 1.7889918088912964,\n",
       " 2.0003268718719482,\n",
       " 2.002371311187744,\n",
       " 1.8640409708023071,\n",
       " 1.7236437797546387,\n",
       " 1.8233743906021118,\n",
       " 1.829587697982788,\n",
       " 1.7127951383590698,\n",
       " 1.8152859210968018,\n",
       " 1.9434651136398315,\n",
       " 1.768396019935608,\n",
       " 2.153512716293335,\n",
       " 1.7659004926681519,\n",
       " 2.068803548812866,\n",
       " 1.9877779483795166,\n",
       " 1.9720312356948853,\n",
       " 1.8472881317138672,\n",
       " 1.752398133277893,\n",
       " 1.9832336902618408,\n",
       " 1.9992198944091797,\n",
       " 1.9601322412490845,\n",
       " 1.875085711479187,\n",
       " 1.972347617149353,\n",
       " 2.0563719272613525,\n",
       " 1.7111233472824097,\n",
       " 1.7790372371673584,\n",
       " 1.959874153137207,\n",
       " 1.8489075899124146,\n",
       " 1.6945854425430298,\n",
       " 1.9214550256729126,\n",
       " 1.6337120532989502,\n",
       " 1.9901596307754517,\n",
       " 2.0528550148010254,\n",
       " 1.8998748064041138,\n",
       " 1.8572481870651245,\n",
       " 1.8536653518676758,\n",
       " 1.8699337244033813,\n",
       " 1.7455461025238037,\n",
       " 1.9215220212936401,\n",
       " 1.9104797840118408,\n",
       " 1.805948257446289,\n",
       " 1.5582510232925415,\n",
       " 1.9576292037963867,\n",
       " 1.820190191268921,\n",
       " 1.8064708709716797,\n",
       " 1.6015257835388184,\n",
       " 1.8390305042266846,\n",
       " 1.9361399412155151,\n",
       " 1.9139783382415771,\n",
       " 1.7734742164611816,\n",
       " 1.8380589485168457,\n",
       " 1.6711628437042236,\n",
       " 1.7117741107940674,\n",
       " 1.7998899221420288,\n",
       " 1.8942254781723022,\n",
       " 1.9265199899673462,\n",
       " 1.7932817935943604,\n",
       " 1.7198493480682373,\n",
       " 1.7857389450073242,\n",
       " 1.97657310962677,\n",
       " 1.833234190940857,\n",
       " 1.7779369354248047,\n",
       " 1.9019711017608643,\n",
       " 2.083209991455078,\n",
       " 1.711398959159851,\n",
       " 1.8935034275054932,\n",
       " 2.1021716594696045,\n",
       " 1.9495899677276611,\n",
       " 1.8920987844467163,\n",
       " 1.9577504396438599,\n",
       " 1.709732174873352,\n",
       " 1.8941123485565186,\n",
       " 1.7952772378921509,\n",
       " 1.8247196674346924,\n",
       " 1.8612561225891113,\n",
       " 1.7726292610168457,\n",
       " 1.6740424633026123,\n",
       " 1.751378059387207,\n",
       " 1.8961453437805176,\n",
       " 1.8086317777633667,\n",
       " 1.7279279232025146,\n",
       " 1.6915156841278076,\n",
       " 1.851844310760498,\n",
       " 1.955040454864502,\n",
       " 1.708620548248291,\n",
       " 1.7052925825119019,\n",
       " 1.8655517101287842,\n",
       " 1.9043517112731934,\n",
       " 1.9495292901992798,\n",
       " 1.79369056224823,\n",
       " 1.7624828815460205,\n",
       " 1.6451797485351562,\n",
       " 1.8346083164215088,\n",
       " 1.7713816165924072,\n",
       " 1.9566079378128052,\n",
       " 1.7028836011886597,\n",
       " 1.7167376279830933,\n",
       " 1.893114447593689,\n",
       " 1.8346577882766724,\n",
       " 2.059610366821289,\n",
       " 1.8317729234695435,\n",
       " 1.7884364128112793,\n",
       " 2.011728525161743,\n",
       " 1.8748422861099243,\n",
       " 1.5899409055709839,\n",
       " 1.7706899642944336,\n",
       " 1.7152307033538818,\n",
       " 1.7499860525131226,\n",
       " 1.743013858795166,\n",
       " 1.6025283336639404,\n",
       " 1.778865933418274,\n",
       " 1.8398807048797607,\n",
       " 1.7107020616531372,\n",
       " 1.8110018968582153,\n",
       " 1.705971598625183,\n",
       " 1.9453099966049194,\n",
       " 1.907227635383606,\n",
       " 1.7678742408752441,\n",
       " 1.8317310810089111,\n",
       " 1.8036279678344727,\n",
       " 1.7878979444503784,\n",
       " 1.7199769020080566,\n",
       " 1.7773607969284058,\n",
       " 1.6594263315200806,\n",
       " 1.783028483390808,\n",
       " 1.7199993133544922,\n",
       " 1.6490092277526855,\n",
       " 1.691015362739563,\n",
       " 1.7694045305252075,\n",
       " 1.7904053926467896,\n",
       " 1.6598320007324219,\n",
       " 1.7100257873535156,\n",
       " 1.6499857902526855,\n",
       " 2.008817672729492,\n",
       " 1.894578218460083,\n",
       " 2.019707679748535,\n",
       " 1.7292757034301758,\n",
       " 1.7111809253692627,\n",
       " 1.7136573791503906,\n",
       " 1.6404145956039429,\n",
       " 1.7078218460083008,\n",
       " 1.8296399116516113,\n",
       " 1.6447627544403076,\n",
       " 1.6015381813049316,\n",
       " 1.8591029644012451,\n",
       " 1.7578420639038086,\n",
       " 1.727433443069458,\n",
       " 1.7637546062469482,\n",
       " 1.8733627796173096,\n",
       " 1.7095812559127808,\n",
       " 1.715785264968872,\n",
       " 1.8600976467132568,\n",
       " 1.5341124534606934,\n",
       " 1.7523157596588135,\n",
       " 1.623307704925537,\n",
       " 1.8426958322525024,\n",
       " 1.7031371593475342,\n",
       " 1.8998875617980957,\n",
       " 1.7330741882324219,\n",
       " 1.9035418033599854,\n",
       " 1.7747697830200195,\n",
       " 1.678189754486084,\n",
       " 1.7256869077682495,\n",
       " 1.695939540863037,\n",
       " 1.8438574075698853,\n",
       " 1.8529932498931885,\n",
       " 1.8434959650039673,\n",
       " 1.66724693775177,\n",
       " 1.7890905141830444,\n",
       " 1.6966537237167358,\n",
       " 1.7811970710754395,\n",
       " 2.007154703140259,\n",
       " 1.9588816165924072,\n",
       " 1.7110263109207153,\n",
       " 1.7106364965438843,\n",
       " 1.5967705249786377,\n",
       " 1.8405203819274902,\n",
       " 1.7120566368103027,\n",
       " 1.7740727663040161,\n",
       " 1.7103933095932007,\n",
       " 1.626935601234436,\n",
       " 1.8657876253128052,\n",
       " 1.6994447708129883,\n",
       " 1.6013351678848267,\n",
       " 1.8480454683303833,\n",
       " 1.7248691320419312,\n",
       " 1.689418911933899,\n",
       " 1.7691030502319336,\n",
       " 1.827399492263794,\n",
       " 1.661924123764038,\n",
       " 1.8270683288574219,\n",
       " 1.9142457246780396,\n",
       " 1.8377033472061157,\n",
       " 1.803977131843567,\n",
       " 1.7074918746948242,\n",
       " 1.8008887767791748,\n",
       " 1.8620845079421997,\n",
       " 1.745284080505371,\n",
       " 1.5505608320236206,\n",
       " 1.8391774892807007,\n",
       " 1.842955470085144,\n",
       " 1.859421730041504,\n",
       " 1.715157389640808,\n",
       " 1.721686601638794,\n",
       " 1.5985525846481323,\n",
       " 1.7063871622085571,\n",
       " 1.7122445106506348,\n",
       " 1.838549017906189,\n",
       " 1.9023391008377075,\n",
       " 1.7865397930145264,\n",
       " 1.7122055292129517,\n",
       " 1.7915605306625366,\n",
       " 1.8228460550308228,\n",
       " 1.7558622360229492,\n",
       " 1.649871587753296,\n",
       " 1.712800145149231,\n",
       " 1.8252503871917725,\n",
       " 1.5711984634399414,\n",
       " 1.728163719177246,\n",
       " 1.7733579874038696,\n",
       " 1.7743865251541138,\n",
       " 1.6670337915420532,\n",
       " 1.817787528038025,\n",
       " 1.7731398344039917,\n",
       " 1.9732646942138672,\n",
       " 1.6847292184829712,\n",
       " 1.6665478944778442,\n",
       " 1.698358416557312,\n",
       " 1.6519221067428589,\n",
       " 1.8483033180236816,\n",
       " 1.6226774454116821,\n",
       " 1.853639841079712,\n",
       " 1.617220163345337,\n",
       " 1.6061451435089111,\n",
       " 1.746277928352356,\n",
       " 1.8305137157440186,\n",
       " 1.782869577407837,\n",
       " 1.7809829711914062,\n",
       " 1.7701060771942139,\n",
       " 1.6432433128356934,\n",
       " 1.7481893301010132,\n",
       " 1.6656193733215332,\n",
       " 2.0012693405151367,\n",
       " 1.8367747068405151,\n",
       " 1.7169615030288696,\n",
       " 1.8313990831375122,\n",
       " 1.8336561918258667,\n",
       " 1.6511105298995972,\n",
       " 1.831335425376892,\n",
       " 1.6913700103759766,\n",
       " 1.7492995262145996,\n",
       " 1.6036906242370605,\n",
       " 1.7368693351745605,\n",
       " 1.7266454696655273,\n",
       " 1.7741752862930298,\n",
       " 1.5356965065002441,\n",
       " 1.751901388168335,\n",
       " 1.9127365350723267,\n",
       " 1.8543858528137207,\n",
       " 1.654755711555481,\n",
       " 1.8338245153427124,\n",
       " 1.6468722820281982,\n",
       " 1.660610556602478,\n",
       " 1.8403589725494385,\n",
       " 1.9743058681488037,\n",
       " 1.693120002746582,\n",
       " 1.913381576538086,\n",
       " 1.757119059562683,\n",
       " 1.8973615169525146,\n",
       " 1.7769653797149658,\n",
       " 1.9540327787399292,\n",
       " 1.7523853778839111,\n",
       " 1.711839199066162,\n",
       " 1.8472038507461548,\n",
       " 1.9585199356079102,\n",
       " 1.5619856119155884,\n",
       " 1.8508355617523193,\n",
       " 1.799042820930481,\n",
       " 1.8061420917510986,\n",
       " 1.7040164470672607,\n",
       " 1.9106829166412354,\n",
       " 1.6824795007705688,\n",
       " 1.585636854171753,\n",
       " 1.677186369895935,\n",
       " 1.8392173051834106,\n",
       " 1.6225559711456299,\n",
       " 1.8941102027893066,\n",
       " 1.8314752578735352,\n",
       " 2.1502366065979004,\n",
       " 1.7266799211502075,\n",
       " 1.881413459777832,\n",
       " 1.5519049167633057,\n",
       " 1.7743616104125977,\n",
       " 1.7260040044784546,\n",
       " 1.4882358312606812,\n",
       " 1.6597907543182373,\n",
       " 1.859102487564087,\n",
       " 1.5269478559494019,\n",
       " 1.6440761089324951,\n",
       " 1.7253276109695435,\n",
       " 1.8267414569854736,\n",
       " 1.8976898193359375,\n",
       " 1.7680411338806152,\n",
       " 1.8698458671569824,\n",
       " 1.7992541790008545,\n",
       " 1.8931695222854614,\n",
       " 1.8131070137023926,\n",
       " 1.6969233751296997,\n",
       " 1.6824898719787598,\n",
       " 1.6503251791000366,\n",
       " 1.726373553276062,\n",
       " 1.7160636186599731,\n",
       " 1.8149100542068481,\n",
       " 1.8317240476608276,\n",
       " 1.9771403074264526,\n",
       " 1.833641529083252,\n",
       " 1.9606775045394897,\n",
       " 1.8520427942276,\n",
       " 1.792187213897705,\n",
       " 1.7839608192443848,\n",
       " 1.7818751335144043,\n",
       " 1.6490734815597534,\n",
       " 1.5875935554504395,\n",
       " 1.5927319526672363,\n",
       " 1.9112091064453125,\n",
       " 1.7141605615615845,\n",
       " 1.6504194736480713,\n",
       " 1.761012077331543,\n",
       " 1.9566435813903809,\n",
       " 1.9338709115982056,\n",
       " 1.8865164518356323,\n",
       " 1.7776679992675781,\n",
       " 1.7177480459213257,\n",
       " 1.7661207914352417,\n",
       " 1.6572940349578857,\n",
       " 1.5839349031448364,\n",
       " 1.8090156316757202,\n",
       " 1.8958202600479126,\n",
       " 1.7713521718978882,\n",
       " 1.7138032913208008,\n",
       " 1.8067141771316528,\n",
       " 1.601873517036438,\n",
       " 1.6466964483261108,\n",
       " 1.8948171138763428,\n",
       " 1.5948846340179443,\n",
       " 1.6118096113204956,\n",
       " 1.7739895582199097,\n",
       " 1.6600844860076904,\n",
       " ...]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_gradient_results[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK. We're doing shared encoder now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_accuracy(encoder, decoder, inf_gen, test_steps):\n",
    "    correct = 0\n",
    "    num_data = 0\n",
    "\n",
    "    #grab a batch from the test loader\n",
    "    for j in range(test_steps):\n",
    "        examples, labels = next(inf_gen)\n",
    "        embedding = encoder.forward(examples)\n",
    "        outputs = decoder.forward(embedding)\n",
    "\n",
    "        #for each output in the batch, check if the label is correct\n",
    "        for i, output in enumerate(outputs):\n",
    "            num_data += 1\n",
    "\n",
    "            max_i = np.argmax(output.detach().numpy())\n",
    "            if max_i == labels[i]:\n",
    "                correct += 1\n",
    "\n",
    "    acc = float(correct)/num_data\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "60000\n",
      "10000\n",
      "3750\n",
      "625\n",
      "3750\n",
      "625\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_1 = 16\n",
    "BATCH_SIZE_2 = 16\n",
    "\n",
    "mnist_trainset_1 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_trainset_2 = datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_1 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_testset_2 = datasets.MNIST(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "print(len(mnist_trainset_1))\n",
    "print(len(mnist_testset_1))\n",
    "print(len(mnist_trainset_2))\n",
    "print(len(mnist_testset_2))\n",
    "\n",
    "max_trainsteps_1 = len(mnist_trainset_1) // BATCH_SIZE_1\n",
    "max_teststeps_1 = len(mnist_testset_1) // BATCH_SIZE_1\n",
    "max_trainsteps_2 = len(mnist_trainset_2) // BATCH_SIZE_2\n",
    "max_teststeps_2 = len(mnist_testset_2) // BATCH_SIZE_2\n",
    "\n",
    "print(max_trainsteps_1)\n",
    "print(max_teststeps_1)\n",
    "print(max_trainsteps_2)\n",
    "print(max_teststeps_2)\n",
    "\n",
    "train_loader_1 = DataLoader(mnist_trainset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "test_loader_1 = DataLoader(mnist_testset_1, batch_size=BATCH_SIZE_1, shuffle=True, drop_last=True)\n",
    "train_loader_2 = DataLoader(mnist_trainset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "test_loader_2 = DataLoader(mnist_testset_2, batch_size=BATCH_SIZE_2, shuffle=True, drop_last=True)\n",
    "\n",
    "inf_train_1 = inf_loader_generator(train_loader_1)\n",
    "inf_test_1 = inf_loader_generator(test_loader_1)\n",
    "inf_train_2 = inf_loader_generator(train_loader_2)\n",
    "inf_test_2 = inf_loader_generator(test_loader_2)\n",
    "\n",
    "iters_per_epoch_train_1 = len(mnist_trainset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_test_1 = len(mnist_testset_1)//BATCH_SIZE_1\n",
    "iters_per_epoch_train_2 = len(mnist_trainset_2)//BATCH_SIZE_2\n",
    "iters_per_epoch_test_2 = len(mnist_testset_2)//BATCH_SIZE_2\n",
    "\n",
    "# next(iter(train_loader))[0].shape --> torch.Size([16, 1, 28, 28])\n",
    "# This means we have 16 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mnist_Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=(5,5))\n",
    "        self.conv2 = nn.Conv2d(28, 32, kernel_size=(5,5))\n",
    "        #ok, i undersatnd this now. the output from conv2 is 32 channels. the remaining width and height after\n",
    "        # two 5x5 filters with NO PADDING and STRIDE 1 (defaults) is 20x20. So 32 channels * 20 h * 20 w\n",
    "        self.fc1 = nn.Linear(32*20*20, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        # print(x.size()) --> torch.Size([16, 32, 20, 20])\n",
    "        x = x.view(-1, 32*20*20)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return torch.softmax(x,dim=1)\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=28):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv = nn.Conv2d(input_size, hidden_size, kernel_size=(5,5))\n",
    "        self.output_shape = (hidden_size,24,24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return x\n",
    "    \n",
    "class DecoderCNN(nn.Module):\n",
    "    def __init__(self, hidden_size=28):\n",
    "        super(DecoderCNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv = nn.Conv2d(hidden_size, 32, kernel_size=(5,5))\n",
    "        self.fc1 = nn.Linear(32*20*20, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = x.view(-1, 32*20*20)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return torch.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encdec():\n",
    "    \n",
    "    # initialize the models\n",
    "    hidden_size = 28\n",
    "    encoder = EncoderCNN(1, hidden_size)\n",
    "    decoder = DecoderCNN(hidden_size)\n",
    "    \n",
    "    # encoder \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(),lr=0.001)\n",
    "    \n",
    "    # decoder \n",
    "    decoder_criterion = nn.CrossEntropyLoss()\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(),lr=0.001)\n",
    "    \n",
    "    # number of epochs and iterations\n",
    "    training = True\n",
    "    epochs = 20\n",
    "    train_steps_per_epoch = 2000\n",
    "    test_steps_per_epoch = 300\n",
    "    model_steps = 0\n",
    "    \n",
    "    # plotting criteria\n",
    "    train_losses = []\n",
    "    test_accs = []  \n",
    "    \n",
    "    print(\"Start of Epoch %d\" % (model_steps/train_steps_per_epoch))\n",
    "    while training:\n",
    "        \n",
    "        # get next batch\n",
    "        x_batch_train, y_batch_train = next(inf_train_1)\n",
    "        \n",
    "        # zero out the gradients from the previous iteration\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        #compute loss\n",
    "        embedding = encoder.forward(x_batch_train)\n",
    "        outputs = decoder.forward(embedding)\n",
    "        loss = decoder_criterion(outputs, y_batch_train)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # perform the backpropagation\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        model_steps += 1\n",
    "        \n",
    "        # perform the backpropagation\n",
    "        if model_steps % train_steps_per_epoch == 0:\n",
    "            print(\"Average Train Loss:\", np.mean(train_losses))\n",
    "            test_accs.append(encdec_accuracy(encoder, decoder, inf_test_1, test_steps_per_epoch))\n",
    "            print(\"Test Accuracy:\", test_accs[-1])\n",
    "            print(\"Start of Epoch %d\" % (model_steps/train_steps_per_epoch))\n",
    "            \n",
    "        if model_steps > (train_steps_per_epoch*epochs) + 1:\n",
    "            break\n",
    "    return [encdec_accuracy(encoder, decoder, inf_test_1, test_steps_per_epoch*2), (train_losses, test_accs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_enc2dec():\n",
    "    \n",
    "    # initialize the models\n",
    "    hidden_size = 28\n",
    "    encoder = EncoderCNN(1, hidden_size)\n",
    "    decoder = DecoderCNN(hidden_size)\n",
    "    decoder2 = DecoderCNN(hidden_size)\n",
    "    \n",
    "    # encoder \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(),lr=0.001)\n",
    "    \n",
    "    # decoder \n",
    "    decoder_criterion = nn.CrossEntropyLoss()\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(),lr=0.001)\n",
    "    decoder_criterion2 = nn.CrossEntropyLoss()\n",
    "    decoder_optimizer2 = optim.Adam(decoder2.parameters(),lr=0.001)\n",
    "    \n",
    "    \n",
    "    # number of epochs and iterations\n",
    "    training = True\n",
    "    epochs = 20\n",
    "    train_steps_per_epoch = 2000\n",
    "    test_steps_per_epoch = 300\n",
    "    model_steps = 0\n",
    "    \n",
    "    # plotting criteria\n",
    "    train_losses = []\n",
    "    train_losses2 = []\n",
    "    test_accs = []\n",
    "    test_accs2 = []\n",
    "    \n",
    "    print(\"Start of Epoch %d\" % (model_steps/train_steps_per_epoch))\n",
    "    while training:\n",
    "        \n",
    "        # get next batch\n",
    "        x_batch_train, y_batch_train = next(inf_train_1)\n",
    "        x_batch_train2, y_batch_train2 = next(inf_train_2)\n",
    "        \n",
    "        # zero out the gradients from the previous iteration\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        decoder_optimizer2.zero_grad()\n",
    "\n",
    "        # compute loss\n",
    "        embedding = encoder.forward(x_batch_train)\n",
    "        outputs = decoder.forward(embedding)\n",
    "        loss = decoder_criterion(outputs, y_batch_train)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # perform the backpropagation\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        #DON'T FORGET TO RE-ZERO THE ENCODER GRADIENT!!! OTHERWISE YOU'RE REUSING THE FIRST GRAD\n",
    "        encoder_optimizer.zero_grad()\n",
    "    \n",
    "        # compute loss and backprop for second model\n",
    "        embedding = encoder.forward(x_batch_train2)\n",
    "        outputs = decoder2.forward(embedding)\n",
    "        loss = decoder_criterion2(outputs, y_batch_train2)\n",
    "        loss.backward()\n",
    "        train_losses2.append(loss.item())\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer2.step()\n",
    "        \n",
    "        model_steps += 1\n",
    "        \n",
    "        # perform the backpropagation\n",
    "        if model_steps % train_steps_per_epoch == 0:\n",
    "            print(\"Average Train Loss:\", np.mean(train_losses))\n",
    "            print(\"Average Train Loss2:\", np.mean(train_losses2))\n",
    "            test_accs.append(encdec_accuracy(encoder, decoder, inf_test_1, test_steps_per_epoch))\n",
    "            test_accs2.append(encdec_accuracy(encoder, decoder2, inf_test_2, test_steps_per_epoch))\n",
    "            print(\"Test Accuracy:\", test_accs[-1])\n",
    "            print(\"Test Accuracy2:\", test_accs2[-1])\n",
    "            print(\"Start of Epoch %d\" % (model_steps/train_steps_per_epoch))\n",
    "            \n",
    "        if model_steps > (train_steps_per_epoch*epochs) + 1:\n",
    "            break\n",
    "    return [\n",
    "        [encdec_accuracy(encoder, decoder, inf_test_1, test_steps_per_epoch*2), (train_losses, test_accs)],\n",
    "        [encdec_accuracy(encoder, decoder2, inf_test_2, test_steps_per_epoch*2), (train_losses2, test_accs2)]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Epoch 0\n",
      "Average Train Loss: 1.6856881410479545\n",
      "Test Accuracy: 0.8764583333333333\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.6145659938454628\n",
      "Test Accuracy: 0.9714583333333333\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.573092342098554\n",
      "Test Accuracy: 0.9739583333333334\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.5520765731930732\n",
      "Test Accuracy: 0.9760416666666667\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.5392769233345986\n",
      "Test Accuracy: 0.975\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.530297676285108\n",
      "Test Accuracy: 0.9783333333333334\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.5235731596691269\n",
      "Test Accuracy: 0.9783333333333334\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.5183095410987735\n",
      "Test Accuracy: 0.9754166666666667\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.5144596832725736\n",
      "Test Accuracy: 0.9808333333333333\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.5110546961665154\n",
      "Test Accuracy: 0.9808333333333333\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.5083946413018487\n",
      "Test Accuracy: 0.97875\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.5061841736584902\n",
      "Test Accuracy: 0.9735416666666666\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.5043589269152053\n",
      "Test Accuracy: 0.98375\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.5026671694687435\n",
      "Test Accuracy: 0.9733333333333334\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.5012005381941795\n",
      "Test Accuracy: 0.9814583333333333\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.500050297267735\n",
      "Test Accuracy: 0.980625\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.4990297555257293\n",
      "Test Accuracy: 0.9804166666666667\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.4979836179349157\n",
      "Test Accuracy: 0.9847916666666666\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.497088280702892\n",
      "Test Accuracy: 0.9810416666666667\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.4964346538364888\n",
      "Test Accuracy: 0.9745833333333334\n",
      "Start of Epoch 20\n",
      "----------------------\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.732296301305294\n",
      "Test Accuracy: 0.7670833333333333\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.6960019907653332\n",
      "Test Accuracy: 0.778125\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.6826595552563668\n",
      "Test Accuracy: 0.7877083333333333\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.6756135302782058\n",
      "Test Accuracy: 0.7795833333333333\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6710656175255776\n",
      "Test Accuracy: 0.7808333333333334\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.6675181834896406\n",
      "Test Accuracy: 0.7725\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.6651359413095883\n",
      "Test Accuracy: 0.7914583333333334\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.6636344673931598\n",
      "Test Accuracy: 0.7858333333333334\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.6615863975948757\n",
      "Test Accuracy: 0.783125\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.6602297990441321\n",
      "Test Accuracy: 0.795625\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.6591209399971094\n",
      "Test Accuracy: 0.7814583333333334\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.6580809355527162\n",
      "Test Accuracy: 0.7860416666666666\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.6576907738630589\n",
      "Test Accuracy: 0.779375\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.6571588179809706\n",
      "Test Accuracy: 0.7829166666666667\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.6565055894096692\n",
      "Test Accuracy: 0.7729166666666667\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.6560740514211356\n",
      "Test Accuracy: 0.7870833333333334\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.6555624641180038\n",
      "Test Accuracy: 0.7820833333333334\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.6551387059324318\n",
      "Test Accuracy: 0.7860416666666666\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.6548606020619994\n",
      "Average Train Loss: 1.6545461003541946\n",
      "Test Accuracy: 0.7870833333333334\n",
      "Start of Epoch 20\n",
      "----------------------\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.7739132652878762\n",
      "Test Accuracy: 0.7872916666666666\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.7207767900228501\n",
      "Test Accuracy: 0.858125\n",
      "Start of Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-3d5200d0a2b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_encdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-ea7a6837ec90>\u001b[0m in \u001b[0;36mtrain_encdec\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_encdec()\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Epoch 0\n",
      "Average Train Loss: 1.8712317019104958\n",
      "Average Train Loss2: 1.6492022513747215\n",
      "Test Accuracy: 0.5779166666666666\n",
      "Test Accuracy2: 0.8677083333333333\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.8107159850001335\n",
      "Average Train Loss2: 1.619957497626543\n",
      "Test Accuracy: 0.6785416666666667\n",
      "Test Accuracy2: 0.879375\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.7870267972548803\n",
      "Average Train Loss2: 1.6088859236240387\n",
      "Test Accuracy: 0.6875\n",
      "Test Accuracy2: 0.8779166666666667\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.7744839192330837\n",
      "Average Train Loss2: 1.6020357965826988\n",
      "Test Accuracy: 0.671875\n",
      "Test Accuracy2: 0.8820833333333333\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.7662615366697312\n",
      "Average Train Loss2: 1.5959362713456153\n",
      "Test Accuracy: 0.6827083333333334\n",
      "Test Accuracy2: 0.9610416666666667\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.7607055297791958\n",
      "Average Train Loss2: 1.5783692828516165\n",
      "Test Accuracy: 0.6889583333333333\n",
      "Test Accuracy2: 0.9772916666666667\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.756841068966048\n",
      "Average Train Loss2: 1.5655271249072893\n",
      "Test Accuracy: 0.6910416666666667\n",
      "Test Accuracy2: 0.9733333333333334\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.7534197559803724\n",
      "Average Train Loss2: 1.5554753618314863\n",
      "Test Accuracy: 0.6785416666666667\n",
      "Test Accuracy2: 0.9802083333333333\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.7509838521745469\n",
      "Average Train Loss2: 1.5475163800319036\n",
      "Test Accuracy: 0.6770833333333334\n",
      "Test Accuracy2: 0.9804166666666667\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.7490039100825787\n",
      "Average Train Loss2: 1.5413252499699592\n",
      "Test Accuracy: 0.6891666666666667\n",
      "Test Accuracy2: 0.9754166666666667\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.7473122286471454\n",
      "Average Train Loss2: 1.5363301360607147\n",
      "Test Accuracy: 0.68125\n",
      "Test Accuracy2: 0.975625\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.7460150127361218\n",
      "Average Train Loss2: 1.5321090976297855\n",
      "Test Accuracy: 0.6897916666666667\n",
      "Test Accuracy2: 0.9760416666666667\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.7448876749781461\n",
      "Average Train Loss2: 1.528479689671443\n",
      "Test Accuracy: 0.6785416666666667\n",
      "Test Accuracy2: 0.9754166666666667\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.7435661566598075\n",
      "Average Train Loss2: 1.5253311341532638\n",
      "Test Accuracy: 0.6816666666666666\n",
      "Test Accuracy2: 0.9775\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.7428676950613657\n",
      "Average Train Loss2: 1.5224542111754418\n",
      "Test Accuracy: 0.6860416666666667\n",
      "Test Accuracy2: 0.981875\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.7420516924597322\n",
      "Average Train Loss2: 1.5200880277007818\n",
      "Test Accuracy: 0.6835416666666667\n",
      "Test Accuracy2: 0.97875\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.7410551730254118\n",
      "Average Train Loss2: 1.5178919537347906\n",
      "Test Accuracy: 0.6839583333333333\n",
      "Test Accuracy2: 0.976875\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.7407429131600591\n",
      "Average Train Loss2: 1.5160737455884616\n",
      "Test Accuracy: 0.6779166666666666\n",
      "Test Accuracy2: 0.9725\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.7400630148962923\n",
      "Average Train Loss2: 1.514475739949628\n",
      "Test Accuracy: 0.69375\n",
      "Test Accuracy2: 0.98375\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.73950378703475\n",
      "Average Train Loss2: 1.512839303830266\n",
      "Test Accuracy: 0.68125\n",
      "Test Accuracy2: 0.9727083333333333\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.773978310585022\n",
      "Average Train Loss2: 2.019312346339226\n",
      "Test Accuracy: 0.76625\n",
      "Test Accuracy2: 0.4714583333333333\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.7209706775546074\n",
      "Average Train Loss2: 2.00410045427084\n",
      "Test Accuracy: 0.7879166666666667\n",
      "Test Accuracy2: 0.47020833333333334\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.698856593489647\n",
      "Average Train Loss2: 1.998214613835017\n",
      "Test Accuracy: 0.77875\n",
      "Test Accuracy2: 0.47854166666666664\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.687806815803051\n",
      "Average Train Loss2: 1.9950099293738603\n",
      "Test Accuracy: 0.7904166666666667\n",
      "Test Accuracy2: 0.4689583333333333\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6800368098378182\n",
      "Average Train Loss2: 1.9931164644241333\n",
      "Test Accuracy: 0.78125\n",
      "Test Accuracy2: 0.4764583333333333\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.6748936919470627\n",
      "Average Train Loss2: 1.9912834987243015\n",
      "Test Accuracy: 0.7854166666666667\n",
      "Test Accuracy2: 0.473125\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.6709604300430843\n",
      "Average Train Loss2: 1.990083091676235\n",
      "Test Accuracy: 0.7914583333333334\n",
      "Test Accuracy2: 0.47854166666666664\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.668181842289865\n",
      "Average Train Loss2: 1.9894457546323538\n",
      "Test Accuracy: 0.7808333333333334\n",
      "Test Accuracy2: 0.4708333333333333\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.6657885536551476\n",
      "Average Train Loss2: 1.9884898922244707\n",
      "Test Accuracy: 0.7827083333333333\n",
      "Test Accuracy2: 0.475\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.6640649805784224\n",
      "Average Train Loss2: 1.9880120115935802\n",
      "Test Accuracy: 0.7833333333333333\n",
      "Test Accuracy2: 0.479375\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.662470095981251\n",
      "Average Train Loss2: 1.9873798294284126\n",
      "Test Accuracy: 0.785\n",
      "Test Accuracy2: 0.4689583333333333\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.661015544215838\n",
      "Average Train Loss2: 1.9868040889253218\n",
      "Test Accuracy: 0.7885416666666667\n",
      "Test Accuracy2: 0.475625\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.6600812977689963\n",
      "Average Train Loss2: 1.9866693441180083\n",
      "Test Accuracy: 0.774375\n",
      "Test Accuracy2: 0.474375\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.659159862262862\n",
      "Average Train Loss2: 1.9864281275825841\n",
      "Test Accuracy: 0.791875\n",
      "Test Accuracy2: 0.471875\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.6583360198497772\n",
      "Average Train Loss2: 1.9862589977025986\n",
      "Test Accuracy: 0.7808333333333334\n",
      "Test Accuracy2: 0.47729166666666667\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.6576310114823283\n",
      "Average Train Loss2: 1.986058219704777\n",
      "Test Accuracy: 0.79\n",
      "Test Accuracy2: 0.4714583333333333\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.6571118868624464\n",
      "Average Train Loss2: 1.9858292410408749\n",
      "Test Accuracy: 0.7852083333333333\n",
      "Test Accuracy2: 0.47583333333333333\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.6564771503673659\n",
      "Average Train Loss2: 1.9855284337633186\n",
      "Test Accuracy: 0.7866666666666666\n",
      "Test Accuracy2: 0.4766666666666667\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.6560011271991228\n",
      "Average Train Loss2: 1.9853046433172727\n",
      "Test Accuracy: 0.7872916666666666\n",
      "Test Accuracy2: 0.480625\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.6557808916836978\n",
      "Average Train Loss2: 1.985152246338129\n",
      "Test Accuracy: 0.7770833333333333\n",
      "Test Accuracy2: 0.4658333333333333\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.738513817667961\n",
      "Average Train Loss2: 1.7150832919478416\n",
      "Test Accuracy: 0.8683333333333333\n",
      "Test Accuracy2: 0.86625\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.6666435152292252\n",
      "Average Train Loss2: 1.6570699064433574\n",
      "Test Accuracy: 0.8777083333333333\n",
      "Test Accuracy2: 0.8679166666666667\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.6412253228624663\n",
      "Average Train Loss2: 1.6355748692552248\n",
      "Test Accuracy: 0.8654166666666666\n",
      "Test Accuracy2: 0.868125\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.6268195046782494\n",
      "Average Train Loss2: 1.6240103547871112\n",
      "Test Accuracy: 0.8810416666666666\n",
      "Test Accuracy2: 0.8745833333333334\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6036350951075553\n",
      "Average Train Loss2: 1.6166695075511932\n",
      "Test Accuracy: 0.9639583333333334\n",
      "Test Accuracy2: 0.8777083333333333\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.5844150595565636\n",
      "Average Train Loss2: 1.610989704012871\n",
      "Test Accuracy: 0.9729166666666667\n",
      "Test Accuracy2: 0.8810416666666666\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.5703442285912377\n",
      "Average Train Loss2: 1.6072585405026163\n",
      "Test Accuracy: 0.9825\n",
      "Test Accuracy2: 0.8772916666666667\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.5597870852053166\n",
      "Average Train Loss2: 1.6043161746487022\n",
      "Test Accuracy: 0.973125\n",
      "Test Accuracy2: 0.8795833333333334\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.551275901099046\n",
      "Average Train Loss2: 1.6021937315596475\n",
      "Test Accuracy: 0.97875\n",
      "Test Accuracy2: 0.8858333333333334\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.5446163057923317\n",
      "Average Train Loss2: 1.6002993302226067\n",
      "Test Accuracy: 0.9808333333333333\n",
      "Test Accuracy2: 0.8825\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.5390682856982405\n",
      "Average Train Loss2: 1.5986729606335814\n",
      "Test Accuracy: 0.9804166666666667\n",
      "Test Accuracy2: 0.8764583333333333\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.534380873069167\n",
      "Average Train Loss2: 1.5977090496619542\n",
      "Test Accuracy: 0.980625\n",
      "Test Accuracy2: 0.8877083333333333\n",
      "Start of Epoch 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 1.530325853925485\n",
      "Average Train Loss2: 1.5966617670242602\n",
      "Test Accuracy: 0.965\n",
      "Test Accuracy2: 0.871875\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.5268194014685494\n",
      "Average Train Loss2: 1.5956694611523832\n",
      "Test Accuracy: 0.9797916666666666\n",
      "Test Accuracy2: 0.8754166666666666\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.5235993537068366\n",
      "Average Train Loss2: 1.594873136695226\n",
      "Test Accuracy: 0.9785416666666666\n",
      "Test Accuracy2: 0.875625\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.5209957524612545\n",
      "Average Train Loss2: 1.5942536441385746\n",
      "Test Accuracy: 0.98125\n",
      "Test Accuracy2: 0.8785416666666667\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.5186903234369615\n",
      "Average Train Loss2: 1.5939864382743836\n",
      "Test Accuracy: 0.980625\n",
      "Test Accuracy2: 0.8764583333333333\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.516714824590418\n",
      "Average Train Loss2: 1.59344484651751\n",
      "Test Accuracy: 0.9829166666666667\n",
      "Test Accuracy2: 0.873125\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.5148578729472661\n",
      "Average Train Loss2: 1.5930954696912514\n",
      "Test Accuracy: 0.97875\n",
      "Test Accuracy2: 0.880625\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.513094857943058\n",
      "Average Train Loss2: 1.5927463538646698\n",
      "Test Accuracy: 0.9683333333333334\n",
      "Test Accuracy2: 0.8745833333333334\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.7096138193011283\n",
      "Average Train Loss2: 1.772370382130146\n",
      "Test Accuracy: 0.866875\n",
      "Test Accuracy2: 0.77875\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.650180897951126\n",
      "Average Train Loss2: 1.6851649475991726\n",
      "Test Accuracy: 0.861875\n",
      "Test Accuracy2: 0.8741666666666666\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.6282252875963847\n",
      "Average Train Loss2: 1.6514600708882015\n",
      "Test Accuracy: 0.8808333333333334\n",
      "Test Accuracy2: 0.8802083333333334\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.6167938153892756\n",
      "Average Train Loss2: 1.6337534487694503\n",
      "Test Accuracy: 0.878125\n",
      "Test Accuracy2: 0.8883333333333333\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6096587363123893\n",
      "Average Train Loss2: 1.6174993771076203\n",
      "Test Accuracy: 0.8791666666666667\n",
      "Test Accuracy2: 0.9729166666666667\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.604056774109602\n",
      "Average Train Loss2: 1.5962156079808871\n",
      "Test Accuracy: 0.8822916666666667\n",
      "Test Accuracy2: 0.9679166666666666\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.600198860372816\n",
      "Average Train Loss2: 1.5804696366446358\n",
      "Test Accuracy: 0.8708333333333333\n",
      "Test Accuracy2: 0.9772916666666667\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.593116765856743\n",
      "Average Train Loss2: 1.5684908398389816\n",
      "Test Accuracy: 0.9741666666666666\n",
      "Test Accuracy2: 0.9677083333333333\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.581192424164878\n",
      "Average Train Loss2: 1.5594212758143744\n",
      "Test Accuracy: 0.9802083333333333\n",
      "Test Accuracy2: 0.970625\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.5714217514455318\n",
      "Average Train Loss2: 1.5520114422738551\n",
      "Test Accuracy: 0.9775\n",
      "Test Accuracy2: 0.9735416666666666\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.5635191158381376\n",
      "Average Train Loss2: 1.54581208426844\n",
      "Test Accuracy: 0.9825\n",
      "Test Accuracy2: 0.9666666666666667\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.556711691548427\n",
      "Average Train Loss2: 1.5406814659535886\n",
      "Test Accuracy: 0.98125\n",
      "Test Accuracy2: 0.9679166666666666\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.5509519603573358\n",
      "Average Train Loss2: 1.5363405773960628\n",
      "Test Accuracy: 0.9791666666666666\n",
      "Test Accuracy2: 0.9829166666666667\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.5460622082139766\n",
      "Average Train Loss2: 1.5326397344214575\n",
      "Test Accuracy: 0.9810416666666667\n",
      "Test Accuracy2: 0.980625\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.5417055528203647\n",
      "Average Train Loss2: 1.5295795711278914\n",
      "Test Accuracy: 0.976875\n",
      "Test Accuracy2: 0.9477083333333334\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.5379943023696543\n",
      "Average Train Loss2: 1.5268151070550084\n",
      "Test Accuracy: 0.98\n",
      "Test Accuracy2: 0.978125\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.5346450325636303\n",
      "Average Train Loss2: 1.5244988469797023\n",
      "Test Accuracy: 0.9825\n",
      "Test Accuracy2: 0.9779166666666667\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.531763708713982\n",
      "Average Train Loss2: 1.5223723491827648\n",
      "Test Accuracy: 0.9785416666666666\n",
      "Test Accuracy2: 0.9720833333333333\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.5291480075246409\n",
      "Average Train Loss2: 1.5204310824808323\n",
      "Test Accuracy: 0.981875\n",
      "Test Accuracy2: 0.9733333333333334\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.5270457738876342\n",
      "Average Train Loss2: 1.5187210044413806\n",
      "Test Accuracy: 0.9689583333333334\n",
      "Test Accuracy2: 0.9729166666666667\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.7845977239012718\n",
      "Average Train Loss2: 1.7657572876214982\n",
      "Test Accuracy: 0.7739583333333333\n",
      "Test Accuracy2: 0.7720833333333333\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.7250003053247929\n",
      "Average Train Loss2: 1.6970215568244458\n",
      "Test Accuracy: 0.77875\n",
      "Test Accuracy2: 0.8795833333333334\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.7031173069675762\n",
      "Average Train Loss2: 1.658490887025992\n",
      "Test Accuracy: 0.7791666666666667\n",
      "Test Accuracy2: 0.8816666666666667\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.6906396659910679\n",
      "Average Train Loss2: 1.6387912786453962\n",
      "Test Accuracy: 0.7875\n",
      "Test Accuracy2: 0.8860416666666666\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6819988376021384\n",
      "Average Train Loss2: 1.6147039006233215\n",
      "Test Accuracy: 0.7754166666666666\n",
      "Test Accuracy2: 0.9597916666666667\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.6765470793445905\n",
      "Average Train Loss2: 1.594181205958128\n",
      "Test Accuracy: 0.784375\n",
      "Test Accuracy2: 0.971875\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.672371782856328\n",
      "Average Train Loss2: 1.5791435145735742\n",
      "Test Accuracy: 0.7891666666666667\n",
      "Test Accuracy2: 0.9752083333333333\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.6693954867571592\n",
      "Average Train Loss2: 1.567788396216929\n",
      "Test Accuracy: 0.7814583333333334\n",
      "Test Accuracy2: 0.9710416666666667\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.6667561419142618\n",
      "Average Train Loss2: 1.5590358024239541\n",
      "Test Accuracy: 0.7852083333333333\n",
      "Test Accuracy2: 0.9702083333333333\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.6648241881906987\n",
      "Average Train Loss2: 1.5519106545865535\n",
      "Test Accuracy: 0.779375\n",
      "Test Accuracy2: 0.9764583333333333\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.6628090814677152\n",
      "Average Train Loss2: 1.5457385479970411\n",
      "Test Accuracy: 0.7904166666666667\n",
      "Test Accuracy2: 0.9708333333333333\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.6616826437413692\n",
      "Average Train Loss2: 1.5407651548037926\n",
      "Test Accuracy: 0.7872916666666666\n",
      "Test Accuracy2: 0.976875\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.6603725094611828\n",
      "Average Train Loss2: 1.53644354816125\n",
      "Test Accuracy: 0.7816666666666666\n",
      "Test Accuracy2: 0.9697916666666667\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.6594204469408307\n",
      "Average Train Loss2: 1.532805448868445\n",
      "Test Accuracy: 0.7735416666666667\n",
      "Test Accuracy2: 0.9772916666666667\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.6583540366331737\n",
      "Average Train Loss2: 1.5295663142402967\n",
      "Test Accuracy: 0.7945833333333333\n",
      "Test Accuracy2: 0.9708333333333333\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.6578588362149895\n",
      "Average Train Loss2: 1.5268548610322177\n",
      "Test Accuracy: 0.7758333333333334\n",
      "Test Accuracy2: 0.974375\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.6572941276255775\n",
      "Average Train Loss2: 1.5244476422281825\n",
      "Test Accuracy: 0.7877083333333333\n",
      "Test Accuracy2: 0.9804166666666667\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.6566722646388743\n",
      "Average Train Loss2: 1.5223173721366459\n",
      "Test Accuracy: 0.783125\n",
      "Test Accuracy2: 0.9691666666666666\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.656038718449442\n",
      "Average Train Loss2: 1.5204261420306406\n",
      "Test Accuracy: 0.788125\n",
      "Test Accuracy2: 0.9752083333333333\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.6555801922649145\n",
      "Average Train Loss2: 1.5186969014197589\n",
      "Test Accuracy: 0.7895833333333333\n",
      "Test Accuracy2: 0.9789583333333334\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.6636342121958732\n",
      "Average Train Loss2: 1.6475866373181343\n",
      "Test Accuracy: 0.8754166666666666\n",
      "Test Accuracy2: 0.9554166666666667\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.6282110816538333\n",
      "Average Train Loss2: 1.576833633095026\n",
      "Test Accuracy: 0.8810416666666666\n",
      "Test Accuracy2: 0.9666666666666667\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.6138650949199995\n",
      "Average Train Loss2: 1.5496054071187972\n",
      "Test Accuracy: 0.8758333333333334\n",
      "Test Accuracy2: 0.9666666666666667\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.6065444164723157\n",
      "Average Train Loss2: 1.5349939530342818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8747916666666666\n",
      "Test Accuracy2: 0.968125\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.60144730104208\n",
      "Average Train Loss2: 1.5258106170892716\n",
      "Test Accuracy: 0.885625\n",
      "Test Accuracy2: 0.9816666666666667\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.5978533722360928\n",
      "Average Train Loss2: 1.5195492649277051\n",
      "Test Accuracy: 0.8841666666666667\n",
      "Test Accuracy2: 0.9785416666666666\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.5950458746637617\n",
      "Average Train Loss2: 1.5147152874299459\n",
      "Test Accuracy: 0.88625\n",
      "Test Accuracy2: 0.9802083333333333\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.5930699144974352\n",
      "Average Train Loss2: 1.5110556743443013\n",
      "Test Accuracy: 0.8797916666666666\n",
      "Test Accuracy2: 0.9722916666666667\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.5914693107273843\n",
      "Average Train Loss2: 1.5084449854824278\n",
      "Test Accuracy: 0.8783333333333333\n",
      "Test Accuracy2: 0.98\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.5900892759561538\n",
      "Average Train Loss2: 1.5062758719086646\n",
      "Test Accuracy: 0.8877083333333333\n",
      "Test Accuracy2: 0.9791666666666666\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.5889542463367636\n",
      "Average Train Loss2: 1.5045021473494442\n",
      "Test Accuracy: 0.880625\n",
      "Test Accuracy2: 0.97875\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.5880398871302606\n",
      "Average Train Loss2: 1.5029063289910556\n",
      "Test Accuracy: 0.8852083333333334\n",
      "Test Accuracy2: 0.9697916666666667\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.5872927476580327\n",
      "Average Train Loss2: 1.5015395607260558\n",
      "Test Accuracy: 0.8904166666666666\n",
      "Test Accuracy2: 0.9702083333333333\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.5866130427462715\n",
      "Average Train Loss2: 1.500219510793686\n",
      "Test Accuracy: 0.8720833333333333\n",
      "Test Accuracy2: 0.9775\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.585718569588661\n",
      "Average Train Loss2: 1.4991083177487055\n",
      "Test Accuracy: 0.886875\n",
      "Test Accuracy2: 0.978125\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.5853023273348807\n",
      "Average Train Loss2: 1.4980679362304508\n",
      "Test Accuracy: 0.8929166666666667\n",
      "Test Accuracy2: 0.9775\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.5846621827377994\n",
      "Average Train Loss2: 1.497375338975121\n",
      "Test Accuracy: 0.875\n",
      "Test Accuracy2: 0.951875\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.584330527368519\n",
      "Average Train Loss2: 1.496585125717852\n",
      "Test Accuracy: 0.8727083333333333\n",
      "Test Accuracy2: 0.9758333333333333\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.5840382141690506\n",
      "Average Train Loss2: 1.4960565780840422\n",
      "Test Accuracy: 0.8910416666666666\n",
      "Test Accuracy2: 0.9695833333333334\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.583604179161787\n",
      "Average Train Loss2: 1.4953961615145206\n",
      "Test Accuracy: 0.880625\n",
      "Test Accuracy2: 0.979375\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.8439072090387345\n",
      "Average Train Loss2: 1.6801661490797997\n",
      "Test Accuracy: 0.6845833333333333\n",
      "Test Accuracy2: 0.8533333333333334\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.763024336963892\n",
      "Average Train Loss2: 1.6391065615415572\n",
      "Test Accuracy: 0.8622916666666667\n",
      "Test Accuracy2: 0.870625\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.703572669506073\n",
      "Average Train Loss2: 1.6223176568945248\n",
      "Test Accuracy: 0.8822916666666667\n",
      "Test Accuracy2: 0.8679166666666667\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.6726577289253473\n",
      "Average Train Loss2: 1.6139282339662313\n",
      "Test Accuracy: 0.8785416666666667\n",
      "Test Accuracy2: 0.8658333333333333\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6531807182073592\n",
      "Average Train Loss2: 1.6076674257874488\n",
      "Test Accuracy: 0.8795833333333334\n",
      "Test Accuracy2: 0.8770833333333333\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.6400236317118009\n",
      "Average Train Loss2: 1.6029072419802348\n",
      "Test Accuracy: 0.88875\n",
      "Test Accuracy2: 0.8789583333333333\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.6304209785972323\n",
      "Average Train Loss2: 1.5999571135469846\n",
      "Test Accuracy: 0.888125\n",
      "Test Accuracy2: 0.8775\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.6233020589128138\n",
      "Average Train Loss2: 1.5976016981825232\n",
      "Test Accuracy: 0.8829166666666667\n",
      "Test Accuracy2: 0.874375\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.617712763759825\n",
      "Average Train Loss2: 1.5954208785361714\n",
      "Test Accuracy: 0.8860416666666666\n",
      "Test Accuracy2: 0.8733333333333333\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.6129808728039265\n",
      "Average Train Loss2: 1.5934749113798141\n",
      "Test Accuracy: 0.880625\n",
      "Test Accuracy2: 0.88375\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.608967276220972\n",
      "Average Train Loss2: 1.592157250117172\n",
      "Test Accuracy: 0.8866666666666667\n",
      "Test Accuracy2: 0.8783333333333333\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.605932674780488\n",
      "Average Train Loss2: 1.5909163679579894\n",
      "Test Accuracy: 0.8860416666666666\n",
      "Test Accuracy2: 0.88125\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.6032074471024367\n",
      "Average Train Loss2: 1.5898574753036865\n",
      "Test Accuracy: 0.8841666666666667\n",
      "Test Accuracy2: 0.880625\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.6007525044424193\n",
      "Average Train Loss2: 1.589025431134871\n",
      "Test Accuracy: 0.88625\n",
      "Test Accuracy2: 0.8758333333333334\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.5987785903294882\n",
      "Average Train Loss2: 1.5880915381113687\n",
      "Test Accuracy: 0.8860416666666666\n",
      "Test Accuracy2: 0.8839583333333333\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.597069154586643\n",
      "Average Train Loss2: 1.5875814257226883\n",
      "Test Accuracy: 0.8810416666666666\n",
      "Test Accuracy2: 0.87\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.5954828629248283\n",
      "Average Train Loss2: 1.5870554559651544\n",
      "Test Accuracy: 0.8864583333333333\n",
      "Test Accuracy2: 0.876875\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.5942103162209194\n",
      "Average Train Loss2: 1.5863512607945336\n",
      "Test Accuracy: 0.8841666666666667\n",
      "Test Accuracy2: 0.890625\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.592942813976815\n",
      "Average Train Loss2: 1.5858427647916895\n",
      "Test Accuracy: 0.8854166666666666\n",
      "Test Accuracy2: 0.8754166666666666\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.591776974618435\n",
      "Average Train Loss2: 1.585503544357419\n",
      "Test Accuracy: 0.8841666666666667\n",
      "Test Accuracy2: 0.8766666666666667\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.7271251463294028\n",
      "Average Train Loss2: 1.835036882519722\n",
      "Test Accuracy: 0.7716666666666666\n",
      "Test Accuracy2: 0.6879166666666666\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.6975542385280131\n",
      "Average Train Loss2: 1.7814882458448411\n",
      "Test Accuracy: 0.7727083333333333\n",
      "Test Accuracy2: 0.8720833333333333\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.6868471273183823\n",
      "Average Train Loss2: 1.7390445594787598\n",
      "Test Accuracy: 0.7672916666666667\n",
      "Test Accuracy2: 0.873125\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.680726370498538\n",
      "Average Train Loss2: 1.7168956120014192\n",
      "Test Accuracy: 0.77875\n",
      "Test Accuracy2: 0.878125\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.67630094383955\n",
      "Average Train Loss2: 1.702842265713215\n",
      "Test Accuracy: 0.775\n",
      "Test Accuracy2: 0.8775\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.673012563387553\n",
      "Average Train Loss2: 1.6933063281973204\n",
      "Test Accuracy: 0.773125\n",
      "Test Accuracy2: 0.8797916666666666\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.6711374012231828\n",
      "Average Train Loss2: 1.6867585205520903\n",
      "Test Accuracy: 0.7775\n",
      "Test Accuracy2: 0.8720833333333333\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.6690783874168993\n",
      "Average Train Loss2: 1.6814383862018585\n",
      "Test Accuracy: 0.781875\n",
      "Test Accuracy2: 0.8804166666666666\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.6676550634503364\n",
      "Average Train Loss2: 1.6775247021781075\n",
      "Test Accuracy: 0.776875\n",
      "Test Accuracy2: 0.8847916666666666\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.6663110961914063\n",
      "Average Train Loss2: 1.6738475844264031\n",
      "Test Accuracy: 0.7647916666666666\n",
      "Test Accuracy2: 0.881875\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.6654654803763735\n",
      "Average Train Loss2: 1.6715086180134253\n",
      "Test Accuracy: 0.7816666666666666\n",
      "Test Accuracy2: 0.8891666666666667\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.6644264873365562\n",
      "Average Train Loss2: 1.6693018968900044\n",
      "Test Accuracy: 0.775\n",
      "Test Accuracy2: 0.8770833333333333\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.6639063980395978\n",
      "Average Train Loss2: 1.6673663836396657\n",
      "Test Accuracy: 0.7697916666666667\n",
      "Test Accuracy2: 0.8795833333333334\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.6633109711919511\n",
      "Average Train Loss2: 1.6656666073160513\n",
      "Test Accuracy: 0.785\n",
      "Test Accuracy2: 0.8814583333333333\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.6626908355553944\n",
      "Average Train Loss2: 1.664447861278057\n",
      "Test Accuracy: 0.7735416666666667\n",
      "Test Accuracy2: 0.8822916666666667\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.6623108396157622\n",
      "Average Train Loss2: 1.6630605507157743\n",
      "Test Accuracy: 0.7725\n",
      "Test Accuracy2: 0.8804166666666666\n",
      "Start of Epoch 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 1.661788646876812\n",
      "Average Train Loss2: 1.6618691991392305\n",
      "Test Accuracy: 0.78625\n",
      "Test Accuracy2: 0.881875\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.661386869897445\n",
      "Average Train Loss2: 1.6609014891584715\n",
      "Test Accuracy: 0.7785416666666667\n",
      "Test Accuracy2: 0.8833333333333333\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.6609882007805925\n",
      "Average Train Loss2: 1.6599515284048882\n",
      "Test Accuracy: 0.778125\n",
      "Test Accuracy2: 0.8852083333333334\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.6608090828627349\n",
      "Average Train Loss2: 1.6590979658812284\n",
      "Test Accuracy: 0.7808333333333334\n",
      "Test Accuracy2: 0.8820833333333333\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.8353909572958946\n",
      "Average Train Loss2: 1.6326385554671288\n",
      "Test Accuracy: 0.6875\n",
      "Test Accuracy2: 0.9572916666666667\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.7664109604656697\n",
      "Average Train Loss2: 1.5686374396383762\n",
      "Test Accuracy: 0.7820833333333334\n",
      "Test Accuracy2: 0.9670833333333333\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.7342354450821877\n",
      "Average Train Loss2: 1.5436858656605086\n",
      "Test Accuracy: 0.8533333333333334\n",
      "Test Accuracy2: 0.968125\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.69651206356287\n",
      "Average Train Loss2: 1.531223186776042\n",
      "Test Accuracy: 0.8789583333333333\n",
      "Test Accuracy2: 0.979375\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.6729799844145774\n",
      "Average Train Loss2: 1.5230687178492546\n",
      "Test Accuracy: 0.8795833333333334\n",
      "Test Accuracy2: 0.9697916666666667\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.6566939582328002\n",
      "Average Train Loss2: 1.5176205525795619\n",
      "Test Accuracy: 0.8683333333333333\n",
      "Test Accuracy2: 0.9745833333333334\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.6449393704363278\n",
      "Average Train Loss2: 1.5136359856213841\n",
      "Test Accuracy: 0.8847916666666666\n",
      "Test Accuracy2: 0.9795833333333334\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.633061540544033\n",
      "Average Train Loss2: 1.5104727188721299\n",
      "Test Accuracy: 0.9783333333333334\n",
      "Test Accuracy2: 0.973125\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.616557932138443\n",
      "Average Train Loss2: 1.5080529128114382\n",
      "Test Accuracy: 0.980625\n",
      "Test Accuracy2: 0.9620833333333333\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.603031858575344\n",
      "Average Train Loss2: 1.5059419855237006\n",
      "Test Accuracy: 0.9729166666666667\n",
      "Test Accuracy2: 0.9764583333333333\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.5917879097570073\n",
      "Average Train Loss2: 1.5039829941662874\n",
      "Test Accuracy: 0.9775\n",
      "Test Accuracy2: 0.9802083333333333\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.5825066798329352\n",
      "Average Train Loss2: 1.502677038843433\n",
      "Test Accuracy: 0.975625\n",
      "Test Accuracy2: 0.9739583333333334\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.5746064088986471\n",
      "Average Train Loss2: 1.501447085215495\n",
      "Test Accuracy: 0.97625\n",
      "Test Accuracy2: 0.975\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.5678770501783916\n",
      "Average Train Loss2: 1.500293759486505\n",
      "Test Accuracy: 0.9808333333333333\n",
      "Test Accuracy2: 0.9783333333333334\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.5620435627420743\n",
      "Average Train Loss2: 1.4994032587091128\n",
      "Test Accuracy: 0.983125\n",
      "Test Accuracy2: 0.9691666666666666\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.5568941364511848\n",
      "Average Train Loss2: 1.498733582571149\n",
      "Test Accuracy: 0.97875\n",
      "Test Accuracy2: 0.979375\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.552454640504192\n",
      "Average Train Loss2: 1.49810072453583\n",
      "Test Accuracy: 0.9820833333333333\n",
      "Test Accuracy2: 0.979375\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.5485161993073093\n",
      "Average Train Loss2: 1.4975561526980665\n",
      "Test Accuracy: 0.983125\n",
      "Test Accuracy2: 0.9720833333333333\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.5450228687462053\n",
      "Average Train Loss2: 1.49684569555521\n",
      "Test Accuracy: 0.97625\n",
      "Test Accuracy2: 0.9772916666666667\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.541694840490818\n",
      "Average Train Loss2: 1.4965184917896985\n",
      "Test Accuracy: 0.9739583333333334\n",
      "Test Accuracy2: 0.9741666666666666\n",
      "Start of Epoch 20\n",
      "Start of Epoch 0\n",
      "Average Train Loss: 1.9505195366144181\n",
      "Average Train Loss2: 1.6558752481937409\n",
      "Test Accuracy: 0.588125\n",
      "Test Accuracy2: 0.9591666666666666\n",
      "Start of Epoch 1\n",
      "Average Train Loss: 1.8827615966498852\n",
      "Average Train Loss2: 1.5810588934123515\n",
      "Test Accuracy: 0.6047916666666666\n",
      "Test Accuracy2: 0.9535416666666666\n",
      "Start of Epoch 2\n",
      "Average Train Loss: 1.858682825167974\n",
      "Average Train Loss2: 1.555401563803355\n",
      "Test Accuracy: 0.5952083333333333\n",
      "Test Accuracy2: 0.9575\n",
      "Start of Epoch 3\n",
      "Average Train Loss: 1.8459642262607814\n",
      "Average Train Loss2: 1.5410765684247016\n",
      "Test Accuracy: 0.6008333333333333\n",
      "Test Accuracy2: 0.9708333333333333\n",
      "Start of Epoch 4\n",
      "Average Train Loss: 1.8373827916026115\n",
      "Average Train Loss2: 1.5325400221586227\n",
      "Test Accuracy: 0.5985416666666666\n",
      "Test Accuracy2: 0.949375\n",
      "Start of Epoch 5\n",
      "Average Train Loss: 1.833335236777862\n",
      "Average Train Loss2: 1.52648107406497\n",
      "Test Accuracy: 0.5970833333333333\n",
      "Test Accuracy2: 0.975625\n",
      "Start of Epoch 6\n",
      "Average Train Loss: 1.8294369395460401\n",
      "Average Train Loss2: 1.5217931559681892\n",
      "Test Accuracy: 0.6064583333333333\n",
      "Test Accuracy2: 0.9472916666666666\n",
      "Start of Epoch 7\n",
      "Average Train Loss: 1.8267099415063859\n",
      "Average Train Loss2: 1.518700099274516\n",
      "Test Accuracy: 0.5975\n",
      "Test Accuracy2: 0.9675\n",
      "Start of Epoch 8\n",
      "Average Train Loss: 1.8244545401268535\n",
      "Average Train Loss2: 1.5157296900020705\n",
      "Test Accuracy: 0.6039583333333334\n",
      "Test Accuracy2: 0.965\n",
      "Start of Epoch 9\n",
      "Average Train Loss: 1.8223856720030307\n",
      "Average Train Loss2: 1.5140011536598206\n",
      "Test Accuracy: 0.6122916666666667\n",
      "Test Accuracy2: 0.9554166666666667\n",
      "Start of Epoch 10\n",
      "Average Train Loss: 1.8208478978763927\n",
      "Average Train Loss2: 1.5120416766838594\n",
      "Test Accuracy: 0.5927083333333333\n",
      "Test Accuracy2: 0.9754166666666667\n",
      "Start of Epoch 11\n",
      "Average Train Loss: 1.8193622222691774\n",
      "Average Train Loss2: 1.5104114831437667\n",
      "Test Accuracy: 0.6070833333333333\n",
      "Test Accuracy2: 0.9775\n",
      "Start of Epoch 12\n",
      "Average Train Loss: 1.81821273455253\n",
      "Average Train Loss2: 1.508937273818713\n",
      "Test Accuracy: 0.604375\n",
      "Test Accuracy2: 0.9660416666666667\n",
      "Start of Epoch 13\n",
      "Average Train Loss: 1.8172146126925945\n",
      "Average Train Loss2: 1.5079241035921234\n",
      "Test Accuracy: 0.6033333333333334\n",
      "Test Accuracy2: 0.96375\n",
      "Start of Epoch 14\n",
      "Average Train Loss: 1.8164467752377191\n",
      "Average Train Loss2: 1.5071672391732533\n",
      "Test Accuracy: 0.581875\n",
      "Test Accuracy2: 0.9397916666666667\n",
      "Start of Epoch 15\n",
      "Average Train Loss: 1.8154088203832508\n",
      "Average Train Loss2: 1.5067834553942083\n",
      "Test Accuracy: 0.6025\n",
      "Test Accuracy2: 0.974375\n",
      "Start of Epoch 16\n",
      "Average Train Loss: 1.8150208912141184\n",
      "Average Train Loss2: 1.505984904366381\n",
      "Test Accuracy: 0.609375\n",
      "Test Accuracy2: 0.9679166666666666\n",
      "Start of Epoch 17\n",
      "Average Train Loss: 1.8144593849579493\n",
      "Average Train Loss2: 1.5055207360784213\n",
      "Test Accuracy: 0.6052083333333333\n",
      "Test Accuracy2: 0.963125\n",
      "Start of Epoch 18\n",
      "Average Train Loss: 1.8137556741363123\n",
      "Average Train Loss2: 1.5048876137545235\n",
      "Test Accuracy: 0.594375\n",
      "Test Accuracy2: 0.97375\n",
      "Start of Epoch 19\n",
      "Average Train Loss: 1.8132914237886668\n",
      "Average Train Loss2: 1.5044099895685912\n",
      "Test Accuracy: 0.5995833333333334\n",
      "Test Accuracy2: 0.97125\n",
      "Start of Epoch 20\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    results.append(train_enc2dec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69375, 0.791875, 0.9829166666666667, 0.9825, 0.7945833333333333, 0.8929166666666667, 0.88875, 0.78625, 0.983125, 0.6122916666666667]\n",
      "[0.98375, 0.480625, 0.8877083333333333, 0.9829166666666667, 0.9804166666666667, 0.9816666666666667, 0.890625, 0.8891666666666667, 0.9802083333333333, 0.9775]\n",
      "0.8408958333333334\n",
      "0.9034583333333334\n",
      "0.8721770833333332\n"
     ]
    }
   ],
   "source": [
    "''' Return value from one training session\n",
    "results.append(train_enc2dec())\n",
    "\n",
    "result = \n",
    "[\n",
    "  [encdec_accuracy(encoder, decoder, inf_test_1, test_steps_per_epoch*2), (train_losses, test_accs)],\n",
    "  [encdec_accuracy(encoder, decoder2, inf_test_2, test_steps_per_epoch*2), (train_losses2, test_accs2)]\n",
    "]\n",
    "\n",
    "20 epochs trained 20 times, with two models per training. So 40 models each trained for 20 epochs.\n",
    "'''\n",
    "\n",
    "max_accs = []\n",
    "max_accs2 = []\n",
    "\n",
    "for result in results:\n",
    "    max_accs.append(max(result[0][1][1]))\n",
    "    max_accs2.append(max(result[1][1][1]))\n",
    "\n",
    "print(max_accs)\n",
    "print(max_accs2)\n",
    "print(np.mean(max_accs))\n",
    "print(np.mean(max_accs2))\n",
    "print(np.mean(max_accs + max_accs2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
