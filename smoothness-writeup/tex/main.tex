%%%%%%%%%
% FLAGS %
%%%%%%%%%

% Is this an internal version? (i.e., not public-facing.)
\ifdefined \isinternal \else \def \isinternal{1} \fi

% Is this an extended version? (e.g., includes appendix.)
\ifdefined \isextended \else \def \isextended{1} \fi

% Should authors be identified? (e.g., not submitted for blind peer review.)
\ifdefined \isauthorid \else \def \isauthorid{1} \fi

% Defaults can be overridden from the Makefile
\newif \ifinternal \if \isinternal 0 \internalfalse \else \internaltrue \fi
\newif \ifextended \if \isextended 0 \extendedfalse \else \extendedtrue \fi
\newif \ifauthorid \if \isauthorid 0 \authoridfalse \else \authoridtrue \fi

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\documentclass{article}

\usepackage[T5,T1]{fontenc}

\usepackage{longtable}

\input{darais-latex-imports}
\input{darais-latex-macros}

\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\begin{document}

\section{A Tiny Model}

A single layer neural net implements essentially just a single multiplication.

We have an input vector ⸨x⸩, a weights vector ⸨θ⸩, and the
classification function ⸨f⸩ is:
M⁅ f⸤θ⸥(x) ≜ xθᵀ M⁆

Here is the mean squared error loss function over scalars:
M⁅ L(a,b) ≜ (a - b)² M⁆
and the loss of our function ⸨f⸩ {w.r.t.} a training example ⸨y⸩ is:
M⁅ L⸤f⸤θ⸥⸥(x,y) Aː[t]rcl
                A⁅ ⧼≜⧽ (f⸤θ⸥(x) - y)² 
                A⁃ ⧼=⧽ (xθᵀ - y)²
                A⁆
M⁆
The derivative of the loss is:
M⁅ \frac{∂L⸤f⸤θ⸥⸥(x,y)}{∂θᵀ} Aː[t]rcl
                             A⁅ ⧼≜⧽ \frac{∂(xθᵀ - y)²}{∂θᵀ}
                             A⁃ ⧼=⧽ 2(xθᵀ - y)\frac{∂(xθᵀ - y)}{∂θᵀ}
                             A⁃ ⧼=⧽ 2(xθᵀ - y)(\frac{∂(xθᵀ)}{∂θᵀ} - \frac{∂y}{∂θᵀ})
                             A⁃ ⧼=⧽ 2(xθᵀ - y)\frac{∂(xθᵀ)}{∂θᵀ}
                             A⁃ ⧼=⧽ 2x(xθᵀ - y)\frac{∂θᵀ}{∂θᵀ}
                             A⁃ ⧼=⧽ 2x(xθᵀ - y)
                             A⁆
M⁃
M⁃ \frac{∂L⸤f⸤θ⸥⸥(x,y)}{∂θᵀ} Aː[t]rcl
                             A⁅ ⧼≜⧽ \frac{∂(xθᵀ - y)²}{∂θᵀ}
                             A⁃ ⧼=⧽ \frac{∂((xθᵀ)² - 2xyθᵀ - y²)}{∂θᵀ}
                             A⁃ ⧼=⧽ \frac{∂((xθᵀ)²)}{∂θᵀ} - \frac{∂(2xyθᵀ)}{∂θᵀ} - \frac{∂(y²)}{∂θᵀ}
                             A⁃ ⧼=⧽ \frac{∂((xθᵀ)²)}{∂θᵀ} - 2xy\frac{∂θᵀ}{∂θ}
                             A⁃ ⧼=⧽ 2x²θᵀ\frac{∂θᵀ}{∂θᵀ} - 2xy\frac{∂θᵀ}{∂θᵀ}
                             A⁃ ⧼=⧽ 2x²θᵀ - 2xy
                             A⁃ ⧼=⧽ 2x(xθᵀ - y)
                             A⁃ ⧼=⧽ ... 2xᵀ(xθᵀ - y)
                             A⁆
M⁆



\end{document}
\endinput
