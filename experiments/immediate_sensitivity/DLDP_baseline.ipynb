{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import yappi\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(7)\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\n",
    "\n",
    "%matplotlib inline\n",
    "print(torch.__version__) \n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for loading up the dataset\n",
    "\n",
    "def load_adult_data(path):\n",
    "    column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "                    'martial_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                    'capital_gain', 'capital_loss', 'hours_per_week', 'country', 'target']\n",
    "    input_data = (pd.read_csv(path, names=column_names,\n",
    "                              na_values=\"?\", sep=r'\\s*,\\s*', engine='python')\n",
    "                  .loc[lambda df: df['race'].isin(['White', 'Black'])])\n",
    "\n",
    "    # targets; 1 when someone makes over 50k , otherwise 0\n",
    "    y = (input_data['target'] == '>50K').astype(int)\n",
    "\n",
    "    # features; note that the 'target' and sentive attribute columns are dropped\n",
    "    X = (input_data\n",
    "         .drop(columns=['target', 'fnlwgt'])\n",
    "         .fillna('Unknown')\n",
    "         .pipe(pd.get_dummies, drop_first=True))\n",
    "\n",
    "    y = y.to_frame()\n",
    "    for col in X.columns:\n",
    "        X[col] = X[col].astype('float32')\n",
    "\n",
    "    for col in y.columns:\n",
    "        y[col] = y[col].astype('float32')\n",
    "\n",
    "    print(f\"features X: {X.shape[0]} samples, {X.shape[1]} attributes\")\n",
    "    print(f\"targets y: {y.shape} samples\")\n",
    "    return X, y\n",
    "\n",
    "class PandasDataSet(TensorDataset):\n",
    "    def __init__(self, *dataframes):\n",
    "        tensors = (self._df_to_tensor(df) for df in dataframes)\n",
    "        super(PandasDataSet, self).__init__(*tensors)\n",
    "\n",
    "    def _df_to_tensor(self, df):\n",
    "        if isinstance(df, pd.Series):\n",
    "            df = df.to_frame('dummy')\n",
    "        return torch.from_numpy(df.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features X: 30940 samples, 95 attributes\n",
      "targets y: (30940, 1) samples\n",
      "        age  education_num  capital_gain  capital_loss  hours_per_week  \\\n",
      "27719  17.0            7.0           0.0           0.0            12.0   \n",
      "936    47.0           14.0           0.0           0.0            25.0   \n",
      "3936   46.0           11.0           0.0           0.0            38.0   \n",
      "8500   45.0           14.0           0.0        1902.0            50.0   \n",
      "3882   51.0           10.0           0.0           0.0            40.0   \n",
      "\n",
      "       workclass_Local-gov  workclass_Never-worked  workclass_Private  \\\n",
      "27719                  0.0                     0.0                1.0   \n",
      "936                    0.0                     0.0                1.0   \n",
      "3936                   0.0                     0.0                0.0   \n",
      "8500                   0.0                     0.0                1.0   \n",
      "3882                   0.0                     0.0                1.0   \n",
      "\n",
      "       workclass_Self-emp-inc  workclass_Self-emp-not-inc  ...  \\\n",
      "27719                     0.0                         0.0  ...   \n",
      "936                       0.0                         0.0  ...   \n",
      "3936                      0.0                         0.0  ...   \n",
      "8500                      0.0                         0.0  ...   \n",
      "3882                      0.0                         0.0  ...   \n",
      "\n",
      "       country_Puerto-Rico  country_Scotland  country_South  country_Taiwan  \\\n",
      "27719                  0.0               0.0            0.0             0.0   \n",
      "936                    0.0               0.0            0.0             0.0   \n",
      "3936                   0.0               0.0            0.0             0.0   \n",
      "8500                   0.0               0.0            0.0             0.0   \n",
      "3882                   0.0               0.0            0.0             0.0   \n",
      "\n",
      "       country_Thailand  country_Trinadad&Tobago  country_United-States  \\\n",
      "27719               0.0                      0.0                    1.0   \n",
      "936                 0.0                      0.0                    1.0   \n",
      "3936                0.0                      0.0                    1.0   \n",
      "8500                0.0                      0.0                    1.0   \n",
      "3882                0.0                      0.0                    1.0   \n",
      "\n",
      "       country_Unknown  country_Vietnam  country_Yugoslavia  \n",
      "27719              0.0              0.0                 0.0  \n",
      "936                0.0              0.0                 0.0  \n",
      "3936               0.0              0.0                 0.0  \n",
      "8500               0.0              0.0                 0.0  \n",
      "3882               0.0              0.0                 0.0  \n",
      "\n",
      "[5 rows x 95 columns]\n"
     ]
    }
   ],
   "source": [
    "# load adult data set\n",
    "path = 'adult.data'\n",
    "# path = 'adult.data'\n",
    "X, y = load_adult_data(path)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# split into train/test set\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, stratify=y, random_state=7)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 24752\n",
      "# testing samples: 6188\n",
      "# batches: 247\n",
      "# training samples: 6188\n",
      "# batches: 61\n"
     ]
    }
   ],
   "source": [
    "# Set up training & testing data\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = PandasDataSet(X_train, y_train)\n",
    "test_data = PandasDataSet(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "print('# training samples:', len(train_data))\n",
    "print('# testing samples:', len(test_data))\n",
    "print('# batches:', len(train_loader))\n",
    "\n",
    "print('# training samples:', len(test_data))\n",
    "print('# batches:', len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, test_loader):\n",
    "    correct = 0\n",
    "\n",
    "    for examples, labels in test_loader:\n",
    "        output = model.forward(examples)\n",
    "        batch_correct = torch.sum(torch.abs(output - labels) < 0.5)\n",
    "        correct += batch_correct\n",
    "\n",
    "    acc = float(correct)/len(test_data)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden=32, p_dropout=0.2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(n_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.network(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten and reconstruct network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(grads):\n",
    "    shapes = [g.shape for g in grads]\n",
    "    flats = [x.view(-1) for x in grads]\n",
    "    lens = [len(f) for f in flats]\n",
    "    view = torch.cat(flats)\n",
    "    return view, shapes, lens\n",
    "\n",
    "def reshape(view, shapes, lens):\n",
    "    i = 0\n",
    "    tensors = []\n",
    "    for s, l in zip(shapes, lens):\n",
    "        flat = view[i: i + l]\n",
    "        tensors.append(flat.view(s))\n",
    "        i += l\n",
    "    return tensors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd_hacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate DP Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_grad(model, loss, C):\n",
    "    # do gradient for each element in loss\n",
    "    first_order_grads = [torch.autograd.grad([l], model.parameters(), retain_graph=True, create_graph=True) for l in loss]\n",
    "    shapes = []\n",
    "    lens = []\n",
    "    views = []\n",
    "    # flatten views out per sample\n",
    "    for f in first_order_grads:\n",
    "        v, shapes, lens = flatten(f)\n",
    "        views.append(v)\n",
    "    #views = [torch.cat([x.view(-1) for x in f]) for f in first_order_grads]\n",
    "\n",
    "\n",
    "    # a norm for every sample\n",
    "    grad_l2_norms = [torch.norm(v, p = 2) for v in views]\n",
    "    \n",
    "    # divisors turned out to be recipricol multiplication\n",
    "    divisors = [C/norm.item() if norm.item() > C else 1 for norm in grad_l2_norms]\n",
    "        \n",
    "    # the part where we clip\n",
    "    clipped_grads = [v*d for v, d in zip(views, divisors)]\n",
    "    \n",
    "    # sum of gradients\n",
    "    cg_sum = torch.stack(clipped_grads, dim=0).sum(dim=0)\n",
    "    \n",
    "    # reshape per model shape\n",
    "    cgs = reshape(cg_sum, shapes, lens)\n",
    "    return cgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(epsilon):\n",
    "    # reset the model\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(7)\n",
    "    model = Classifier(n_features=n_features)\n",
    "    # reduction = none makes sure that we get per sample loss\n",
    "    model_criterion = nn.BCELoss(reduction='none')\n",
    "    model_optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "    # number of epochs and iterations\n",
    "    epochs = 10\n",
    "    iters = epochs * BATCH_SIZE\n",
    "\n",
    "    # parameters for Renyi differential privacy\n",
    "    alpha = 2\n",
    "    epsilon_iter = epsilon / iters\n",
    "    C = 1.5\n",
    "    \n",
    "    # plotting criteria\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "        for x_batch_train, y_batch_train in train_loader:\n",
    "            model_optimizer.zero_grad()\n",
    "            inp = Variable(x_batch_train, requires_grad=True)\n",
    "    \n",
    "            outputs = model.forward(inp)\n",
    "            loss = model_criterion(outputs, y_batch_train)\n",
    "            clipper = clipped_grad(model, loss, C)\n",
    "            # we have to average the losses because they are per sample now\n",
    "            mean_loss = loss.mean()\n",
    "            mean_loss.backward()\n",
    "            train_losses.append(mean_loss) \n",
    "\n",
    "            # this is the scale of the Gaussian noise to be added to the batch gradient\n",
    "            sigma = np.sqrt((C**2 * alpha) / (2 * epsilon_iter))\n",
    "\n",
    "            count = 0\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p.grad = clipper[count] + (sigma * torch.randn(1))\n",
    "                    count += 1\n",
    "\n",
    "            model_optimizer.step()\n",
    "\n",
    "\n",
    "        test_accs.append(accuracy(model, test_loader))\n",
    "        print(\"Accuracy:\", test_accs[-1])\n",
    "    return accuracy(model, test_loader), (train_losses, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### autograd_hacks version\n",
    "\n",
    "\n",
    "def clipped_autograd(model, C):\n",
    "    clipped_grads = []\n",
    "    max_norms = []\n",
    "    for param in model.parameters():\n",
    "        t1 = torch.transpose(param.grad1, 0, 1)\n",
    "        if len(param.grad.shape) == 1:\n",
    "            d = 1\n",
    "            t = t1\n",
    "        else:\n",
    "            d = (1,2)\n",
    "            t = torch.transpose(t1, 1, 2)\n",
    "        norms = param.grad1.data.norm(dim=d)\n",
    "        torch.equal(norms, param.grad1.norm(dim=d))\n",
    "        max_norms.append(torch.max(norms))\n",
    "        c_norms = C / norms \n",
    "        \n",
    "        clips_with_ones = torch.minimum(c_norms, torch.ones(c_norms.shape))\n",
    "        clipped_grads.append(torch.matmul(t, clips_with_ones))\n",
    "        \n",
    "    return clipped_grads, max(max_norms)\n",
    "\n",
    "def run_experiment_autograd(epsilon, C=1.5):\n",
    "    # reset the model\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(7)\n",
    "    model = Classifier(n_features=n_features)\n",
    "    # reduction = none makes sure that we get per sample loss\n",
    "    model_criterion = nn.BCELoss()\n",
    "    model_optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "    autograd_hacks.add_hooks(model)\n",
    "\n",
    "    # number of epochs and iterations\n",
    "    epochs = 10\n",
    "    iters = epochs * BATCH_SIZE\n",
    "\n",
    "    # parameters for Renyi differential privacy\n",
    "    alpha = 2\n",
    "    epsilon_iter = epsilon / iters\n",
    "    \n",
    "    # plotting criteria\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    max_norms = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "        for x_batch_train, y_batch_train in train_loader:\n",
    "            model_optimizer.zero_grad()\n",
    "            inp = Variable(x_batch_train, requires_grad=True)\n",
    "            outputs = model.forward(inp)\n",
    "            loss = model_criterion(outputs, y_batch_train)\n",
    "            # we have to average the losses because they are per sample now\n",
    "            loss.backward()\n",
    "            autograd_hacks.compute_grad1(model)\n",
    "            clipper, mn = clipped_autograd(model, C)\n",
    "            max_norms.append(mn)\n",
    "            autograd_hacks.clear_backprops(model)\n",
    "            train_losses.append(loss) \n",
    "\n",
    "            # this is the scale of the Gaussian noise to be added to the batch gradient\n",
    "            sigma = np.sqrt((C**2 * alpha) / (2 * epsilon_iter))\n",
    "\n",
    "            count = 0\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p.grad = clipper[count] + (sigma * torch.randn(1))\n",
    "                    count += 1\n",
    "\n",
    "            model_optimizer.step()\n",
    "\n",
    "\n",
    "        test_accs.append(accuracy(model, test_loader))\n",
    "        print(\"Accuracy:\", test_accs[-1])\n",
    "    return accuracy(model, test_loader), (train_losses, test_accs, max_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Accuracy: 0.5906593406593407\n",
      "Start of epoch 1\n",
      "Accuracy: 0.7596961861667744\n",
      "Start of epoch 2\n",
      "Accuracy: 0.7569489334195216\n",
      "Start of epoch 3\n",
      "Accuracy: 0.7546864899806076\n",
      "Start of epoch 4\n",
      "Accuracy: 0.7498383968972204\n",
      "Start of epoch 5\n",
      "Accuracy: 0.7558177117000646\n",
      "Start of epoch 6\n",
      "Accuracy: 0.7529088558500323\n",
      "Start of epoch 7\n",
      "Accuracy: 0.7478991596638656\n",
      "Start of epoch 8\n",
      "Accuracy: 0.7672915319974144\n",
      "Start of epoch 9\n",
      "Accuracy: 0.7668067226890757\n"
     ]
    }
   ],
   "source": [
    "exper = run_experiment_autograd(10, C=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11736.4648)\n"
     ]
    }
   ],
   "source": [
    "acc, tup = exper\n",
    "tl, ta, mn = tup\n",
    "print(max(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-a66f0ee54949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#yappi.set_clock_type(\"cpu\")  # Use set_clock_type(\"wall\") for wall time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#yappi.start()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexperiment_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrun_experiment_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#yappi.get_func_stats().print_all()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#yappi.get_thread_stats().print_all()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-a66f0ee54949>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#yappi.set_clock_type(\"cpu\")  # Use set_clock_type(\"wall\") for wall time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#yappi.start()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexperiment_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrun_experiment_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#yappi.get_func_stats().print_all()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#yappi.get_thread_stats().print_all()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-283fc0a7fe53>\u001b[0m in \u001b[0;36mrun_experiment_autograd\u001b[0;34m(epsilon)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mautograd_hacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_grad1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mclipper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclipped_autograd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mmax_norms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mautograd_hacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_backprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-283fc0a7fe53>\u001b[0m in \u001b[0;36mclipped_autograd\u001b[0;34m(model, C)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclipped_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \"\"\"\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \"\"\"\n\u001b[1;32m   1112\u001b[0m         gen = self._named_members(\n\u001b[0;32m-> 1113\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilons = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "#yappi.set_clock_type(\"cpu\")  # Use set_clock_type(\"wall\") for wall time\n",
    "#yappi.start()\n",
    "experiment_results = [run_experiment_autograd(epsilon)[0] for epsilon in epsilons]\n",
    "#yappi.get_func_stats().print_all()\n",
    "#yappi.get_thread_stats().print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEZCAYAAADCJLEQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvk0lEQVR4nO3deXydZZ338c9J0ixtli5JysgItBR+7CAUadmaKiLzIIhsLePuOOrIOAyOAqM4CqiIMs+MiiMOPkPRYaRlEwcRnJGmBQqUChSk8KMtlEWhSds0Tdrs5zx/XPdJT0+TNCdNcpZ+369XXnfOdV/3fV+ny/mda48lEglERERyTVG2CyAiIjIQBSgREclJClAiIpKTFKBERCQnKUCJiEhOKsl2AQqFmfUSAv62bJdFRCSPVANxd98tHilAjZ4iIFZVVVWT7YKIiOSLtrY2GKQ1TwFq9GyrqqqqWbVqVbbLISKSN2bPnk1bW9uALU/qgxIRkZykACUiIjlJAUpERHKSApSIiOQkBSgREclJClAiIpKTFKBERGTE3trSxer17YzF1k2aByUiIhl79e0OFi9t4pHntxJPwDc/OYMTDq0e1WcoQImIyLC99Pp27mhs4skXd86tfce0Ug6YXj7qz1KAEhGRISUSCVa/0s7ipU08u769P/2d9WUsmFfPvGOnUFIcG/XnKkCJiMiAEokET760jcVLm3jpjR396bPeUcGC+fWcfEQNRUWjH5iSFKBERGQXffEEj/5hK4uXNvHq25396UceNImFDfWccGgVsdjYBaYkBSgREQGgpzfOw8+2cGdjE3/c3N2ffsIhVSycX89RMyrHtTwKUCIi+7iunjgPPrWZu5c309za059+ypE1LJhfzyH7T8xKuRSgRET2Uds7+/j1k5u599Fmtrb3AlBUBPOPncJF8+o5cAxG5mVCAUpEZB/Tur2X+1Zs4r9XbKK9sw+AkuIYZ54wlQvn1fFnU8uyXMJAAUpEZB+xeVsP9zzSzAMrN9PZHQegbEIRZ580jfNPq2Na9YQsl3BXClAiIgXu7S1d3Lm8md+u2kJvX1iSqLK8mHNOruWDJ9dSMyk3Q0FulkpERPbaaxs7uXNZE0tXtxAPFSZqJpVw/qm1nD2nlknlxdkt4B4oQImIFJi1f9zB4qVNrFjTSnIN17qaCVxweh3vnz2N8tL8WCdcAUpEpED84dV27ljaxO/XtvWn7T+tlIsa6nnPcVOYUJIfgSlJAUpEJI8lEgl+/3IbdzQ28cKG7f3pB+1XzoKGek47ejLFY7gc0VhSgBIRyUPxeIIVa1pZvLSJdX/q6E+3d05k4fx6TjqselyWIxpLClAiInmkty/BstUtLF7WxBtNXf3pxx5cycL59Rw7szLvA1OSApSISB7o7onzP09v4c5lzWxs2blO3kmHV7OgoZ7DD5iUxdKNDQUoEZEc1tHVxwMrN3PPI81saYuWI4rBaUdPZkFDPTP+rCLLJRw7ClAiIjmoraOXX63YxH2PbaKtIyxHVFwE7z1+KhfPq2f/2txYjmgsKUCJiOSQlrYe7n2smfuf2ExHV5hdW1oS46wTp3HB6XXUTy7NcgnHjwKUiEgOaNrazV3Lm3joqS1094bZtRVlRZwzp5YPnVrL5MrcWidvPChAiYhk0ZvNXSxZtpGHn2mhL1qOqKqimPNOqeOck6dRVbHvfkzvu+9cRCSLXnmrg8WNTTz6/Fbi0XJEU6tKuOC0ev7i3VOpKMvtdfLGQ9YDlJldAlwNzAQ2ANe7+88GybsI+Phg93L3WJSvErgOOB+YAjwNfNndn0q732XAF4D9gReBr7r7b/buHYmIDO7F17dzx9ImVr60rT9tvymlXDSvnjNOmEJpni1HNJayGqDM7CLgduD7wIPAecBtZrbD3e8a4JLrgJvT0mYBtwH/npL278A5wFXAWuAfgIfN7Fh3fyV69peB64FvAL8H/gr4lZmd7u6Pj8obFBEhLEf07Pp2Fi9tYvUr7f3p76wvY2HDdOYdM5ni4sKYXDuasl2Duh5Y4u6XR68fMrOphEC0W4By9/XA+uRrMysGfgisBi6L0iqAi4Fr3f1HUdoKoAn4KHCNmU0Cvgrc6O7fjPI8CKwA/gn4i9F/qyKyr4nHE6x8aRt3NDbhb+zoTz9k/woWNNQz94gaivJ0nbzxkLUAZWYzgYOBf0w7dRdwsZnNcPdX93CbzwHHA3PdPTm1uhQoAtpS8m0HOoFp0euTgBrg7mQGd0+Y2T3At82sNOV+IiIZ6YsneOS5rSxe1sSGtzv70486aBIL59dz/CFVBbMc0VjKZg3qsOjoaenroqMBgwaoqJ/pGuDn7r4yme7urWZ2G/D3ZvZodL8rgSrgjmE8u4TQH/ZSRu9GRPZ5Pb1xHn6mhSXLmvjT5p3fcWcfWsWChnqOmlGZxdLln2wGqJrouC0tPVnzqd7D9Z8iDID49gDnvgI8ACQDVwL4tLuvSHt2W9p1w322iEi/zu44Dz21mbseaWZTaw8AsRicfGQNCxrqOWT/iVkuYX7KZoBK1m8Tg6TH93D9pcB97v5yaqKZ1QNPAl3AXwIbgQuBn5hZu7sviZ6R/txMni0iwvbOPu5/YhP3PrqJ1u3ROnlFMP/YKVzcUM8B9eVZLmF+y2aAao2O6bWVqrTzuzGzY4BDCaP00n0a+HNgVnLEHmEE32TgJjO7K7p3DKhk11rUHp8tItK6vZf7HmvmV49vYntn+D5bUhzjzNlTuej0OvabWvjr5I2HbAaoZP/PLOD5lPRZaecH8gGgndCMl+5A4K2U4JS0HLgEqEt79jNpz+4CXttT4UVk37OptYd7HmnigZVb6OoJgam8tIj/c9I0zj+1jmnV+95yRGMpawHK3deZ2auE5rd7U05dAKx199eHuHwOsMrduwY458CnzewQd1+bkj6X0N+1hTCcfHv07GcAzCxGmNi7XCP4RCTVW1u6uHNZE//z+xZ6+0LvQGV5MeeeXMsHT66lelK2Z+wUpmz/qV4L3GpmLcD9wLmEOUwLAcysjjAUfY27pw6mOJqBa08A/4+wOsQDZvZ1wvync4GPAVe6ew/QY2Y3Al8zs17gCcKgixOAhlF9hyKSt17b2MmSxiYan2shHvVMT64s4UOn1nH2SdOYVK7liMZSVgOUuy8yszLgS4S+o1eAj7n74ijL2cCtwHygMeXS6UDLIPdsNbNTgRsIk3jLCMsYXeLud6RkvQboBT4DXAGsAc5198dG592JSL56+c0dLG5sYsULO7uj62omcOHp9bz/xKmUTdByROMhlkgMNJhNMmVmW6uqqmpWrVqV7aKIyAg9/2o7dyzdyNNrdy5HtP+0Ui5umM784yYzQevkjbrZs2fT1tbW6u6T089lu4lPRCSrEokEq15uY3FjEy9s2N6fPvPPylnQMJ1TjqqhWMsRZYUClIjsk+LxBI+90MrixibW/6mjP/2wd05k4fzpvPswLUeUbQpQIrJP6e1LsPTZFu5c1sQbzTsHAh93cCUL50/nmJmTFJhyhAKUiOwTunvi/Pb3W7hreTMbW3bOJJlzeDULGuo57IBJWSydDEQBSkQKWkdXH79+cjP3PNpMS1u0HFEMTj9mMhc31DNjv4osl1AGowAlIgWpbUcvv1qxiftWbKKtow8IyxG9911TuGhePfvXajmiXKcAJSIFZUtbD/c+2syvn9hMR3eYXVs2IcZZJ07jgtPqqJtcmuUSynApQIlIQdjY0s3dy5t4aNUWunvD/M6JZUWcM7eW806pZXKl1snLNwpQIpLX3mzuZMmyJh5+poW+aDmi6onFnHdKHefMraWyQssR5SsFKBHJS+v/1MHixo08+odWkgviTKsu4YLT6vmLd0+lvFSBKd8pQIlIXlnz2nYWL93ISt+5ldt+U0u5aF49Zxw/hVItR1QwFKBEJOclEgmeXR/WyXvulZ3LER1QX8aChunMO2YyxcWaXFtoFKBEJGfF4wmefGkbi5c24W/u6E8/ZP8KFsyfztzDqynSOnkFSwFKRHJOX1+C5c9vZUljExs2dvanHzVjEgsbpnP8IZVajmgfoAAlIjmjuzfO754O6+S9tWXnckSzD61iwfx6jjqoMoulk/GmACUiWdfZHefBpzZz1/JmNm/rASAWg1OOrGFBQz2z9p+Y5RJKNihAiUjWbO/s478f38S9jzWzbXtYjqioCN5zXFiO6ID68iyXULJJAUpExt3W9l5++Vgz//34JnZ0hdm1E0pinHnCVC6aV8/0KVqOSBSgRGQcNbd2c88jzfxm5Ra6ekJgKi8t4uyTpnH+qXVMrdZyRLKTApSIjLk/be7izmVN/O/TLfT2hWUfKiuK+eDJtZw7t5bqSfookt3pX4WIjJkNb3ewuLGJ5c9tJR4tRzSlsoTzT6vj/5w0jYllWo5IBqcAJSKjzt/YweLGjTy+Zlt/Wv3kCVx4ej1nzp5K2QQtRyR7pgAlIqMikUjw/KvbuWPpRp5Z196fvn9tGQsa6pl/3BRKtByRZEABSkT2SiKR4ClvY3HjRta8tnM5opl/Vs7C+dM5+cgairUckYyAApSIjEhfPMGKF1q5Y+lGXnlr53JEhx8wkYXzp3OiVWk5ItkrClAikpHevgRLn21hybIm3mzu6k9/16xKFs6fztEzJikwyahQgBKRYenqifM/q7Zw5/Immrb29KfPPaKaBQ3TsXdqOSIZXQpQIjKkHV19PPDkZu55pJmW9l4AimJw+jGTubihnhn7VWS5hFKoFKD2MTu6+vhdNFmysqKYSeXFVFYU7/J7RWmR9tgR2nb0ct+KTdy3YhPtHWGdvJLiGGccP4WLTq/nHbVlWS6hFLqMApSZ3QH8F/Abd+/ZU37JPfet2MTPfvv2kHliMZhUVsykimImlRdRVVHS/3tleUjvP6b+Hh0rSovUB5HHtmzr4Z5Hm3ngyc10dIfliMomxDjrxGlccHoddTVaJ0/GR6Y1qNOBi4BWM7ubEKwa3T0x6iWTMfF6tPlbZUUxE8uKaOvooyNarDMpkYD2zj7aO/tG9Iyiop0BriqqmU1KD2QD1NzCsYiyCQpw2bCxpZu7ljfx0Kot9PSG/9ITy4o4Z24t551Sx+RKNbjI+Mr0X9z+QAOwEPgQ8ClgY1SzusPdV45u8WS0NW0Nm8B98ORaPnLGfkDYvXRHVwhI2ztSjh3pafFwjNLaOsLvnd27Brh4HNqi80PX1QZWUhwLwaq8KKq5FQ/cHDlA4KusKKZUqxRk5I2mTpYsa2Lpsy30RX+V1ZOKOe+UOs6ZU0tlhZYjkuzIKEBFNaWlwFIz+zxwJrAA+ChwmZm9SqhV3e7uPtqFlb3XHI2+qqvZuWp0cXGMqoklVE0c2Tfk3r5Ef9BKBrD2KMAlf089bu+Mglv0uqsnsdv9Wrf30rp9ZO9xQkmsP2jtEsh2a44sSqnNlVBZUcSk8mImlOwbAW7dn3aweGkTj73QSiL6K5hWPYELTqvjL949lfJSBSbJrhHX2d29D/gN8BszOxS4hhCsrga+amZPAje4+32jUlLZa319if7dSusmj14/QklxjJpJJdSMcEXq7t44Ozr7aO+IR4Gsd9caW0pNrj+4pdTikqtjJ/X0Jmhp7+0fcZapsgmxXZold2+GHKBWlxIQc305nxc2hOWIVr3c1p+239RSLp5Xz3uPn0LpPhKgJfeNOECZ2RGE/qiLgcOAXuDXwO1AAvgscI+ZfcPdrxuFsspe2tzW07+idP3k3Nl3p7SkiNLKIiZXjuz67p744M2SqTW3jl2DW3tnnPaO3v5mraSungRdPb1saRtZgCsvLRqkObJot+bI9CbKieXFY7IsUCKR4Om17Sxu3Mjzr+6smh44vZwFDfWcfvRkinM8sMq+J9NRfIcRAtLFwOFR8mPApcASd9+Skn2xmT0BfBEYNECZ2SWEWtdMYANwvbv/bJC8i4CPD3Yvd4+Z2TeArw/xNg5y99fMbAJwJfAJYD9gDXC1u/92iGvzWrL/CaC2gEZilU4oYuqEIqZWZR50E4kEXT2JwZslBwh86efjaQGusztOZ3e8v7aaqYqyot0HlwzWD5d2nFi26xSBeDzBEy9u446lG1n7x47+9EP2r2Dh/OnMObxaUwokZ2Vag1oTHZ8HvgL8l7u/MUT+N4BBJ0uY2UWEGtf3gQeB84DbzGyHu981wCXXATenpc0CbgP+PXr90+heqaYBdxL6z5LlvQa4Avga8BQhUD1gZqe5++NDvKe8tSnqf6qeVEx5qZpxAGKxGOWlMcpLi5g2gt1cE4kEnd3xAQaUpAwsSTZZdsZ3P9/Z19//k9TRFY9GVmYe4NKnCHR0xXlry84vJkfPmMTC+dN516xKjZSUnJdpgPoOYQDEC8PMvzDqqxrM9YSa1+XR64fMbCohEO0WoNx9PbA++drMioEfAquBy6I8bwJvpl5nZvcCm4EPu3vy++4ngJ+5+/VRnqXAKcBngIIMUE2t4QOvvoBqT9kWi8WoKCumoqyYuhFcH48n6IgC3EADSlJrdQOl7xjmFIETrYoFDdM58qBJe/FuRcZXpqP4vmJmB5jZdwgDIFoAzOxKoD5Ka0rJP2hwMrOZwMHAP6adugu42MxmuPureyjS54Djgbnu3j1QBjM7m1Azu8jdt6acKgf6e4ndvc/MthJqWwWpOWriq8uh/qd9XVFRNCCjfGQj5vriYYrAblMBogDW3RtntlUx6x1aJ0/yT6Z9UEcBjUAN8AugJTo1Bfg8sNDMTh1GYIEwsAIgfTj6uuTjgEHvY2aVhGa6nw82/8rMYsD3gGUDNBl+H7jczH4FrCIMlT+OUKsrSMkFPutHcQSfZFdxUYyqihKqKjSJVgpPph0R3yHUOo5w99XJRHe/CjgC6AZuGOa9aqLjtrT0ZK2meg/Xf4oQGL89RJ5zCIM5Bhqk8S/ASuB/ga2EpsJr3X3JHp6bt5I1qNoa1aBEJPdlGqDmAP/q7mvTT0S1ppuAecO8V7KHNn2ZpGR62tio3VwK3OfuLw+R52+BZ9z9d6mJZlYGPAIcA/w1MJ8Q6L5iZv8wjLLnpWbVoEQkj2TaLlBE6LsZTAwY7tr7rdExvaZUlXZ+N2Z2DHAocNUQeaYSAs8VA5y+gBCc5rt7Y5TWGDUJftvMFrn75j2+gzyyPaXjXH1QIpIPMq1BPQF81swmp5+I+oQ+DTw5zHsl+55mpaXPSjs/kA8A7cADQ+Q5ixCAB2qyOzA6rkhLXw6UDlCmvLepdeeQZdWgRCQfZFqDugZYBvzBzG4nDGiIEz7QLyFMeP3kcG7k7uuitfsuBO5NOXUBsNbdXx/i8jnAKnfv2kOeDe7+x4EeHx1PA1Kb/+YSmhyHenZeSvY/lRTHmKJVqUUkD2Q6zPxJM3sfcCPwJXb2F0GYi/SJDCe5XgvcamYtwP3AuYRVKhYCmFkdYSj6GndPHUxxNEPXnpJ51gxy7leEybm3m9nXCHOrGghNhje7+1sZvIe8kJwDVVs9QSsHiEheyPirtLs/ApwUBY8DgWLg9ZF8qLv7omjAwpcIzYOvAB9z98VRlrOBWwl9SY0pl05n5xD3wUwHnh7kub1RoL2eMMKvmlAbvBz4cabvIx9oDpSI5JtYIn2dlb1kZnXu3jyqN80DZra1qqqqZtWqVdkuyoBuXPI6v3umhfccN4UvLzgg28UREQFg9uzZtLW1tbr75PRzGdegzOyjhH6iSnYdZFFCGIF3JGGggeSQJtWgRCTPZLqSxBWEZrFuwgTbWsK6d9OAiUAH8INRLqOMAs2BEpF8k+kw808SBkPUE0a8xQj9QzWEibPlhKHokkPi8QSb+jcqVA1KRPJDpgHqIMIK4G3u/gphoMJp7t7n7j8GFgN/P7pFlL21tb23f9dZ1aBEJF9kGqB6SFkBHFhLWJEhaSlhhQfJIbtuVKgalIjkh0wD1IvAySmvHZid8noyQ2xQKNnRHM2BqtyLbR1ERMZbpqP4bgX+LZq79FnChNc7zezrhOB1OaGPSnKIRvCJSD7KdCWJm83szwmrhPcA9wB3AF+PsmwDrhzVEspeS47gq1PznojkkYya+MxsmrtfDdS6e7e7J9z9LwlbbJwPHJrhUkcyDnbWoDRAQkTyR6ZNfM+Y2S3uvssGgNHyR5Kjds6BUg1KRPJHpoMk6oC3x6IgMnaSgyRUgxKRfJJpgLod+IyZHTQGZZEx0NUTp3V7L6BBEiKSXzJt4osDhwFrzWwd0AT0peVJuPt7R6NwsveSzXugSboikl8yDVDvAzZFv5cDWhY7xzW3hgESRTGYVqUalIjkj0yHmc8Yq4LI2EiO4JtaPYHiYm1UKCL5I9M+KMkzGsEnIvkq0+02Hh5OPnd/z8iKI6Otfw5UjfqfRCS/ZNoHNRNI34K3mLAvVDmwAfjD3hdLRsumVm2zISL5KdM+qIMGSjezYuCDwE+BG/e+WDJamrRRoYjkqVHpg4r2g7oHuAW4YTTuKXsvkUjQrIViRSRPjfYgibXAsaN8Txmh1u19dPdqo0IRyU+jFqCiLTg+Qpi8KzkgOQcKtJK5iOSf0RrFVwYYMIWdW29IliX7n8pLi6is0EaFIpJfRmMUH4Tljl4CfgH8294WSkZHav9TLKZJuiKSX0ZlFJ/kpuQq5vWaAyUieSjTGhRmdgDweeAGd2+J0q4A6oHvurv6oHKERvCJSD7LdEfdo4CngX9g14VipwKXEjY01Hp9OUJzoEQkn2U6iu87QBtwhLuvTia6+1XAEUA3mgeVM1SDEpF8lmmAmgP8q7uvTT/h7q8CNwHzRqNgsne6e+NsaQsbFaoGJSL5KNMAVURYc28wMaBi5MWR0bJ5286NCms1B0pE8lCmAeoJ4LNmNjn9hJlVAp8GnhyFcsleSt1JVwFKRPJRpqP4rgGWAX8ws9uBdYRt4GcBlwD7AZ8c1RLKiCT7n6ZUlVBaom2/RCT/ZDoP6kkzex9hxfIvEZr0klYDn3D3x0exfDJCTZoDJSJ5LuN5UO7+CHCSmdUBBxL2g3rd3d8a7cLJyGkEn4jku72dqLsqSrvCzDRRN4c0aw6UiOS5TBeLPQpoBGoI6+61RKeSE3UvMbNToyHnw73nJcDVhHX+NgDXu/vPBsm7CPj4YPdy95iZfYOhF6w9yN1fi+73IeBrwOGEVdhvA651997hlj9XNakGJSJ5LqsTdc3sIuB24LfAeYTgd5uZXTjIJdcBc9N+PkoYqHFzlOenA+T5ANABPAC8ET37AuBuwqjDs4EfAVcA3xxu+XNV2Kgw2updI/hEJE9l2sQ3B7husIm6ZnYT4UN+uK4Hlrj75dHrh8xsKiEQ3TXAM9YD65Ovo63mf0gYoHFZlOdN4M3U68zsXmAz8GF3j5tZjDDQ43Z3/5so28PRs9+bQflzUntnHx3dcQDq1MQnInkq0wA1ahN1zWwmcDDwj2mn7gIuNrMZw2gq/BxwPDDX3bsHymBmZxNqZxe5+9Yo+XjgIMLQ+H5RTTDvpc6BqlcTn4jkqWxO1D0sOnpa+rrkLYe6OHreNcDP3X3lIHliwPeAZe6eWiM7Jjr2mNmDZtZlZs1mdp2Z5f2koWSAmlASo2ZSxuNgRERyQjYn6tZEx21p6W3RsXoP13+KsIPvt4fIcw5hAMQX0tLrouMvCQMjbgAagK8SNl/8xh6endOSQ8zra7RRoYjkr2xO1E1em75DbzI9vofrLwXuc/eXh8jzt8Az7v67tPRkx8xid786+n2pmU0BrjSzG9y9Yw/Pz1lNrckRfOp/EpH8lXFzlrs/4u4nEWpL7yaMktvf3d/l7o3RBN7haI2O6TWlqrTzuzGzY4BDgZ8PkWcqMH+QPMla2gNp6Q8R+tgOHey++aB/BJ/6n0Qkj41kou5HgQuASlICnJmVEILLkeysoQwl2fc0C3g+JX1W2vmBfABoZ/cAk+oswvtbMsC55CjEsrT0ZLnTa3V5pX8OlJY5EpE8lumOulcAi4D3A0cT9n46mFCTOpVQ8/jBcO7l7uuAV4H0OU8XAGvd/fUhLp8DrHL3rj3k2eDufxzg3HJgB7AwLf0DhOHoLw5V9ly3cxUJ1aBEJH9lWoP6JKGvaR5hoME6QjPaa8BnCBsWPpHB/a4FbjWzFuB+4FzgYqLAETUXHgyscffUwRRHM3TtKZlnzUAn3L3dzK4BbjCzLdGzz4ze32Xu3jPQdfmgry/RvxeU+qBEJJ9l2gd1EPAzd29z91cISx2d5u597v5jYDHw98O9mbsvIsxlej9hRF0D8DF3XxxlORt4nDBvKdV0di6zNJgh87j7dwlB9SxCsPsQ8Dfu/sPhlj8XbWnrIR41UKoGJSL5LNMaVA87BxhA6Ms5JuX1UuBbmdzQ3X8C/GSQc4sITYrp6ROHcd8jhpHnFuCWPRYyjzTtslGhalAikr8yrUG9CJyc8tqB2SmvJ7P7wAMZR8k5UNWTiikvzfs5xyKyD8u0BnUr8G9mVgZ8FvgVcKeZfZ0QvC4n9FFJlmijQhEpFJlO1L3ZzP6cMAG2B7gHuIOd21tsA64c1RJKRrRRoYgUipFM1L0aqHX3bndPuPtfEkb1nQ8cqi3fs6upf5sN1aBEJL+NaCXR9A39om3gJQeoBiUihUK96AWmuVVbvYtIYVCAKiA7uvpo7+gDVIMSkfynAFVAdt2oUDUoEclvClAFJNn/VFIcY0qlNioUkfymAFVAknOgaqsnUFSkjQpFJL8pQBWQZA2qVv1PIlIAFKAKSP82G5oDJSIFQAGqgDS3ag6UiBQOBagC0rRVc6BEpHAoQBWIeDzBptbkRoWqQYlI/lOAKhBb23vp7Qs7FaoGJSKFQAGqQDRFI/gAamtUgxKR/KcAVSCSa/BVlhczqbw4y6UREdl7ClAFoklzoESkwChAFYidc6AUoESkMChAFYjm/hF8GiAhIoVBAapAJJc5qlcTn4gUCAWoAtG/1btqUCJSIBSgCkBXT5zW7b2AJumKSOFQgCoA2qhQRAqRAlQBSC4SWxSDaVWqQYlIYVCAKgDJGtTU6gkUF2ujQhEpDApQBUAj+ESkEClAFYDkVu912qhQRAqIAlSWJRIJHnxqM0+saR3xPZI1KI3gE5FCogCVZU1be/j+PW9y3X9u6B/sMJJ7gEbwiUhhUYDKsmnVE6ieWEw8ActWb834+kQioRqUiBQkBagsKymOcdrRkwFoHEGAat3eR3dv2KhQfVAiUkgUoHLA/OOmALD+Tx283tSZ0bWpzYIaxScihUQBKgccfsDE/uCSaS0qOQeqvLSIygptVCgihaMk2wUws0uAq4GZwAbgenf/2SB5FwEfH+xe7h4zs28AXx/ikQe5+2tp9y0BHgda3f2MTMo/GoqKYjQcO4Uly5pofLaFj54xnVhseBNuU/ufhnuNiEg+yGoNyswuAm4HfgucBzQCt5nZhYNcch0wN+3no0AcuDnK89MB8nwA6AAeAN4Y4L5XAbP39v3sjYbjJgPw1pZu/I0dw74uOQeqXv1PIlJgsl2Duh5Y4u6XR68fMrOphEB0V3pmd18PrE++NrNi4IfAauCyKM+bwJup15nZvcBm4MPuHk87dyzwFeDtUXpPIzJjvwoO2q+cDW930rh6K4cdMGlY12kEn4gUqqzVoMxsJnAwcHfaqbuAw8xsxjBu8zngeOBz7j7gJCIzO5tQO7vc3bemnZsA3Ab8APBMyj8W5h87GYDlz22lry8xrGs0B0pEClU2m/gOi47pgWFddLShLjazSuAa4OfuvnKQPDHge8Ayd9+tRkboqypl6D6rcTPv2DCar6W9l9WvtA/rGtWgRKRQZTNA1UTHbWnpbdGxeg/XfwqYAnx7iDznAIcTmgx3YWYnAl8CPuHuXXss7TiYPqWUIw6cCMDSZ1v2mL+nN05Le7RRofqgRKTAZDNAJYecpbdlJdPjDO1S4D53f3mIPH8LPOPuv0tNNLNyQtPevw5W+8qW5Jyox15opatn6D+Czdt6SER/eqpBiUihyWaASq6Oml5Tqko7vxszOwY4FPj5EHmmAvMHyfNNwnu/zsxKomHmMSAWvc7aeO3Tjp5McRF0dMVZ+VJ65XJXTSk76dbWKECJSGHJZoBK9j3NSkuflXZ+IB8A2gnDxgdzFmGU4pIBzl1I6ONqB3qin9OB90S/zxuq4GOpZlIJxx8SYvSeJu0m+5+mVJVQWqI51yJSWLL2qebu64BXCcEi1QXAWnd/fYjL5wCr9tB3NAfY4O5/HODcOcCJaT9PAyuj338/rDcxRpLNfCtf2kZ7R9+g+TQHSkQKWbbnQV0L3GpmLcD9wLnAxcBCADOrIwxFX+Puqe1dRzN07SmZZ81AJ9z9+fQ0M2sDet19VaZvYrTNObyasglFdPXEeewPW3n/idMGzKcRfCJSyLLaLuTuiwhzmd4P/BJoAD7m7oujLGcTliA6Pu3S6cCehrkNJ09OqigrZu4RoWtu6RDNfM2aAyUiBSzbNSjc/SfATwY5twhYNED6xGHc94gMy9GQSf6x1nDsFBpXb+W5V9rZvK2HadW715KaohqUBkiISCFSz3qOOuHQKqonFpNIhJUl0oWNCpM1KAUoESk8ClA5qqQ4xqnRRoYDTdrd3hmnozvMk6pTE5+IFCAFqByWXJtv7R87eLN5140MkwMkQDUoESlMClA57IgDJ1FXM/BGhslJuhNKYtRMynpXoojIqFOAymFFRbH+faIan20hkdi5KlSyBlVfo40KRaQwKUDluPnRCud/3NzNy2929Kc3tSbnQKn/SUQKkwJUjjtov3IOnF4OQOPqnYMlkiP4NElXRAqVAlSOi8VizI+a+ZY9t5W+eGjma46WOdI2GyJSqBSg8sC8YyYD0NLWy3PRRob9fVCqQYlIgVKAygP7TS3r38iwcXXYDn7TtmQTn2pQIlKYFKDyREM0WOLR57fydks38WgvQ9WgRKRQKUDlidOOnkxREezoivPAk5v702vVByUiBUoBKk9Mrizh+FlhI8MHVoYAVT2pmPJS/RWKSGHSp1seSW5k2BmtwaeNCkWkkClA5ZE5R1RTNmHnqhGaAyUihUwBKo9MLCtmzuE1/a81B0pECpkCVJ5Jrs0HqkGJSGFTgMozJxxS1b96eXIJJBGRQqR9GvLMhJIivvWpmaz/UwezD63KdnFERMaMAlQeOvgdFRz8jopsF0NEZEypiU9ERHKSApSIiOQkBSgREclJClAiIpKTFKBERCQnKUCJiEhO0jDz0VPd1tbG7Nmzs10OEZG80dbWBlA90DkFqNETB4ra2tq2ZbsgIiJ5pJrw+bmbWCKRGOeyiIiI7Jn6oEREJCcpQImISE5SgBIRkZykACUiIjlJAUpERHKSApSIiOQkBSgREclJClAiIpKTFKBERCQnKUCJiEhOUoASEZGcpMVis8DMLgGuBmYCG4Dr3f1nWS3UGDKzIuAzwOcJ73kjcB/wdXdvy2bZxpOZ3QMc4+6zsl2WsWRmpwPfBo4HtgJ3A//o7u3ZLNdYM7PPAZcBBwDrgRvc/fbslmpsmNlxwFPADHd/MyX9TOBbwJGE/+c3ufs/j/Q5qkGNMzO7CLgd+C1wHtAI3GZmF2axWGPtCuAm4NeE9/zPwMeBO7NYpnFlZh8BPpTtcow1M5sD/A/wNnAucC3wEeCn2SzXWDOzzwA/Jvwb/yDwv8B/Rv/fC4qZGXA/aRUcMzs5Sn8JOJ/wOfc9M/vSSJ+l1czHmZmtA1a5+8KUtMWEb9aHZ69kY8PMYsBm4BfufmlK+gLgDuBd7v5sloo3LszsHcAfgO1AVyHXoMxsWfRrg7snorRLgS8CR7v7jqwVbgyZ2Qqg093fk5K2HOhz9/nZK9noMbMSQkvId4AeYCrwzmQNysz+F6h09zkp19wQXbOfu3dl+kzVoMaRmc0EDiY0eaS6CzjMzGaMf6nGXBXwn8B/paW/FB0PHt/iZMVPCTXm32W7IGPJzGqB04AfJ4MTgLv/yN0PLtTgFCkH0purNwPTslCWsXIq8F1CC8iVqSfMrBw4nYE/2yYDJ4/kgeqDGl+HRUdPS18XHQ14dfyKM/bcfRvwdwOcOi86vjB+pRl/ZvZp4ARCm/yNWS7OWDsaiAFbolaBDwC9hC8nX3T3jmwWbox9H7glatJ7CDiT8P6/ktVSja4XgZnu3mRmn0g7NxOYwNCfbUszfaAC1PiqiY7pu+4mv3kNuO1xoTGzk4CrgF+6+0t7yp+vzOxA4P8Cn3T3TaHpvqDVRcdFwL3AOcCxwDeBCuATWSnV+PgF8B5gSUrabe7+vSyVZ9S5+8YhTo/JZ5sC1PiKRcf0jr9k+oDbHhcSMzuF0JH6KvDpLBdnzER9b/8BPODu6c0ehao0Oq5I6W98OPqzuNHMrnX3V7JUtrH2K0Iz1heBp4GTgH8ys23uPlALQqEZ7LMtaUSfbQpQ46s1OqZ/m6hKO1+QooERi4CXgbPcfXN2SzSmLgWOAY6OOpch+k8cve5L7acpEMlvyw+kpT9E6Lc4Gii4ABWNXns/oaa8KEpeZmZbgZ+Y2S3u/ny2yjdOBvtsq047nxENkhhfyfbZ9FFcs9LOFxwz+yKhGeRx4HR3fyvLRRprFwK1wFuEEU89wMcIg0J6CMPsC83a6FiWlp6sWRVaQE46MDo+lpa+PDoeMY5lyZb1QB+j/NmmADWO3H0doWkrfc7TBcBad399/Es19szsrwjfoJcQak4FXVOMfBY4Me3nfuDN6Pf/zl7RxsyLwGvAwrT05GCJx8e9ROMj+eF7elr63Oi4YfyKkh3u3kkIyOdHTbpJFxBqT6tGcl818Y2/a4FbzayF8IF1LnAxu/+nLghmVg/8gPDBdRNwfNpggXXuvikbZRtL7r7bN0Yz20yYBzWi/6y5zt0TZnYl8Asz+09Cc+4JhFVTfuDuzdks31hx96fN7JfAv5pZNfAMMBv4J+A37v5kNss3jr5JmKB8h5ktIvTJfRm4aqRTDFSDGmdRG/XnCG3WvwQagI+5++LslWpMnQVMJDSDPEL4Fp36c1b2iiajLfp3fD6hWet+Ql/ctYQPqkK2EPghcDnwIGEA0I3sA6uHJLn7w4Qa0+GEz7YPA1929++O9J5aSUJERHKSalAiIpKTFKBERCQnKUCJiEhOUoASEZGcpAAlIiI5SQFKRERykgKUyD7CzBaZWWKw1yK5RitJiOw7fkKY6S+SFxSgRPYR7p5cvUMkL6iJT0REcpJqUCJZZGZzCWvVzYmSHgeudveV0fkNhGa5x4GvAtOBZ6M8S1PuMwX4F8KurtMJq6YvAa6JVpomWsDz4+6eutp0enkOJCz6eRZhnzIHbnL3W1LyLIrK+1HCenMnEvaCWgxcWeBbu8s4Ug1KJEvM7H3AMsJ22V8jBIYDgOVmdlpK1vcBPwLuivLVAw+Z2byUPEsI21rcQligtRG4irCS/HDLMwN4CvhgdJ8vA1uAfzez9AU/64HfAi8BlxH2QvoCcM1wnyeyJ6pBiWSBmRUBNwMrgXnu3hel30SoIf0AeFeU/QDgQ+7+yyjPzwm7En8HmBttaXIGYeXoG6NrfhrtyzMzg2JdD0wDTnT3p6Nn/Qi4D/iSmd3m7i9EeacAf+fuP4xe32JmawgrWF+RwTNFBqUalEh2vIsQPH4JTDGzWjOrBSoImxkeZ2Z/HuV9KRmcAKJ9lX4OnBQFp1agHfi8mV1gZpOifJ9y9zOGUxgzKwbOBh5KBqfoHnHgW4Tt6s9Nu2xJ2uvVhOZFkVGhACWSHQdHx+8BzWk/l0fn3hkd1wxw/VpC0DjQ3bsIO/hOJzQDbjazh8zsM2ZWPszy1AKVDLw194vR8cC09PQNCLuA4mE+T2SP1MQnkh3JD/KvAU8Mkuel6Ng9xPV9AO7+X2b2IHAeoSZ0BnAmoVZ1UhTEhjLowAl2fpHdpRxR7UpkzChAiWTHhujY7u67TJ41sxOBqUByNNzB7O4QQnB61cwqgeOAF9z9P4D/MLNS4LuEAQxnEpoNh9IMbAcOG+CcRcc39nAPkVGlJj6R7FgFvAX8XRRgADCzakLfzq1Ab5R8opnNSckzHfgI8LC7twBHAY8Af5XM4+7dwDPRy749FSYapPEb4EwzOz7lWTHgSiAB/DrztykycqpBiWSBu/eY2RcIwehpM/sp0An8NaGv58Pu3mtmEPp2fmNm/0KoVV1K+HL5peh2TxIC1LfM7ADgOUL/1RcIzYTDXd7oKsI8qkYz+yEhgH4oSvu/7j5QX5jImFENSiRL3P1uQvPbm4S+qOuAbcC57v6LlKxPEILHZ4B/IgyaOMXdn4vukyD0Pd1MmAt1U5T3bmB+VJsaTnnWAycBDwCfIzQRTgb+yt3/YS/eqsiIxBIJLWYskquilSQ2uHtDlosiMu5UgxIRkZykACUiIjlJAUpERHKS+qBERCQnqQYlIiI5SQFKRERykgKUiIjkJAUoERHJSQpQIiKSkxSgREQkJ/1/MvUahhuQhKMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilons, experiment_results)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
