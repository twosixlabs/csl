{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 01. Datasets and Dataloaders\n",
    "\n",
    "This example notebook is inded to serve as a guide for csl datasets module and its associated load and dataloader methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/carlos-torres/Documents/twosix/projects/clamped/csl/documents\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "# os.chdir(\"../\")\n",
    "import sys\n",
    "sys.path.append(\"/home/carlos-torres/Documents/twosix/projects/clamped/csl/csl\")\n",
    "\n",
    "import csl.datasets as dset\n",
    "import csl.utils.utils as cutils\n",
    "import csl.synthesizers as syn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Loading a dataset**\n",
    "\n",
    "`load(name)`: loads any supported dataset\n",
    "  * Standard dataset: native dataset from torchvision\n",
    "  * Synthetic dataset: expected in ImageFolder structure (uses torchvision.datasets.ImageFolder)\n",
    "\n",
    "\n",
    "  * **Inputs:** \n",
    "      * `name`: str,  the name of the dataset\n",
    "      * `image_size`: [optional] int, the square pixels dimensions used to resize images and ensure compliance. Default value is defined in `dataset.IMAGE_DIMS`\n",
    "      * `data_directory`: [optional] str, path to the folder dataset in the system (where the dataset is stored). Default value is defined in `datasets.DATA_DIR`\n",
    "\n",
    "\n",
    "  * **Returns:** (train_set, test_set)\n",
    "    - train_set: torchvision.datasets\n",
    "    - test_set: torchvision.datasets\n",
    "\n",
    "\n",
    "  * **Alternatively:** see `Dataset.load()` for datasets and customized train and test transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the vae-synthesized version of fashion-mnist \n",
    "DS_NAME = \"fashion-mnist_vae\"\n",
    "\n",
    "train_ds, test_ds = dset.load(DS_NAME)\n",
    "\n",
    "# get some help and additional informaiton by printing the `help`\n",
    "# help(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Creating a set of dataloaders (train & test)**\n",
    "\n",
    "`get_dataloaders(name)`: wraps around the `load` method to create dataloaders of a single supported type\n",
    "  * Standard dataset: native dataset from torchvision\n",
    "  * Synthetic dataset: expected in ImageFolder structure (uses torchvision.datasets.ImageFolder)\n",
    "\n",
    "\n",
    "  * **Inputs:** \n",
    "      * `name`: str,  the name of the dataset\n",
    "      * `image_size`: [optional] int, the square pixels dimensions used to resize images and ensure compliance. Default value is defined in `dataset.IMAGE_DIMS`      \n",
    "      * `batch_size`: [optional] int, the square pixels dimensions used to resize images and ensure compliance. Default value is 16.\n",
    "      * `num_workers`: [optional] int, the number of workers to use in dataloading (moving from memory to cpu or gpu). Default value is 2.\n",
    "      * `class_idx`: [optional], int, the index number of the class to select from the dataset (e.g., set class_idx = 0, to select only 0-class samples). The default value is 'all' and returns all classes.\n",
    "      * `num_samples`: [optional] int, the number of samples to return (i.e., set it to 5, 29, etc. to return a very specific number of samples.). It must be smaller or equal than the number of actual dataset samples for that class. Default value is 'all' and returns all available samples.`\n",
    "      * `data_directory`: [optional] str, path to the folder dataset in the system (where the dataset is stored). Default value is defined in `datasets.DATA_DIR`\n",
    "\n",
    "\n",
    "  * **Returns:** (train_loader, test_loader)\n",
    "      - train_loader: torch.utils.data.DataLoader\n",
    "      - test_loader: torch.utils.data.DataLoader\n",
    "\n",
    "\n",
    "  * **Alternatively:** see `Dataset.create_dataloaders()` for datasets and customized train and test transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "DS_NAME = \"mnist\"\n",
    "train_loader, test_loader = dset.get_dataloaders(DS_NAME, batch_size=BATCH_SIZE)\n",
    "\n",
    "# visualize the batch\n",
    "batch, labels = next(iter(train_loader))\n",
    "plot_title = f\"{DS_NAME}-Dataset: Sample Batch ({BATCH_SIZE}x{BATCH_SIZE})\"\n",
    "cutils.visualize_batch(batch, plot_title=plot_title, grid_dims=BATCH_SIZE**.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Creating a set of dataloaders (train & test)**\n",
    "\n",
    "`get_hybrid_dataloaders`: wraps around the `Dataset.load()` and `Dataset.create_dataloaders()` methods to create hybrid dataloaders (native + synthetic) versions \n",
    "  * Standard dataset: native dataset from torchvision\n",
    "  * Synthetic dataset: expected in ImageFolder structure (uses torchvision.datasets.ImageFolder)\n",
    "\n",
    "\n",
    "  * **Inputs:** \n",
    "      * `name`: str, the name of the dataset\n",
    "      * `image_size`: [optional] int, the square pixels dimensions used to resize images and ensure compliance. Default value is defined in `dataset.IMAGE_DIMS`      \n",
    "      * `method`: str, the name of the method used to synthesized the dataset (supported: vae, cvae, dcgan). Default value = 'vae'.\n",
    "      * `batch_size`: [optional] int, the square pixels dimensions used to resize images and ensure compliance. Default value is 16.\n",
    "      * `original_portion`: [optional] float, stratified portion of the original data used in the expected dataloader. Default value is 0.5. (e.g., 50% of the original dataset with similar data-label distribution).\n",
    "      * `synthetic_portion`: [optional] float, stratified portion of the synthetic data used in the expected dataloader. Default value is 0.5. (e.g., 50% of the synthetic dataset with similar data-label distribution).\n",
    "      * `num_workers`: [optional] int, the number of workers to use in dataloading (moving from memory to cpu or gpu). Default value is 2.\n",
    "      * `class_idx`: [optional], int, the index number of the class to select from the dataset (e.g., set class_idx = 0, to select only 0-class samples). The default value is 'all' and returns all classes.\n",
    "      * `num_samples`: [optional] int, the number of samples to return (i.e., set it to 5, 29, etc. to return a very specific number of samples.). It must be smaller or equal than the number of actual dataset samples for that class. Default value is 'all' and returns all available samples.`\n",
    "      * `data_directory`: [optional] str, path to the folder dataset in the system (where the dataset is stored). Default value is defined in `datasets.DATA_DIR`\n",
    "\n",
    "\n",
    "  * **Returns:** (train_loader, test_loader)\n",
    "      - train_loader: torch.utils.data.DataLoader\n",
    "      - test_loader: torch.utils.data.DataLoader\n",
    "\n",
    "\n",
    "  * **Alternatively:** see `Dataset.create_hybrid_dataloaders()` for datasets and customized train and test transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DS_NAME = \"fashion-mnist\"\n",
    "SYN_METHOD = \"vae\"\n",
    "ORIGINAL_PORTION = .25\n",
    "SYNTHETIC_PORTION = .25\n",
    "\n",
    "kwargs = {\n",
    "    \"dataset_name\": DS_NAME,\n",
    "    \"method\": SYN_METHOD,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"original_portion\": ORIGINAL_PORTION,\n",
    "    \"synthetic_portion\": SYNTHETIC_PORTION,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    # \"data_directory\": data_directory,\n",
    "    # \"input_size\": image_size,\n",
    "}\n",
    "\n",
    "\n",
    "train_loader, test_loader = dset.get_hybrid_dataloaders(**kwargs)\n",
    "\n",
    "batch, labels = next(iter(train_loader))\n",
    "\n",
    "# visualize the batch\n",
    "plot_title = f\"{DS_NAME}-Dataset: Sample Batch ({BATCH_SIZE}x{BATCH_SIZE})\"\n",
    "cutils.visualize_batch(batch, plot_title=plot_title, grid_dims=BATCH_SIZE**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
