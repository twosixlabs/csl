%%%%%%%%%
% FLAGS %
%%%%%%%%%

% Is this an internal version? (i.e., not public-facing.)
\ifdefined \isinternal \else \def \isinternal{1} \fi

% Is this an extended version? (e.g., includes appendix.)
\ifdefined \isextended \else \def \isextended{1} \fi

% Should authors be identified? (e.g., not submitted for blind peer review.)
\ifdefined \isauthorid \else \def \isauthorid{1} \fi

% Defaults can be overridden from the Makefile
\newif \ifinternal \if \isinternal 0 \internalfalse \else \internaltrue \fi
\newif \ifextended \if \isextended 0 \extendedfalse \else \extendedtrue \fi
\newif \ifauthorid \if \isauthorid 0 \authoridfalse \else \authoridtrue \fi

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\documentclass{article}

\usepackage[T5,T1]{fontenc}

\usepackage{longtable}

\input{darais-latex-imports}
\input{darais-latex-macros}

\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\begin{document}

\section{A Tiny Model (No Vectors)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector â¸¨ğ± â‰œ [xâ‚,xâ‚‚]â¸©, a weights vector â¸¨ğ›‰ â‰œ [Î¸â‚,Î¸â‚‚]â¸©, and the
classification function â¸¨fâ¸© is:
Mâ… fâ¸¤ğ›‰â¸¥(ğ±) â‰œ Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ Mâ†

Here is the mean squared error loss function over scalars:
Mâ… L(a,b) â‰œ (a - b)Â² Mâ†
and the loss of our function â¸¨fâ¸© {w.r.t.} a training example â¸¨yâ¸© is:
Mâ… Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) AË[t]rcl
                Aâ… â§¼â‰œâ§½ (fâ¸¤ğ›‰â¸¥(ğ±) - y)Â² 
                Aâƒ â§¼=â§½ (Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ - y)Â²
                Aâƒ â§¼=â§½ Î¸â‚Â²xâ‚Â² + Î¸â‚‚Â²xâ‚‚Â² + 2Î¸â‚Î¸â‚‚xâ‚xâ‚‚ - 2Î¸â‚xâ‚y - 2Î¸â‚‚xâ‚‚y + yÂ²
                Aâ†
Mâ†
Here is the gradient of â¸¨Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y)â¸© {w.r.t.} â¸¨ğ›‰â¸©:
Mâ… âˆ‡â¸¤ğ›‰â¸¥Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) AË[t]rcl
                    Aâ… â§¼â‰œâ§½ â€˜[ \frac{âˆ‚(Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y))}{âˆ‚Î¸â‚} , \frac{âˆ‚(Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y))}{âˆ‚Î¸â‚‚} â€™]
                    Aâƒ â§¼=â§½ â€˜[ 2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y , 2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y â€™]
                    Aâ†
Mâ†
The Â«immediate sensitivityÂ» is defined as:
Mâ… Â«ğˆğ’Â»â¸¤ğ±,ğ›‰â¸¥(f) AË[t]rcl
                  Aâ… â§¼â‰œâ§½ â€– âˆ‡â¸¤ğ±â¸¥ â€– âˆ‡â¸¤ğ›‰â¸¥ Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) â€–â‚‚ â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ğ±â¸¥ â€– â€˜[ 2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y , 2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y â€™] â€–â‚‚ â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ğ±â¸¥ âˆš{(2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y)Â² + (2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y)Â²} â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ğ±â¸¥ 2âˆš{(xâ‚Â² + xâ‚‚Â²)(Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ - y)Â²} â€–â‚‚
                  Aâ†
Mâ†
% We want to know â¸¨Î²â¸© such that the above quantity is â¸¨Î²â¸©-smooth.
% The derivative of the loss is:
% Mâ… \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
%                              Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€ - y)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)(\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚y}{âˆ‚Î¸áµ€})
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
%                              Aâ†
% Mâƒ
% Mâƒ \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
%                              Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â² - 2xyÎ¸áµ€ - yÂ²)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(2xyÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(yÂ²)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸}
%                              Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€ - 2xy
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
%                              Aâƒ â§¼=â§½ ... 2xáµ€(xÎ¸áµ€ - y)
%                              Aâ†
% Mâ†

\section{A Tiny Model (Vectorized)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector â¸¨xâ¸©, a weights vector â¸¨Î¸â¸©, and the
classification function â¸¨fâ¸© is:
Mâ… fâ¸¤Î¸â¸¥(x) â‰œ xÎ¸áµ€ Mâ†

Here is the mean squared error loss function over scalars:
Mâ… L(a,b) â‰œ (a - b)Â² Mâ†
and the loss of our function â¸¨fâ¸© {w.r.t.} a training example â¸¨yâ¸© is:
Mâ… Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y) AË[t]rcl
                Aâ… â§¼â‰œâ§½ (fâ¸¤Î¸â¸¥(x) - y)Â² 
                Aâƒ â§¼=â§½ (xÎ¸áµ€ - y)Â²
                Aâ†
Mâ†
The Â«immediate sensitivityÂ» is defined as:
Mâ… â€– âˆ‡â‚“ â€– âˆ‡â¸¤Î¸â¸¥ Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y) â€–â‚‚ â€–â‚‚
Mâ†
We want to know â¸¨Î²â¸© such that the above quantity is â¸¨Î²â¸©-smooth.
The derivative of the loss is:
Mâ… \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
                             Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€ - y)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)(\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚y}{âˆ‚Î¸áµ€})
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
                             Aâ†
Mâƒ
Mâƒ \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
                             Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â² - 2xyÎ¸áµ€ - yÂ²)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(2xyÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(yÂ²)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸}
                             Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€ - 2xy
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
                             Aâƒ â§¼=â§½ ... 2xáµ€(xÎ¸áµ€ - y)
                             Aâ†
Mâ†

\section{New}

% Need: IS is â¸¨Î²â¸©-smooth.

% Know: IS(x)
% Need to know: IS is â¸¨Î²â¸©-smooth
% Need to know: â¸¨ | IS(x) - IS(x') | â‰¤ ???â¸©

% If IS is â¸¨Î²â¸©-smooth then â¸¨| IS(x) - IS(x') | â‰¤ Î²â¸©

% We care about: local sensitivity of gradient G

% â€– G(x) - G(x') â€–â‚‚ â‰¤ IS(x) + Î²

% if G' is Î²-smooth then:

% â€–G'(x) - G'(x')â€– â‰¤ Î² â€–x - x'â€–        (defn of smoothness)
% â€–G'(x) - G'(x')â€– â‰¤ Î²                 (â€–x - x'â€– â‰¤ 1 by assumption)


% LSâ¸¤Gâ¸¥(x) = maxâ¸¤x' . d(x, x') â‰¤ 1â¸¥ â€– G(x) - G(x') â€–

% MVT says:
% âˆ€ x, x'. âˆƒ x''. x â‰¤ x'' â‰¤ x' âˆ§ â€–G(x) - G(x')â€– / â€–x - x'â€– = G'(x'')

%    â€–G(x) - G(x')â€– / â€–x-x'â€– = G'(x'') for some x''           (mean value theorem)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– = G'(x) + â€–G'(x) - G'(x'')â€–      (arithmetic)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– â‰¤ G'(x) + Î² â€–x - x''â€–            (def. of Î²-smoothness)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– â‰¤ G'(x) + Î²                      (def. absolute value)
% => â€–G(x) - G(x')â€–          â‰¤ G'(x) + Î²                      (inequality)
% => LSâ¸¤Gâ¸¥(x)                â‰¤ G'(x) + Î²                      (def. of LS)



% => G(x) - G(x') = G'(x) + â€–G'(x) - G'(x'')â€– 
% => G(x) - G(x') â‰¤ G'(x) + Î²                          (by above)
% => LSâ¸¤Gâ¸¥(x)     â‰¤ G'(x) + Î²                          (def of LS)



\section{Current Situation}

For all of the following, â¸¨â€–Xâ€–â‚â¸© is the L1 norm, â¸¨â€–Xâ€–â‚‚â¸© is the L2 norm, â¸¨â€–Xâ€–â¸¤âˆâ¸¥â¸©
is the Lâªâˆâ« norm, and â¸¨â€–Xâ€–â¸© (without a subscript) is some norm in a parametrized
metric space. Same goes for distances â¸¨â€–X-Yâ€–â‚â¸©, â¸¨â€–X-Yâ€–â‚‚â¸©, â¸¨â€–X-Yâ€–â¸¤âˆâ¸¥â¸© and
â¸¨â€–X-Yâ€–â¸©.

\subsection{From \cite{smooth-sensitivity}}

\begin{definition}[Global Sensitivity \citep{smooth-sensitivity}]
  The Global Sensitivity of â¸¨fâ¸© is â¸¨Â«GSÂ»â¸¤fâ¸¥â¸© where:
  Mâ… Xâ… â©Š Â«GSÂ»â¸¤fâ¸¥ â‰œ \max\limitsâ¸¤x,y:â€–x-yâ€–=1â¸¥ â€– f(x) - f(y) â€–â‚ 
        â©Š âŸª\citep[Â§ 1.2, Definition 1.3]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: â¸¨â€–x-yâ€–â¸© is an abstract distance but â¸¨â€– f(x) - f(y) â€–â‚â¸©  is
  specifically L1 distance.
\end{definition}

\begin{definition}[Local Sensitivity \citep{smooth-sensitivity}]
  The Local Sensitivity of â¸¨fâ¸© at â¸¨xâ¸© is â¸¨Â«LSÂ»â¸¤fâ¸¥(x)â¸© where:
  Mâ… Xâ… â©Š Â«LSÂ»â¸¤fâ¸¥(x) â‰œ \max\limitsâ¸¤y:â€–x-yâ€–=1â¸¥ â€– f(x) - f(y) â€–â‚
        â©Š âŸª\citep[Â§ 1.3, Definition 1.6]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: â¸¨â€–x-yâ€–â¸© is an abstract distance but â¸¨â€– f(x) - f(y) â€–â‚â¸©  is
  specifically L1 distance.
\end{definition}

\begin{definition}[DP-â¸¨Î²â¸©-Smoothness \citep{smooth-sensitivity}]\ \\
  A function â¸¨fâ¸© is DP-â¸¨Î²â¸©-Smooth if:
  Mâ… Xâ… â©Š Â«GSÂ»â¸¤\ln(f(â‹…))â¸¥ â‰¤ Î²
        â©Š âŸª\citep[Â§ 2.1]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Alternatively, â¸¨fâ¸© is DP-â¸¨Î²â¸©-smooth if:
  Mâ… Xâ… â©Š âˆ€ x,yâª â€–x-yâ€– = 1 âŸ¹ f(x) â‰¤ eâ¸¢Î²â¸£f(y)
        â©Š âŸª\citep[Â§ 2.1, Definition 2.1, (2)]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: \cite{smooth-sensitivity} just calls this â€œâ¸¨Î²â¸©-smoothâ€.
  \subparagraph{Why are these the same?} 
  âŸª
  Mâ… AËlcl@{â }l
     Aâ… â§¼ â§½ Â«GSÂ»â¸¤\ln(f(â‹…))â¸¥ â‰¤ Î²
     Aâƒ â§¼âŸºâ§½ âˆ€ x,yâª â€–x-yâ€– = 1 â‡’ â€–\ln(f(x)) - \ln(f(y))â€–â‚ â‰¤ Î² & âŸ…definitionâŸ†
     Aâƒ â§¼âŸºâ§½ âˆ€ x,yâª â€–x-yâ€– = 1 â‡’ \ln(f(x)) â‰¤ Î² + \ln(f(y))    & âŸ…(somehow?)âŸ†
     Aâƒ â§¼âŸºâ§½ âˆ€ x,yâª â€–x-yâ€– = 1 â‡’ f(x) â‰¤ eâ¸¢Î²â¸£f(y)              & âŸ…algebraâŸ†
     Aâ†
  Mâ†
  âŸ«
\end{definition}

\begin{definition}[DP-â¸¨Î²â¸©-Smooth Sensitivity \citep{smooth-sensitivity}]\ \\
  A function â¸¨fâ¸© has DP-â¸¨Î²â¸©-Smooth sensitivity if its local sensitivity is
  â¸¨Î²â¸©-smooth:
  Mâ… Â«GSÂ»â¸¤\ln(Â«LSÂ»â¸¤fâ¸¥(â‹…))â¸¥ â‰¤ Î² Mâ†
  or:
  Mâ… âˆ€ x,yâª â€–x-yâ€– = 1 âŸ¹ Â«LSÂ»â¸¤fâ¸¥(x) â‰¤ eâ¸¢Î²â¸£Â«LSÂ»â¸¤fâ¸¥(y) Mâ†
  \subparagraph{What if â¸¨Â«LSÂ»â¸¤fâ¸¥â¸© isn't already smooth?}
  âŸª
  \cite{smooth-sensitivity} defines a construction of â¸¨Sâ¸¢*â¸£â¸¤f,Î²â¸¥â¸©
  which is the best upper bound on the local sensitivity of â¸¨fâ¸© which is also
  â¸¨Î²â¸©-smooth; this construction is:
  Mâ… Sâ¸¢*â¸£â¸¤f,Î²â¸¥(x) = \maxâ¸¤yâ¸¥Â«LSÂ»â¸¤fâ¸¥(y)eâ¸¢-Î²â€–x-yâ€–â¸£ Mâ†
  âŸ«
\end{definition}

\subsection{From \cite{metrics-local-sensitivity}}

\begin{definition}[Generalized Global Sensitivity \citep{metrics-local-sensitivity}]
  The Generalized Global Sensitivity of â¸¨fâ¸© is â¸¨Â«GGSÂ»â¸¤fâ¸¥â¸© where:
  Mâ… Xâ… â©Š Â«GGSÂ»â¸¤fâ¸¥ â‰œ \maxâ¸¤x,yâ¸¥ \frac{â€–f(x)-f(y)â€–}{â€–x-yâ€–}
        â©Š âŸª\citep[Â§ 3.2, Definition 2]{metrics-local-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: \cite{metrics-local-sensitivity} just calls this â€œGlobal Sensitivityâ€
\end{definition}

\begin{definition}[Derivative Sensitivity \citep{metrics-local-sensitivity}]
  The Derivative Sensitivity of â¸¨fâ¸© at â¸¨xâ¸© is â¸¨Â«DSÂ»â¸¤fâ¸¥(x)â¸© where:
  Mâ… Xâ… â©Š Â«DSÂ»â¸¤fâ¸¥(x) â‰œ â€–fâ€²(x)â€–
        â©Š âŸª\citep[Â§ 4.2, Definition 12]{metrics-local-sensitivity}âŸ«
     Xâ†
  Mâ†
\end{definition}

\paragraph{From ML literature}

\begin{definition}[ML-â¸¨Î²â¸©-Smoothness]
  A function â¸¨fâ¸© is ML-â¸¨Î²â¸©-Smooth if:
  Mâ… Xâ… â©Š â€–fâ€²(x) - fâ€²(y)â€– â‰¤ Î²â€–x-yâ€– 
        â©Š âŸª\citep[Â§ 3]{convex-optimization-ml-slides}âŸ«
     Xâ†
  Mâ†
  or:
  Mâ… Â«GGSÂ»â¸¤fâ€²â¸¥ â‰¤ Î² Mâ†
\end{definition}

\subsection{New Definitions}

\begin{definition}[Immediate Sensitivity]
  The Immediate Sensitivity of â¸¨fâ¸© at â¸¨xâ¸© is â¸¨Â«IMÂ»â¸¤fâ¸¥(x)â¸© where:
  Mâ… Â«ISÂ»â¸¤fâ¸¥(x) â‰œ â€–fâ€²(x)â€– Mâ†
  Jâ… Note: â¸¨Â«ISÂ»â¸¤fâ¸¥(x) = Â«DSÂ»â¸¤fâ¸¥(x)â¸©.
  Jâƒ Note: In our case studies we pick specifically the L2 norm, so in that
     setting, â¸¨Â«ISÂ»â¸¤fâ¸¥(x) â‰œ â€–fâ€²(x)â€–â‚‚â¸© and is an instantiation of â¸¨Â«DSÂ»â¸¤fâ¸¥(x)â¸©
     for a particular norm.
  Jâ†
\end{definition}

\subsection{Where we are}

We have observed that for some architectures, using the L1 norm, â¸¨dÂ«ISÂ»â¸¤fâ¸¥/dxâ¸©
is constant, i.e.,  â¸¨dÂ«ISÂ»â¸¤fâ¸¥/dx = Câ¸© for some scalar â¸¨Câ¸©. So â¸¨Â«ISÂ»â¸¤fâ¸¥â¸© is
ML-â¸¨Î²â¸©-smooth for â¸¨Î²=0â¸©, and â¸¨fâ¸© is ML-â¸¨Î²â¸©-smooth for â¸¨Î²=Câ¸©.

We are currently investigating the following questions:
Eâ… Given an ML-â¸¨Î²â¸©-smooth bound on â¸¨fâ¸©, can we construct some DP-â¸¨Î²â¸©-smooth bound
   on â¸¨fâ¸©?
Eâƒ Given an ML-â¸¨Î²â¸©-smooth bound on some altered â¸¨fâ¸© (e.g., â¸¨\ln(f)â¸©), can we
   construct some DP-â¸¨Î²â¸©-smooth bound on â¸¨fâ¸©?
Eâƒ Given that â¸¨dÂ«ISÂ»â¸¤fâ¸¥/dxâ¸© is constant (or even better; it's â¸¨0â¸©) for our â¸¨fâ¸©
   in question, can we directly say something about â¸¨Â«ISÂ»â¸¤fâ¸¥â¸© being DP-â¸¨Î²â¸©-smooth?
Eâ†

\section{Bounded Global Sensitivity Analysis}

Environment â¸¨Î³â¸© encodes upper and lower bound assumptions on input variables.
E.g., â¸¨Î³(x) = -1,1â¸© encodes that â¸¨xâ¸© is guaranteed to range between â¸¨-1â¸© and
â¸¨1â¸©. This is exactly the interval abstract domain from the literature on
abstract interpretation.

The function â¸¨âŸ¦â€—âŸ§â¸© is the algorithm for bounded global sensitivity analysis. It
takes as input an expression and an environment â¸¨Î³â¸© and returns a mapping from
variables â¸¨x âˆˆ â€¹varâ€ºâ¸© to a pair of Â«sensitivity rangesÂ» â¸¨sË¡,sÊ°â¸©, and a pair of
Â«value rangesÂ» â¸¨rË¡,rÊ°â¸©.

A couple of notes:
Iâ… Most global sensitivity analyses assume positive real numbers as values. We
   must relax this to negative reals in order for â¦‘reluâ¦’ to make sense. A lot
   what is going on (e.g., in the â¸¨Ã—â¸© definition) is accounting for the presence
   of negative reals, e.g., multiplying two large negative-valued lower bounds
   could result in a positive-valued upper bound. Sensitivities are also tracked
   with their sign, e.g., the expression â¸¨-1xâ¸© is â¸¨-1â¸©-sensitive in â¸¨xâ¸©. Because
   sensitivities are now signed, we must track a range of sensitivities.
Iâƒ We could dramatically simplify things by operating over non-negative reals
   and using some hypothetical alternative to relu, but I get the feeling
   supporting negative reals will eventually be essential for ML applications,
   so I've pressed ahead and complicated things with negative reals.
Iâƒ The log operation â€œin mathâ€ is only defined for positive valued reals, so
   I've the operation I encode is instead â€œlog of absolute valueâ€ so it can be
   defined on all reals. This is why the join or meet is taken of absolute value
   lower and upper bounds in the value range. I'm unsure if the numerators
   should be â¸¨sË¡â¸© and â¸¨sÊ°â¸© as written, or if we should also be taking â¸¨sË¡âŠ“sÊ°â¸©
   and â¸¨sË¡âŠ”sÊ°â¸© respectively. (We should work out some small examplesâ€¦)
Iâƒ In the relu operation's definition, I'm similarly unsure about the treatment
   of â¸¨sË¡â¸© and â¸¨sÊ°â¸© in the definitions for â¸¨sË¡â€²â¸© and â¸¨sÊ°â€²â¸©.
Iâ†

Mâ… 
Xâ… âŸªÂ«Setup (syntactic categories, etc.)â€¦Â»âŸ« â©Š
Xâƒ
Xâƒ AËrclcl
   Aâ… n   â§¼âˆˆâ§½ â„•
   Aâƒ r   â§¼âˆˆâ§½ â„
   % Aâƒ rÌ‡   â§¼âˆˆâ§½ â„â¸¢âˆâ¸£  â§¼â‰œâ§½ â„ âŠ â´-âˆ,âˆâµ
   Aâƒ s   â§¼âˆˆâ§½ â„âº    â§¼â‰œâ§½ â´r Â¦ 0 â‰¤ râµ
   Aâƒ x   â§¼âˆˆâ§½ â€¹varâ€º
   Aâƒ e   â§¼âˆˆâ§½ â€¹expâ€º â§¼â©´â§½ r Â¦ x Â¦ e+e Â¦ eÃ—e Â¦ ã‘e Â¦ â¦‘reluâ¦’(e)
   Aâƒ Î³   â§¼âˆˆâ§½ â€¹envâ€º â§¼â‰œâ§½ â€¹varâ€º â†’ â„ Ã— â„
   Aâ†
Xâƒ
Xâƒ âŸªÂ«Bounded Global Sensitivity Algorithmâ€¦Â»âŸ« â©Š
Xâƒ
Xâƒ AËrcl
   Aâ… âŸ¦â€—âŸ§â¸¢â€—â¸£            â§¼âˆˆâ§½ â€¹expâ€º Ã— (â€¹varâ€º â†’ â„ Ã— â„) â†’ (â€¹varâ€º â‡€ â„âº Ã— (â„ Ã— â„))
   Aâƒ âŸ¦râŸ§â¸¢Î³â¸£(x)         â§¼â‰œâ§½ 0,(r,r)
   Aâƒ âŸ¦xâŸ§â¸¢Î³â¸£(y)         â§¼â‰œâ§½ â€˜â´ AËl@{â }c@{â }l
                               Aâ… 1,Î³(y) â§¼âŸªÂ«ifÂ»âŸ«â§½ x = y
                               Aâƒ 0,Î³(y) â§¼âŸªÂ«ifÂ»âŸ«â§½ x â‰  y
                               Aâ† â€™.
   Aâƒ âŸ¦eâ‚+eâ‚‚âŸ§â¸¢Î³â¸£(x)     â§¼â‰œâ§½ AË[t]l
                            Aâ… s,(rË¡,rÊ°)
                            Aâƒ â âŸªÂ«whereÂ»âŸ« â  AË[t]rcl
                                            Aâ… sâ‚,(rË¡â‚,rÊ°â‚) â§¼=â§½ âŸ¦eâ‚âŸ§â¸¢Î³â¸£(x)
                                            Aâƒ sâ‚‚,(rË¡â‚‚,rÊ°â‚‚) â§¼=â§½ âŸ¦eâ‚‚âŸ§â¸¢Î³â¸£(x)
                                            Aâƒ s            â§¼=â§½ sâ‚ + sâ‚‚
                                            Aâƒ rË¡           â§¼=â§½ rË¡â‚ + rË¡â‚‚
                                            Aâƒ rÊ°           â§¼=â§½ rÊ°â‚ + rÊ°â‚‚
                                            Aâ†
                            Aâ†
   Aâƒ âŸ¦eâ‚Ã—eâ‚‚âŸ§â¸¢Î³â¸£(x)     â§¼â‰œâ§½ AË[t]l
                            Aâ… s,(rË¡,rÊ°)
                            Aâƒ â âŸªÂ«whereÂ»âŸ« â  AË[t]rcl
                                            Aâ… sâ‚,(rË¡â‚,rÊ°â‚) â§¼=â§½ âŸ¦eâ‚âŸ§â¸¢Î³â¸£(x)
                                            Aâƒ sâ‚‚,(rË¡â‚‚,rÊ°â‚‚) â§¼=â§½ âŸ¦eâ‚‚âŸ§â¸¢Î³â¸£(x)
                                            Aâƒ s â§¼=â§½ sâ‚(|rË¡â‚‚| âŠ” |rÊ°â‚‚|) + sâ‚‚(|rË¡â‚| âŠ” |rÊ°â‚|)
                                            Aâƒ rË¡ â§¼=â§½ rË¡â‚rË¡â‚‚ âŠ“ rË¡â‚rÊ°â‚‚ âŠ“ rÊ°â‚rË¡â‚‚ âŠ“ rÊ°â‚rÊ°â‚‚
                                            Aâƒ rÊ° â§¼=â§½ rË¡â‚rË¡â‚‚ âŠ” rË¡â‚rÊ°â‚‚ âŠ” rÊ°â‚rË¡â‚‚ âŠ” rÊ°â‚rÊ°â‚‚
                                            Aâ†
                            Aâ†
   Aâƒ âŸ¦eâ¿âŸ§                â§¼â‰œâ§½ AË[t]l
                              Aâ… sâ€²,(rË¡â€²,rÊ°â€²)
                              Aâƒ â âŸªÂ«whereÂ»âŸ« â  AË[t]rcl
                                              Aâ… s,(rË¡,rÊ°) â§¼=â§½ âŸ¦eâŸ§â¸¢Î³â¸£(x)
                                              Aâƒ sâ€² â§¼=â§½ â€¦ nâŸ¨eâŸ©â¸¢n-1â¸£ s
                                              Aâƒ rË¡â€² â§¼=â§½ ???
                                              Aâƒ rÊ°â€² â§¼=â§½ ???
                                              Aâ†
                            Aâ†
   Aâƒ âŸ¦ã‘eâŸ§â¸¢Î³â¸£(x)         â§¼â‰œâ§½ AË[t]l
                              Aâ… sâ€²,(rË¡â€²,rÊ°â€²)
                              Aâƒ â âŸªÂ«whereÂ»âŸ« â  AË[t]rcl
                                              Aâ… s,(rË¡,rÊ°) â§¼=â§½ âŸ¦eâŸ§â¸¢Î³â¸£(x)
                                              Aâƒ sâ€² â§¼=â§½ \frac{s}{rË¡}
                                              Aâƒ rË¡â€² â§¼=â§½ ã‘rË¡
                                              Aâƒ rÊ°â€² â§¼=â§½ ã‘rÊ°
                                              Aâƒ 0   â§¼<â§½ rË¡ â‰¤ rÊ°
                                              Aâ†
                            Aâ†
   Aâƒ âŸ¦â¦‘reluâ¦’(e)âŸ§â¸¢Î³â¸£(x) â§¼â‰œâ§½ AË[t]l
                            Aâ… sâ€²,(rË¡â€²,rÊ°â€²)
                            Aâƒ â âŸªÂ«whereÂ»âŸ« â  AË[t]rcl
                                            Aâ… s,(rË¡,rÊ°) â§¼=â§½ âŸ¦eâŸ§â¸¢Î³â¸£(x)
                                            Aâƒ sâ€² â§¼=â§½ â€˜â´AËl@{â }l@{â }l
                                                        Aâ… 0    & Â«âŸªifâŸ«Â» & rË¡ < 0
                                                        Aâƒ s    & Â«âŸªifâŸ«Â» & rË¡ â‰¥ 0
                                                        Aâ†â€™.
                                            Aâƒ rË¡â€² â§¼=â§½ rË¡ âŠ” 0
                                            Aâƒ rÊ°â€² â§¼=â§½ rÊ° âŠ” 0
                                            Aâ†
                            Aâ†
   Aâ†
Xâ†
Mâ†

Mâ… -10 â‰¤ x â‰¤ 2
Mâƒ â€¹sensâ€º(xx) = 20
Mâƒ AËl
   Aâ… sâ‚ = 1
   Aâƒ sâ‚‚ = 1
   Aâƒ rË¡â‚ = -10
   Aâƒ rÊ°â‚ = 2
   Aâƒ rË¡â‚‚ = -10
   Aâƒ rÊ°â‚‚ = 2
   Aâ†
Mâ†

\section{Enforcing Smoothing}

The Â«immediate sensitivityÂ» is defined as:
Mâ… Â«ğˆğ’Â»â¸¤ğ±,ğ›‰â¸¥(f) AË[t]rcl
                  Aâ… â§¼â‰œâ§½ â€– âˆ‡â¸¤ğ±â¸¥ â€– âˆ‡â¸¤ğ›‰â¸¥ Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) â€–â‚‚ â€–â‚‚
                  Aâ†
Mâ†

\begin{itemize}
\item Minibatch is â¸¨\bar{x}, \bar{y}â¸© of length â¸¨nâ¸©, with input size â¸¨mâ¸©
\item Aggregate loss is â¸¨Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(\bar{x},\bar{y})â¸©
\item Aggregate gradient is â¸¨âˆ‡â¸¤Î¸â¸¥ Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(\bar{x},\bar{y})â¸©, a vector of length â¸¨|Î¸|â¸© (one element per model weight)
\item Per-feature immediate sensitivity is â¸¨âˆ‡â¸¤\bar{x}â¸¥ â€– âˆ‡â¸¤Î¸â¸¥ Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(\bar{x},\bar{y}) â€–â‚‚â¸©, a matrix of size â¸¨m Ã— nâ¸© (one sensitivity value per feature)
\item Per-example immediate sensitivity is the row-wise â¸¨Lâ‚‚â¸© norm of the above (how do you write this?), something like â¸¨ â€– âˆ‡â¸¤\bar{x}â¸¥ â€– âˆ‡â¸¤Î¸â¸¥ Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(\bar{x},\bar{y}) â€–â‚‚ â€–â‚‚ â¸©, a vector of length â¸¨nâ¸© (one sensitivity per example in the minibatch)
  \begin{itemize}
  \item Each per-example sensitivity describes the change in the aggregate gradient if you modify that example
  \item Worst-case local sensitivity is the max of these
  \end{itemize}
\end{itemize}

\paragraph{Enforcing smoothing.}
If we have an upper bound on â¸¨LSâ¸¤fâ¸¥(x)â¸© then we can \emph{construct} a
smooth upper bound. From~\cite{smooth-sensitivity}, â¸¨Sâ¸¢*â¸£â¸© is
â¸¨tâ¸©-smooth if we construct it like this:
%
Mâ… Sâ¸¢*â¸£â¸¤f,tâ¸¥ (x) = \maxâ¸¤y âˆˆ ğ’Ÿâ¿â¸¥ LSâ¸¤fâ¸¥(y) â‹… \exp(-t â£d(x, y))
Mâ†

\paragraph{Conjecture:} the total change in the gradient if we modify
â¸¨k â‰¤ nâ¸© examples in the minibatch is upper-bounded by the sum of their
individual local sensitivities. This is because the aggregate gradient
is the average of the individual gradients. In the worst case, if we
change â¸¨kâ¸© examples, all of the resulting changes to the gradient go
``in the same direction'' and change the average by their sum.


If the conjecture is true, then I think this is an upper bound on â¸¨Sâ¸¢*â¸£â¸©:

\begin{verbatim}
For all k âˆˆ 0 ... n:
  Take the largest k local sensitivities from the vector above
  Add them up to get an upper bound on LS_f(y)
  Multiply by exp(-tk)
Take the max of all of these
\end{verbatim}

Unfortunately this gives us an extra parameter to tune (â¸¨tâ¸©).

\paragraph{Claim:} If â¸¨Sâ¸© is a ML-â¸¨Î²â¸©-smooth upper bound on local
sensitivity, then we can bound the local sensitivity at distance â¸¨kâ¸©.
%
Mâ… d(x,y) â‰¤ 1 â‡’ S(x) + Î² â‰¥ S(y)
Mâƒ d(y,z) â‰¤ 1 â‡’ S(y) + Î² â‰¥ S(z)
Mâƒ d(x,z) â‰¤ 2 â‡’ S(x) + 2Î² â‰¥ S(z)â  âŸª(substituting)âŸ«
Mâ†
%
Apply this â¸¨kâ¸© times, you get the general result:
%
Mâ… d(x, y) â‰¤ k â‡’ S(x) + kÎ² â‰¥ S(y)
Mâ†

So we can define the DP-â¸¨tâ¸©-smooth upper bound (where â¸¨nâ¸© is minibatch size
and â¸¨Sâ¸© is ML-â¸¨Î²â¸©-smooth):
%
Mâ… Sâ¸¢*â¸£â¸¤f,tâ¸¥ (x) = \maxâ¸¤k âˆˆ â´0â€¦nâµâ¸¥ (S(x) + kÎ²) â‹… \exp(-tk)
Mâ†


\bibliographystyle{plainnat}
\bibliography{local}

\end{document}
\endinput
