{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn.datasets\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(7)\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True, context=\"talk\")\n",
    "\n",
    "%matplotlib inline\n",
    "print(torch.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "\n",
    "X, y = sklearn.datasets.make_classification(n_samples=1000,\n",
    "                                            n_features=10,\n",
    "                                            n_informative=5,\n",
    "                                            n_redundant=2,\n",
    "                                            n_repeated=0,\n",
    "                                            class_sep=0.5,\n",
    "                                            n_classes=n_classes,\n",
    "                                            random_state = 4)\n",
    "\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train: 800\n",
      "len test: 200\n"
     ]
    }
   ],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "print('len train:', len(X_train))\n",
    "print('len test:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "training_dataset = TensorDataset(torch.from_numpy(X_train).float(), \n",
    "                                 torch.from_numpy(y_train).long())\n",
    "train_loader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "testing_dataset = TensorDataset(torch.from_numpy(X_test).float(), \n",
    "                                torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(testing_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden=256):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_classes),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_immediate_sensitivity(model, criterion, inputs, labels, epoch):\n",
    "    inp = Variable(inputs, requires_grad=True)\n",
    "    \n",
    "    outputs = model.forward(inp)\n",
    "    loss = criterion(torch.squeeze(outputs), labels)\n",
    "    \n",
    "    # (1) first-order gradient (wrt parameters)\n",
    "    first_order_grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "    \n",
    "    # (2) L2 norm of the gradient from (1)\n",
    "    grad_l2_norm = torch.norm(torch.cat([x.view(-1) for x in first_order_grads]), p = 2) # CHANGE\n",
    "    \n",
    "    # (3) Gradient (wrt inputs) of the L2 norm of the gradient from (2)\n",
    "    sensitivity_vec = torch.autograd.grad(grad_l2_norm, inp, retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    # (4) L2 norm of (3) - \"immediate sensitivity\"\n",
    "    s_norm = sensitivity_vec.norm(dim=1, p=2) # CHANGE\n",
    "\n",
    "    immediate_sensitivity, idx = s_norm.max(0)\n",
    "    s = immediate_sensitivity.detach().numpy().item()\n",
    "    \n",
    "    # (5) gradient of (4) - this is \"beta\" for linear-like models\n",
    "    beta_vec = torch.autograd.grad(immediate_sensitivity, inp, retain_graph=True)[0]\n",
    "    #print(beta_vec)\n",
    "    beta_max = beta_vec[idx].norm(p=2) # CHANGE\n",
    "    #print(beta_max)\n",
    "    final_sens = immediate_sensitivity + beta_max\n",
    "    \n",
    "    '''\n",
    "    if epoch > 5:\n",
    "        print(f\"inputs: \",inp)\n",
    "        print(f\"outputs: \", outputs)\n",
    "        print(f\"loss: \", loss)\n",
    "        print(f\"first_order_grads: \", first_order_grads)\n",
    "        print(f\"grad_l2_norm:: \", grad_l2_norm)\n",
    "        print(f\"sensitivity_vec: \", sensitivity_vec)\n",
    "        print(f\"sensitivies: \", s)\n",
    "    '''\n",
    "\n",
    "    loss.backward()\n",
    "    return loss, final_sens.detach().numpy().item(), beta_max.numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    Xt = torch.from_numpy(X).float()\n",
    "    yt = torch.from_numpy(y).long()\n",
    "    outputs = model(Xt)\n",
    "    values, indices = outputs.max(dim=1)\n",
    "    y_hat = indices.detach().numpy()\n",
    "    accuracy = np.sum(y_hat == y) / len(y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps(epsilon, alpha, delta):\n",
    "    ed_eps = epsilon + np.log(1/delta)/(alpha - 1)\n",
    "    print(f'Total epsilon = {ed_eps}, delta = {delta}')\n",
    "    return ed_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(epsilon, epochs, add_noise=False):\n",
    "    # reset the model\n",
    "    model = Classifier(n_features=n_features)\n",
    "    model_criterion = nn.NLLLoss() \n",
    "    model_optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "    # parameters for Renyi differential privacy\n",
    "#     omega = 10\n",
    "#     rho_iter = rho / epochs\n",
    "#     delta = 1e-5\n",
    "#     eps = rho + 2*np.sqrt(rho * np.log(1/delta))\n",
    "#     print(f'Total epsilon = {eps}, delta = {delta}')\n",
    "\n",
    "    # parameters for tCDP differential privacy\n",
    "    alpha = 200\n",
    "    epsilon_iter = epsilon / epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch_train, y_batch_train in train_loader:\n",
    "            model_optimizer.zero_grad()\n",
    "            loss, batch_sensitivity, beta = grad_immediate_sensitivity(model, model_criterion, x_batch_train, y_batch_train,epoch)\n",
    "\n",
    "            imm_sens = batch_sensitivity# / BATCH_SIZE\n",
    "            # this is the scale of the Gaussian noise to be added to the batch gradient\n",
    "            \n",
    "            # For tCDP and smooth sensitivity\n",
    "            # big_x = beta**2 / (4 * (1-(omega * (1 - np.exp(-beta))))**2)\n",
    "            # sigma_sq = 2*(1-(omega * (1-np.exp(-beta)))) / (rho_iter - big_x)\n",
    "\n",
    "        \n",
    "            # For RDP and smooth sensitivity\n",
    "            # calculating renyi divergence directly\n",
    "            t = beta\n",
    "            gamma = alpha * (np.exp(t) - 1) + 1\n",
    "            sigma_sq = 2*gamma**2 * imm_sens**2 / (4* epsilon_iter * gamma**2 - alpha * t**2)\n",
    "            sigma = np.sqrt(sigma_sq)\n",
    "\n",
    "            #print('Smooth:', sigma)\n",
    "\n",
    "            # Assuming IS is a bound on GS (no smooth sensitivity)\n",
    "            sigma = np.sqrt(((batch_sensitivity/BATCH_SIZE)**2 * alpha) / (2 * epsilon_iter))\n",
    "\n",
    "            #print('Global:', sigma)\n",
    "\n",
    "            if add_noise:\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p.grad += (sigma * torch.randn(1).float())\n",
    "\n",
    "            model_optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnear/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = run_experiment(.001, 10, False)\n",
    "accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = run_experiment(1, 10, True)\n",
    "accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_experiment(epsilon):\n",
    "    model = run_experiment(epsilon, 10, True)\n",
    "    return accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments():\n",
    "    epsilons = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    runs = 10\n",
    "    alpha = 200\n",
    "    results = {}\n",
    "    \n",
    "    for eps in epsilons:\n",
    "        ed_eps = get_eps(eps, 200, 1e-5)\n",
    "        results[ed_eps] = [one_experiment(eps) for _ in range(runs)]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epsilon = 0.06785389680889561, delta = 1e-05\n",
      "Total epsilon = 0.1578538968088956, delta = 1e-05\n",
      "Total epsilon = 1.0578538968088955, delta = 1e-05\n",
      "Total epsilon = 10.057853896808895, delta = 1e-05\n",
      "Total epsilon = 100.0578538968089, delta = 1e-05\n"
     ]
    }
   ],
   "source": [
    "all_results = run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'ours'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours_epsilons = [0.06785389680889561, 0.1578538968088956, 1.0578538968088955, 10.057853896808895, 100.0578538968089]\n",
      "ours_means = [0.665, 0.8099999999999999, 0.8720000000000001, 0.9195, 0.9295]\n",
      "ours_stds = [0.1267675037223657, 0.12599603168354154, 0.055551777649324625, 0.013499999999999995, 0.005220153254455257]\n"
     ]
    }
   ],
   "source": [
    "print(f'{setting}_epsilons = {list(all_results.keys())}')\n",
    "print(f'{setting}_means = {[np.mean(vs) for vs in all_results.values()]}')\n",
    "print(f'{setting}_stds = {[np.std(vs) for vs in all_results.values()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
