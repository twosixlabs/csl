%%%%%%%%%
% FLAGS %
%%%%%%%%%

% Is this an internal version? (i.e., not public-facing.)
\ifdefined \isinternal \else \def \isinternal{1} \fi

% Is this an extended version? (e.g., includes appendix.)
\ifdefined \isextended \else \def \isextended{1} \fi

% Should authors be identified? (e.g., not submitted for blind peer review.)
\ifdefined \isauthorid \else \def \isauthorid{1} \fi

% Defaults can be overridden from the Makefile
\newif \ifinternal \if \isinternal 0 \internalfalse \else \internaltrue \fi
\newif \ifextended \if \isextended 0 \extendedfalse \else \extendedtrue \fi
\newif \ifauthorid \if \isauthorid 0 \authoridfalse \else \authoridtrue \fi

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\documentclass{article}

\usepackage[T5,T1]{fontenc}

\usepackage{longtable}

\input{darais-latex-imports}
\input{darais-latex-macros}

\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\begin{document}

\section{A Tiny Model (No Vectors)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector â¸¨ğ± â‰œ [xâ‚,xâ‚‚]â¸©, a weights vector â¸¨ğ›‰ â‰œ [Î¸â‚,Î¸â‚‚]â¸©, and the
classification function â¸¨fâ¸© is:
Mâ… fâ¸¤ğ›‰â¸¥(ğ±) â‰œ Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ Mâ†

Here is the mean squared error loss function over scalars:
Mâ… L(a,b) â‰œ (a - b)Â² Mâ†
and the loss of our function â¸¨fâ¸© {w.r.t.} a training example â¸¨yâ¸© is:
Mâ… Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) AË[t]rcl
                Aâ… â§¼â‰œâ§½ (fâ¸¤ğ›‰â¸¥(ğ±) - y)Â² 
                Aâƒ â§¼=â§½ (Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ - y)Â²
                Aâƒ â§¼=â§½ Î¸â‚Â²xâ‚Â² + Î¸â‚‚Â²xâ‚‚Â² + 2Î¸â‚Î¸â‚‚xâ‚xâ‚‚ - 2Î¸â‚xâ‚y - 2Î¸â‚‚xâ‚‚y + yÂ²
                Aâ†
Mâ†
Here is the gradient of â¸¨Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y)â¸© {w.r.t.} â¸¨ğ›‰â¸©:
Mâ… âˆ‡â¸¤ğ›‰â¸¥Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) AË[t]rcl
                    Aâ… â§¼â‰œâ§½ â€˜[ \frac{âˆ‚(Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y))}{âˆ‚Î¸â‚} , \frac{âˆ‚(Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y))}{âˆ‚Î¸â‚‚} â€™]
                    Aâƒ â§¼=â§½ â€˜[ 2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y , 2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y â€™]
                    Aâ†
Mâ†
The Â«immediate sensitivityÂ» is defined as:
Mâ… Â«ğˆğ’Â»â¸¤ğ±,ğ›‰â¸¥(f) AË[t]rcl
                  Aâ… â§¼â‰œâ§½ â€– âˆ‡â¸¤ğ±â¸¥ â€– âˆ‡â¸¤ğ›‰â¸¥ Lâ¸¤fâ¸¤ğ›‰â¸¥â¸¥(ğ±,y) â€–â‚‚ â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ğ±â¸¥ â€– â€˜[ 2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y , 2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y â€™] â€–â‚‚ â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ğ±â¸¥ âˆš{(2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y)Â² + (2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y)Â²} â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ğ±â¸¥ 2âˆš{(xâ‚Â² + xâ‚‚Â²)(Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ - y)Â²} â€–â‚‚
                  Aâ†
Mâ†
% We want to know â¸¨Î²â¸© such that the above quantity is â¸¨Î²â¸©-smooth.
% The derivative of the loss is:
% Mâ… \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
%                              Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€ - y)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)(\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚y}{âˆ‚Î¸áµ€})
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
%                              Aâ†
% Mâƒ
% Mâƒ \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
%                              Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â² - 2xyÎ¸áµ€ - yÂ²)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(2xyÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(yÂ²)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸}
%                              Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€ - 2xy
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
%                              Aâƒ â§¼=â§½ ... 2xáµ€(xÎ¸áµ€ - y)
%                              Aâ†
% Mâ†

\section{A Tiny Model (Vectorized)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector â¸¨xâ¸©, a weights vector â¸¨Î¸â¸©, and the
classification function â¸¨fâ¸© is:
Mâ… fâ¸¤Î¸â¸¥(x) â‰œ xÎ¸áµ€ Mâ†

Here is the mean squared error loss function over scalars:
Mâ… L(a,b) â‰œ (a - b)Â² Mâ†
and the loss of our function â¸¨fâ¸© {w.r.t.} a training example â¸¨yâ¸© is:
Mâ… Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y) AË[t]rcl
                Aâ… â§¼â‰œâ§½ (fâ¸¤Î¸â¸¥(x) - y)Â² 
                Aâƒ â§¼=â§½ (xÎ¸áµ€ - y)Â²
                Aâ†
Mâ†
The Â«immediate sensitivityÂ» is defined as:
Mâ… â€– âˆ‡â‚“ â€– âˆ‡â¸¤Î¸â¸¥ Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y) â€–â‚‚ â€–â‚‚
Mâ†
We want to know â¸¨Î²â¸© such that the above quantity is â¸¨Î²â¸©-smooth.
The derivative of the loss is:
Mâ… \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
                             Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€ - y)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)(\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚y}{âˆ‚Î¸áµ€})
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
                             Aâ†
Mâƒ
Mâƒ \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
                             Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â² - 2xyÎ¸áµ€ - yÂ²)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(2xyÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(yÂ²)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸}
                             Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€ - 2xy
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
                             Aâƒ â§¼=â§½ ... 2xáµ€(xÎ¸áµ€ - y)
                             Aâ†
Mâ†

\section{New}

% Need: IS is â¸¨Î²â¸©-smooth.

% Know: IS(x)
% Need to know: IS is â¸¨Î²â¸©-smooth
% Need to know: â¸¨ | IS(x) - IS(x') | â‰¤ ???â¸©

% If IS is â¸¨Î²â¸©-smooth then â¸¨| IS(x) - IS(x') | â‰¤ Î²â¸©

% We care about: local sensitivity of gradient G

% â€– G(x) - G(x') â€–â‚‚ â‰¤ IS(x) + Î²

% if G' is Î²-smooth then:

% â€–G'(x) - G'(x')â€– â‰¤ Î² â€–x - x'â€–        (defn of smoothness)
% â€–G'(x) - G'(x')â€– â‰¤ Î²                 (â€–x - x'â€– â‰¤ 1 by assumption)


% LSâ¸¤Gâ¸¥(x) = maxâ¸¤x' . d(x, x') â‰¤ 1â¸¥ â€– G(x) - G(x') â€–

% MVT says:
% âˆ€ x, x'. âˆƒ x''. x â‰¤ x'' â‰¤ x' âˆ§ â€–G(x) - G(x')â€– / â€–x - x'â€– = G'(x'')

%    â€–G(x) - G(x')â€– / â€–x-x'â€– = G'(x'') for some x''           (mean value theorem)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– = G'(x) + â€–G'(x) - G'(x'')â€–      (arithmetic)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– â‰¤ G'(x) + Î² â€–x - x''â€–            (def. of Î²-smoothness)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– â‰¤ G'(x) + Î²                      (def. absolute value)
% => â€–G(x) - G(x')â€–          â‰¤ G'(x) + Î²                      (inequality)
% => LSâ¸¤Gâ¸¥(x)                â‰¤ G'(x) + Î²                      (def. of LS)



% => G(x) - G(x') = G'(x) + â€–G'(x) - G'(x'')â€– 
% => G(x) - G(x') â‰¤ G'(x) + Î²                          (by above)
% => LSâ¸¤Gâ¸¥(x)     â‰¤ G'(x) + Î²                          (def of LS)



\section{Current Situation}

For all of the following, â¸¨â€–Xâ€–â‚â¸© is the L1 norm, â¸¨â€–Xâ€–â‚‚â¸© is the L2 norm, â¸¨â€–Xâ€–â¸¤âˆâ¸¥â¸©
is the Lâªâˆâ« norm, and â¸¨â€–Xâ€–â¸© (without a subscript) is some norm in a parametrized
metric space. Same goes for distances â¸¨â€–X-Yâ€–â‚â¸©, â¸¨â€–X-Yâ€–â‚‚â¸©, â¸¨â€–X-Yâ€–â¸¤âˆâ¸¥â¸© and
â¸¨â€–X-Yâ€–â¸©.

\subsection{From \cite{smooth-sensitivity}}

\begin{definition}[Global Sensitivity \citep{smooth-sensitivity}]
  The Global Sensitivity of â¸¨fâ¸© is â¸¨Â«GSÂ»â¸¤fâ¸¥â¸© where:
  Mâ… Xâ… â©Š Â«GSÂ»â¸¤fâ¸¥ â‰œ \max\limitsâ¸¤x,y:â€–x-yâ€–=1â¸¥ â€– f(x) - f(y) â€–â‚ 
        â©Š âŸª\citep[Â§ 1.2, Definition 1.3]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: â¸¨â€–x-yâ€–â¸© is an abstract distance but â¸¨â€– f(x) - f(y) â€–â‚â¸©  is
  specifically L1 distance.
\end{definition}

\begin{definition}[Local Sensitivity \citep{smooth-sensitivity}]
  The Local Sensitivity of â¸¨fâ¸© at â¸¨xâ¸© is â¸¨Â«LSÂ»â¸¤fâ¸¥(x)â¸© where:
  Mâ… Xâ… â©Š Â«LSÂ»â¸¤fâ¸¥(x) â‰œ \max\limitsâ¸¤y:â€–x-yâ€–=1â¸¥ â€– f(x) - f(y) â€–â‚
        â©Š âŸª\citep[Â§ 1.3, Definition 1.6]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: â¸¨â€–x-yâ€–â¸© is an abstract distance but â¸¨â€– f(x) - f(y) â€–â‚â¸©  is
  specifically L1 distance.
\end{definition}

\begin{definition}[DP-â¸¨Î²â¸©-Smoothness \citep{smooth-sensitivity}]\ \\
  A function â¸¨fâ¸© is DP-â¸¨Î²â¸©-Smooth if:
  Mâ… Xâ… â©Š Â«GSÂ»â¸¤\ln(f(â‹…))â¸¥ â‰¤ Î²
        â©Š âŸª\citep[Â§ 2.1]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Alternatively, â¸¨fâ¸© is DP-â¸¨Î²â¸©-smooth if:
  Mâ… Xâ… â©Š âˆ€ x,yâª â€–x-yâ€– = 1 âŸ¹ f(x) â‰¤ eâ¸¢Î²â¸£f(y)
        â©Š âŸª\citep[Â§ 2.1, Definition 2.1, (2)]{smooth-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: \cite{smooth-sensitivity} just calls this â€œâ¸¨Î²â¸©-smoothâ€.
  \subparagraph{Why are these the same?} 
  âŸª
  Mâ… AËlcl@{â }l
     Aâ… â§¼ â§½ Â«GSÂ»â¸¤\ln(f(â‹…))â¸¥ â‰¤ Î²
     Aâƒ â§¼âŸºâ§½ âˆ€ x,yâª â€–x-yâ€– = 1 â‡’ â€–\ln(f(x)) - \ln(f(y))â€–â‚ â‰¤ Î² & âŸ…definitionâŸ†
     Aâƒ â§¼âŸºâ§½ âˆ€ x,yâª â€–x-yâ€– = 1 â‡’ \ln(f(x)) â‰¤ Î² + \ln(f(y))    & âŸ…(somehow?)âŸ†
     Aâƒ â§¼âŸºâ§½ âˆ€ x,yâª â€–x-yâ€– = 1 â‡’ f(x) â‰¤ eâ¸¢Î²â¸£f(y)              & âŸ…algebraâŸ†
     Aâ†
  Mâ†
  âŸ«
\end{definition}

\begin{definition}[DP-â¸¨Î²â¸©-Smooth Sensitivity \citep{smooth-sensitivity}]\ \\
  A function â¸¨fâ¸© has DP-â¸¨Î²â¸©-Smooth sensitivity if its local sensitivity is
  â¸¨Î²â¸©-smooth:
  Mâ… Â«GSÂ»â¸¤\ln(Â«LSÂ»â¸¤fâ¸¥(â‹…))â¸¥ â‰¤ Î² Mâ†
  or:
  Mâ… âˆ€ x,yâª â€–x-yâ€– = 1 âŸ¹ Â«LSÂ»â¸¤fâ¸¥(x) â‰¤ eâ¸¢Î²â¸£Â«LSÂ»â¸¤fâ¸¥(y) Mâ†
  \subparagraph{What if â¸¨Â«LSÂ»â¸¤fâ¸¥â¸© isn't already smooth?}
  âŸª
  \cite{smooth-sensitivity} defines a construction of â¸¨Sâ¸¢*â¸£â¸¤f,Î²â¸¥â¸©
  which is the best upper bound on the local sensitivity of â¸¨fâ¸© which is also
  â¸¨Î²â¸©-smooth; this construction is:
  Mâ… Sâ¸¢*â¸£â¸¤f,Î²â¸¥(x) = \maxâ¸¤yâ¸¥Â«LSÂ»â¸¤fâ¸¥(y)eâ¸¢-Î²â€–x-yâ€–â¸£ Mâ†
  âŸ«
\end{definition}

\subsection{From \cite{metrics-local-sensitivity}}

\begin{definition}[Generalized Global Sensitivity \citep{metrics-local-sensitivity}]
  The Generalized Global Sensitivity of â¸¨fâ¸© is â¸¨Â«GGSÂ»â¸¤fâ¸¥â¸© where:
  Mâ… Xâ… â©Š Â«GGSÂ»â¸¤fâ¸¥ â‰œ \maxâ¸¤x,yâ¸¥ \frac{â€–f(x)-f(y)â€–}{â€–x-yâ€–}
        â©Š âŸª\citep[Â§ 3.2, Definition 2]{metrics-local-sensitivity}âŸ«
     Xâ†
  Mâ†
  Note: \cite{metrics-local-sensitivity} just calls this â€œGlobal Sensitivityâ€
\end{definition}

\begin{definition}[Derivative Sensitivity \citep{metrics-local-sensitivity}]
  The Derivative Sensitivity of â¸¨fâ¸© at â¸¨xâ¸© is â¸¨Â«DSÂ»â¸¤fâ¸¥(x)â¸© where:
  Mâ… Xâ… â©Š Â«DSÂ»â¸¤fâ¸¥(x) â‰œ â€–fâ€²(x)â€–
        â©Š âŸª\citep[Â§ 4.2, Definition 12]{metrics-local-sensitivity}âŸ«
     Xâ†
  Mâ†
\end{definition}

\paragraph{From ML literature}

\begin{definition}[ML-â¸¨Î²â¸©-Smoothness]
  A function â¸¨fâ¸© is ML-â¸¨Î²â¸©-Smooth if:
  Mâ… Xâ… â©Š â€–fâ€²(x) - fâ€²(y)â€– â‰¤ Î²â€–x-yâ€– 
        â©Š âŸª\citep[Â§ 3]{convex-optimization-ml-slides}âŸ«
     Xâ†
  Mâ†
  or:
  Mâ… Â«GGSÂ»â¸¤fâ€²â¸¥ â‰¤ Î² Mâ†
\end{definition}

\subsection{New Definitions}

\begin{definition}[Immediate Sensitivity]
  The Immediate Sensitivity of â¸¨fâ¸© at â¸¨xâ¸© is â¸¨Â«IMÂ»â¸¤fâ¸¥(x)â¸© where:
  Mâ… Â«ISÂ»â¸¤fâ¸¥(x) â‰œ â€–fâ€²(x)â€– Mâ†
  Jâ… Note: â¸¨Â«ISÂ»â¸¤fâ¸¥(x) = Â«DSÂ»â¸¤fâ¸¥(x)â¸©.
  Jâƒ Note: In our case studies we pick specifically the L2 norm, so in that
     setting, â¸¨Â«ISÂ»â¸¤fâ¸¥(x) â‰œ â€–fâ€²(x)â€–â‚‚â¸© and is an instantiation of â¸¨Â«DSÂ»â¸¤fâ¸¥(x)â¸©
     for a particular norm.
  Jâ†
\end{definition}

\subsection{Where we are}

We have observed that for some architectures, using the L1 norm, â¸¨dÂ«ISÂ»â¸¤fâ¸¥/dxâ¸©
is constant, i.e.,  â¸¨dÂ«ISÂ»â¸¤fâ¸¥/dx = Câ¸© for some scalar â¸¨Câ¸©. So â¸¨Â«ISÂ»â¸¤fâ¸¥â¸© is
ML-â¸¨Î²â¸©-smooth for â¸¨Î²=0â¸©, and â¸¨fâ¸© is ML-â¸¨Î²â¸©-smooth for â¸¨Î²=Câ¸©.

We are currently investigating the following questions:
Eâ… Given an ML-â¸¨Î²â¸©-smooth bound on â¸¨fâ¸©, can we construct some DP-â¸¨Î²â¸©-smooth bound
   on â¸¨fâ¸©?
Eâƒ Given an ML-â¸¨Î²â¸©-smooth bound on some altered â¸¨fâ¸© (e.g., â¸¨\ln(f)â¸©), can we
   construct some DP-â¸¨Î²â¸©-smooth bound on â¸¨fâ¸©?
Eâƒ Given that â¸¨dÂ«ISÂ»â¸¤fâ¸¥/dxâ¸© is constant (or even better; it's â¸¨0â¸©) for our â¸¨fâ¸©
   in question, can we directly say something about â¸¨Â«ISÂ»â¸¤fâ¸¥â¸© being DP-â¸¨Î²â¸©-smooth?
Eâ†

\bibliographystyle{plainnat}
\bibliography{local}

\end{document}
\endinput
