{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Load data files\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "url_x = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_x.npy'\n",
    "url_y = 'https://github.com/jnear/cs211-data-privacy/raw/master/slides/adult_processed_y.npy'\n",
    "\n",
    "with urllib.request.urlopen(url_x) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "X = np.load(f)\n",
    "\n",
    "with urllib.request.urlopen(url_y) as url:\n",
    "    f = io.BytesIO(url.read())\n",
    "y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return vec + np.random.normal(loc=0, scale=sigma, size=vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test set sizes: 36176 9044\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "print('Train and test set sizes:', len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipping and Gradient definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_clip_array(vs , b):\n",
    "    norms = np.linalg.norm(vs, ord = 2, axis = 1)\n",
    "    ratios = vs/norms[:, None]\n",
    "    results = np.where((norms > b)[:, None], b*ratios, vs)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgradient(theta_in, x_in, y_in, C):\n",
    "    x = x_in\n",
    "    y = y_in\n",
    "    theta = theta_in\n",
    "    exponent = y * np.dot(x, theta)\n",
    "    rhs = (y/(1+np.exp(exponent)))\n",
    "    gradients = -(x*rhs[:, None])\n",
    "    clipped_grads = L2_clip_array(gradients, C)\n",
    "    return np.sum(clipped_grads, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline (gradient clipping DP-SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_gradient_descent(epochs, rho):\n",
    "    rho_i = rho/epochs\n",
    "    theta = np.zeros(X_train.shape[1])  # leaks the number of features, without privacy\n",
    "    clipping_param = 1\n",
    "    num_examples = X_train.shape[0]     # leaks the number of training examples, without privacy\n",
    "\n",
    "    BATCH_SIZE = 256\n",
    "    num_batches = int(num_examples / BATCH_SIZE)\n",
    "    batches_X = np.array_split(X, num_batches)\n",
    "    batches_y = np.array_split(y, num_batches)\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        for xs, ys in zip(batches_X, batches_y):\n",
    "            grad_sum        = vgradient(theta, xs, ys, clipping_param)\n",
    "            noisy_grad_sum  = gaussian_mech_zCDP_vec(grad_sum, clipping_param, rho_i)\n",
    "            noisy_avg_grad  = noisy_grad_sum / BATCH_SIZE\n",
    "            theta           = theta - noisy_avg_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21559660262893474"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zcdp_eps(rho, delta):\n",
    "    return rho + 2*np.sqrt(rho * np.log(1/delta))\n",
    "zcdp_eps(0.001, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.21559660262893474\n",
      "mean: 0.7634453781512606\n",
      "std: 0.026823566224831792\n"
     ]
    }
   ],
   "source": [
    "rho = .001\n",
    "epochs = 5\n",
    "print('eps:', zcdp_eps(rho, 1e-5))\n",
    "accs = [accuracy(dp_gradient_descent(epochs, rho)) for _ in range(10)]\n",
    "print('mean:', np.mean(accs))\n",
    "print('std:', np.std(accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calvin's Sensitivity Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sens_torch(many_xs, t, m, a, b, device):\n",
    "    # Clamp and sort all xs's at once, assuming shape is (num_iter, n)\n",
    "    many_xs = many_xs.clamp(a, b).sort(dim=1).values\n",
    "    n = many_xs.size(1)\n",
    "    assert n - 2*m > 0, f'm = {m} and n = {n} means n - 2m = {n-2*m} is negative'\n",
    "\n",
    "    num_iter = many_xs.size(0)\n",
    "\n",
    "    # Concat [b, a] to the end of every xs so that indexing -1 gives a and indexing n gives b, then clamp indices to -1, n\n",
    "    many_xs = torch.cat((many_xs, torch.full((num_iter, 1), b, dtype=torch.float, device=device), \n",
    "                         torch.full((num_iter, 1), a, dtype=torch.float, device=device)), dim=1)\n",
    "\n",
    "    # Generate indices now so they don't need to be every time (will be xs[idx1] - xs[idx2]), this doesn't need to be efficient but w/e\n",
    "    ks = torch.arange(0, n+1, device=device) # distances\n",
    "    #ks = torch.arange(0, 5, device=device) # distances\n",
    "    ls = torch.arange(0, n+2, device=device)\n",
    "    # Use all l values then take lower triangular part of matrix plus (with diagonal shifted by one) to remove values where l > k+1\n",
    "    idx1 = torch.tril(n - m + 1 + ks.reshape(-1, 1) - ls, diagonal=1).clamp(-1, n)\n",
    "    idx2 = (m + 1 - ls).clamp(-1, n)\n",
    "\n",
    "    scalar = torch.exp(-1 * ks * t)\n",
    "\n",
    "    out = torch.empty(num_iter)\n",
    "    for i in range(num_iter):\n",
    "        xs = many_xs[i]\n",
    "\n",
    "        diffs = torch.tril(torch.abs(xs[idx1] - xs[idx2]), diagonal=1)\n",
    "        inner_max = diffs.max(dim=1).values\n",
    "        outer_max = (inner_max*scalar).max()\n",
    "        out[i] = outer_max / (n - 2*m)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth Sensitivity DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lln(sigma, size):\n",
    "    x = np.random.laplace(size=size)\n",
    "    y = np.random.normal(size=size)\n",
    "    return x * np.exp(sigma * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgradient_per_ex(theta, x, y):\n",
    "    exponent = y * np.dot(x, theta)\n",
    "    rhs = (y/(1+np.exp(exponent)))\n",
    "    gradients = -(x*rhs[:, None])\n",
    "    return gradients\n",
    "\n",
    "def smooth_dp_gradient_descent(epochs, rho):\n",
    "    rho_i = rho/epochs\n",
    "    theta = np.zeros(X_train.shape[1])  # leaks the number of features, without privacy\n",
    "    num_examples = X_train.shape[0]     # leaks the number of training examples, without privacy\n",
    "\n",
    "    upper = 1\n",
    "    lower = -1\n",
    "    m = 10\n",
    "    t = 1.0\n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    rho_weight = rho_i / X_train.shape[1]\n",
    "    print('target per-weight rho:', rho_weight)\n",
    "\n",
    "    sigma, s = optimize_sigma(rho_weight, t)    \n",
    "    print('sigma:', sigma, 's:', s)\n",
    "\n",
    "    num_batches = int(num_examples / BATCH_SIZE)\n",
    "    batches_X = np.array_split(X, num_batches)\n",
    "    batches_y = np.array_split(y, num_batches)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for xs, ys in zip(batches_X, batches_y):\n",
    "            gradients       = vgradient_per_ex(theta, xs, ys)\n",
    "            noisy_avg_grad  = trimmed_mean(gradients, t, m, lower, upper, sigma, s)\n",
    "            theta           = theta - (1/(i+1))*noisy_avg_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the trimmed mean\n",
    "def trimmed_mean(many_xs, t, m, a, b, sigma, s):\n",
    "    norms = np.linalg.norm(many_xs, ord=2, axis=1) + 1e-7\n",
    "    normalized_many_xs = many_xs / norms[:, np.newaxis]\n",
    "    \n",
    "    #print('std, non-normalized:', len(np.std(many_xs, axis=0)))\n",
    "    #print('std, normalized:', len(np.std(normalized_many_xs, axis=0)))\n",
    "    \n",
    "#     for s1, s2 in zip(np.std(many_xs, axis=0), np.std(normalized_many_xs, axis=0)):\n",
    "#         print(s1/ s2)\n",
    "  \n",
    "    normalized_many_xs = np.sign(many_xs)\n",
    "    values, counts = np.unique(normalized_many_xs, axis=0, return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    signs = normalized_many_xs[ind]\n",
    "    signs2 = np.sign(np.mean(many_xs, axis=0))\n",
    "    #print(signs == signs2)\n",
    "    #print(signs)\n",
    "    return signs2\n",
    "    1/0\n",
    "    for i in range(104):\n",
    "        print(np.sort(normalized_many_xs[:,i]))\n",
    "\n",
    "    clipped_xs = np.sort(normalized_many_xs.clip(a, b), axis=0)\n",
    "    n = clipped_xs.shape[0]\n",
    "    trimmed_xs = clipped_xs[m:n-m]\n",
    "    width = clipped_xs.shape[1]\n",
    "    \n",
    "    many_xs_torch = torch.from_numpy(clipped_xs)\n",
    "\n",
    "    sens = compute_sens_torch(many_xs_torch.T, t, m, a, b, 'cpu')\n",
    "    print('sens:', np.linalg.norm(sens, ord=2))\n",
    "    noise = torch.tensor(lln(sigma, width))\n",
    "    return np.mean(trimmed_xs, axis=0) + ((sens/s)*noise).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_exp(eps, t, sigma):\n",
    "    return 5 * (eps / t) * sigma**3 - 5 * sigma**2 - 1\n",
    "\n",
    "def optimize_sigma(target_rho, t):\n",
    "    target_eps = np.sqrt(2*target_rho)\n",
    "    sigma_lower = t / target_eps\n",
    "    sigma_upper = max(2*t / target_eps, 1/2)\n",
    "    \n",
    "    loss = opt_exp(target_eps, t, np.mean([sigma_lower, sigma_upper]))\n",
    "    while np.abs(loss) > 0.001:\n",
    "        #print('loss:', loss)\n",
    "        if loss < 0:\n",
    "            sigma_lower = np.mean([sigma_lower, sigma_upper])\n",
    "        else:\n",
    "            sigma_upper = np.mean([sigma_lower, sigma_upper])\n",
    "\n",
    "        loss = opt_exp(target_eps, t, np.mean([sigma_lower, sigma_upper]))\n",
    "\n",
    "    sigma = np.mean([sigma_lower, sigma_upper])\n",
    "    s = np.exp(-(3/2) * sigma**2) * (target_eps - (t / sigma))\n",
    "\n",
    "    return sigma, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target per-weight rho: 0.004807692307692308\n",
      "sigma: 10.217568047844624 s: 1.8329446102499663e-72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnear/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.5 s, sys: 1.32 s, total: 22.8 s\n",
      "Wall time: 5.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8285050862450243"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "accuracy(smooth_dp_gradient_descent(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21559660262893474"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zcdp_eps(0.001, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
