%%%%%%%%%
% FLAGS %
%%%%%%%%%

% Is this an internal version? (i.e., not public-facing.)
\ifdefined \isinternal \else \def \isinternal{1} \fi

% Is this an extended version? (e.g., includes appendix.)
\ifdefined \isextended \else \def \isextended{1} \fi

% Should authors be identified? (e.g., not submitted for blind peer review.)
\ifdefined \isauthorid \else \def \isauthorid{1} \fi

% Defaults can be overridden from the Makefile
\newif \ifinternal \if \isinternal 0 \internalfalse \else \internaltrue \fi
\newif \ifextended \if \isextended 0 \extendedfalse \else \extendedtrue \fi
\newif \ifauthorid \if \isauthorid 0 \authoridfalse \else \authoridtrue \fi

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\documentclass{article}

\usepackage[T5,T1]{fontenc}

\usepackage{longtable}

\input{darais-latex-imports}
\input{darais-latex-macros}

\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\begin{document}

\section{A Tiny Model (No Vectors)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector ⸨𝐱 ≜ [x₁,x₂]⸩, a weights vector ⸨𝛉 ≜ [θ₁,θ₂]⸩, and the
classification function ⸨f⸩ is:
M⁅ f⸤𝛉⸥(𝐱) ≜ θ₁x₁ + θ₂x₂ M⁆

Here is the mean squared error loss function over scalars:
M⁅ L(a,b) ≜ (a - b)² M⁆
and the loss of our function ⸨f⸩ {w.r.t.} a training example ⸨y⸩ is:
M⁅ L⸤f⸤𝛉⸥⸥(𝐱,y) Aː[t]rcl
                A⁅ ⧼≜⧽ (f⸤𝛉⸥(𝐱) - y)² 
                A⁃ ⧼=⧽ (θ₁x₁ + θ₂x₂ - y)²
                A⁃ ⧼=⧽ θ₁²x₁² + θ₂²x₂² + 2θ₁θ₂x₁x₂ - 2θ₁x₁y - 2θ₂x₂y + y²
                A⁆
M⁆
Here is the gradient of ⸨L⸤f⸤𝛉⸥⸥(𝐱,y)⸩ {w.r.t.} ⸨𝛉⸩:
M⁅ ∇⸤𝛉⸥L⸤f⸤𝛉⸥⸥(𝐱,y) Aː[t]rcl
                    A⁅ ⧼≜⧽ ‘[ \frac{∂(L⸤f⸤𝛉⸥⸥(𝐱,y))}{∂θ₁} , \frac{∂(L⸤f⸤𝛉⸥⸥(𝐱,y))}{∂θ₂} ’]
                    A⁃ ⧼=⧽ ‘[ 2θ₁x₁²+ 2θ₂x₁x₂ - 2x₁y , 2θ₂x₂²+ 2θ₁x₁x₂ - 2x₂y ’]
                    A⁆
M⁆
The «immediate sensitivity» is defined as:
M⁅ «𝐈𝐒»⸤𝐱,𝛉⸥(f) Aː[t]rcl
                  A⁅ ⧼≜⧽ ‖ ∇⸤𝐱⸥ ‖ ∇⸤𝛉⸥ L⸤f⸤𝛉⸥⸥(𝐱,y) ‖₂ ‖₂
                  A⁃ ⧼=⧽ ‖ ∇⸤𝐱⸥ ‖ ‘[ 2θ₁x₁²+ 2θ₂x₁x₂ - 2x₁y , 2θ₂x₂²+ 2θ₁x₁x₂ - 2x₂y ’] ‖₂ ‖₂
                  A⁃ ⧼=⧽ ‖ ∇⸤𝐱⸥ √{(2θ₁x₁²+ 2θ₂x₁x₂ - 2x₁y)² + (2θ₂x₂²+ 2θ₁x₁x₂ - 2x₂y)²} ‖₂
                  A⁃ ⧼=⧽ ‖ ∇⸤𝐱⸥ 2√{(x₁² + x₂²)(θ₁x₁ + θ₂x₂ - y)²} ‖₂
                  A⁆
M⁆
% We want to know ⸨β⸩ such that the above quantity is ⸨β⸩-smooth.
% The derivative of the loss is:
% M⁅ \frac{∂L⸤f⸤θ⸥⸥(x,y)}{∂θᵀ} Aː[t]rcl
%                              A⁅ ⧼≜⧽ \frac{∂(xθᵀ - y)²}{∂θᵀ}
%                              A⁃ ⧼=⧽ 2(xθᵀ - y)\frac{∂(xθᵀ - y)}{∂θᵀ}
%                              A⁃ ⧼=⧽ 2(xθᵀ - y)(\frac{∂(xθᵀ)}{∂θᵀ} - \frac{∂y}{∂θᵀ})
%                              A⁃ ⧼=⧽ 2(xθᵀ - y)\frac{∂(xθᵀ)}{∂θᵀ}
%                              A⁃ ⧼=⧽ 2x(xθᵀ - y)\frac{∂θᵀ}{∂θᵀ}
%                              A⁃ ⧼=⧽ 2x(xθᵀ - y)
%                              A⁆
% M⁃
% M⁃ \frac{∂L⸤f⸤θ⸥⸥(x,y)}{∂θᵀ} Aː[t]rcl
%                              A⁅ ⧼≜⧽ \frac{∂(xθᵀ - y)²}{∂θᵀ}
%                              A⁃ ⧼=⧽ \frac{∂((xθᵀ)² - 2xyθᵀ - y²)}{∂θᵀ}
%                              A⁃ ⧼=⧽ \frac{∂((xθᵀ)²)}{∂θᵀ} - \frac{∂(2xyθᵀ)}{∂θᵀ} - \frac{∂(y²)}{∂θᵀ}
%                              A⁃ ⧼=⧽ \frac{∂((xθᵀ)²)}{∂θᵀ} - 2xy\frac{∂θᵀ}{∂θ}
%                              A⁃ ⧼=⧽ 2x²θᵀ\frac{∂θᵀ}{∂θᵀ} - 2xy\frac{∂θᵀ}{∂θᵀ}
%                              A⁃ ⧼=⧽ 2x²θᵀ - 2xy
%                              A⁃ ⧼=⧽ 2x(xθᵀ - y)
%                              A⁃ ⧼=⧽ ... 2xᵀ(xθᵀ - y)
%                              A⁆
% M⁆

\section{A Tiny Model (Vectorized)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector ⸨x⸩, a weights vector ⸨θ⸩, and the
classification function ⸨f⸩ is:
M⁅ f⸤θ⸥(x) ≜ xθᵀ M⁆

Here is the mean squared error loss function over scalars:
M⁅ L(a,b) ≜ (a - b)² M⁆
and the loss of our function ⸨f⸩ {w.r.t.} a training example ⸨y⸩ is:
M⁅ L⸤f⸤θ⸥⸥(x,y) Aː[t]rcl
                A⁅ ⧼≜⧽ (f⸤θ⸥(x) - y)² 
                A⁃ ⧼=⧽ (xθᵀ - y)²
                A⁆
M⁆
The «immediate sensitivity» is defined as:
M⁅ ‖ ∇ₓ ‖ ∇⸤θ⸥ L⸤f⸤θ⸥⸥(x,y) ‖₂ ‖₂
M⁆
We want to know ⸨β⸩ such that the above quantity is ⸨β⸩-smooth.
The derivative of the loss is:
M⁅ \frac{∂L⸤f⸤θ⸥⸥(x,y)}{∂θᵀ} Aː[t]rcl
                             A⁅ ⧼≜⧽ \frac{∂(xθᵀ - y)²}{∂θᵀ}
                             A⁃ ⧼=⧽ 2(xθᵀ - y)\frac{∂(xθᵀ - y)}{∂θᵀ}
                             A⁃ ⧼=⧽ 2(xθᵀ - y)(\frac{∂(xθᵀ)}{∂θᵀ} - \frac{∂y}{∂θᵀ})
                             A⁃ ⧼=⧽ 2(xθᵀ - y)\frac{∂(xθᵀ)}{∂θᵀ}
                             A⁃ ⧼=⧽ 2x(xθᵀ - y)\frac{∂θᵀ}{∂θᵀ}
                             A⁃ ⧼=⧽ 2x(xθᵀ - y)
                             A⁆
M⁃
M⁃ \frac{∂L⸤f⸤θ⸥⸥(x,y)}{∂θᵀ} Aː[t]rcl
                             A⁅ ⧼≜⧽ \frac{∂(xθᵀ - y)²}{∂θᵀ}
                             A⁃ ⧼=⧽ \frac{∂((xθᵀ)² - 2xyθᵀ - y²)}{∂θᵀ}
                             A⁃ ⧼=⧽ \frac{∂((xθᵀ)²)}{∂θᵀ} - \frac{∂(2xyθᵀ)}{∂θᵀ} - \frac{∂(y²)}{∂θᵀ}
                             A⁃ ⧼=⧽ \frac{∂((xθᵀ)²)}{∂θᵀ} - 2xy\frac{∂θᵀ}{∂θ}
                             A⁃ ⧼=⧽ 2x²θᵀ\frac{∂θᵀ}{∂θᵀ} - 2xy\frac{∂θᵀ}{∂θᵀ}
                             A⁃ ⧼=⧽ 2x²θᵀ - 2xy
                             A⁃ ⧼=⧽ 2x(xθᵀ - y)
                             A⁃ ⧼=⧽ ... 2xᵀ(xθᵀ - y)
                             A⁆
M⁆

\section{New}

% Need: IS is ⸨β⸩-smooth.

% Know: IS(x)
% Need to know: IS is ⸨β⸩-smooth
% Need to know: ⸨ | IS(x) - IS(x') | ≤ ???⸩

% If IS is ⸨β⸩-smooth then ⸨| IS(x) - IS(x') | ≤ β⸩

% We care about: local sensitivity of gradient G

% ‖ G(x) - G(x') ‖₂ ≤ IS(x) + β

% if G' is β-smooth then:

% ‖G'(x) - G'(x')‖ ≤ β ‖x - x'‖        (defn of smoothness)
% ‖G'(x) - G'(x')‖ ≤ β                 (‖x - x'‖ ≤ 1 by assumption)


% LS⸤G⸥(x) = max⸤x' . d(x, x') ≤ 1⸥ ‖ G(x) - G(x') ‖

% MVT says:
% ∀ x, x'. ∃ x''. x ≤ x'' ≤ x' ∧ ‖G(x) - G(x')‖ / ‖x - x'‖ = G'(x'')

%    ‖G(x) - G(x')‖ / ‖x-x'‖ = G'(x'') for some x''           (mean value theorem)
% => ‖G(x) - G(x')‖ / ‖x-x'‖ = G'(x) + ‖G'(x) - G'(x'')‖      (arithmetic)
% => ‖G(x) - G(x')‖ / ‖x-x'‖ ≤ G'(x) + β ‖x - x''‖            (def. of β-smoothness)
% => ‖G(x) - G(x')‖ / ‖x-x'‖ ≤ G'(x) + β                      (def. absolute value)
% => ‖G(x) - G(x')‖          ≤ G'(x) + β                      (inequality)
% => LS⸤G⸥(x)                ≤ G'(x) + β                      (def. of LS)



% => G(x) - G(x') = G'(x) + ‖G'(x) - G'(x'')‖ 
% => G(x) - G(x') ≤ G'(x) + β                          (by above)
% => LS⸤G⸥(x)     ≤ G'(x) + β                          (def of LS)






\end{document}
\endinput
