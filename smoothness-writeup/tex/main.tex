%%%%%%%%%
% FLAGS %
%%%%%%%%%

% Is this an internal version? (i.e., not public-facing.)
\ifdefined \isinternal \else \def \isinternal{1} \fi

% Is this an extended version? (e.g., includes appendix.)
\ifdefined \isextended \else \def \isextended{1} \fi

% Should authors be identified? (e.g., not submitted for blind peer review.)
\ifdefined \isauthorid \else \def \isauthorid{1} \fi

% Defaults can be overridden from the Makefile
\newif \ifinternal \if \isinternal 0 \internalfalse \else \internaltrue \fi
\newif \ifextended \if \isextended 0 \extendedfalse \else \extendedtrue \fi
\newif \ifauthorid \if \isauthorid 0 \authoridfalse \else \authoridtrue \fi

%%%%%%%%%%%%
% DOCUMENT %
%%%%%%%%%%%%

\documentclass{article}

\usepackage[T5,T1]{fontenc}

\usepackage{longtable}

\input{darais-latex-imports}
\input{darais-latex-macros}

\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\begin{document}

\section{A Tiny Model (No Vectors)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector â¸¨ð± â‰œ [xâ‚,xâ‚‚]â¸©, a weights vector â¸¨ð›‰ â‰œ [Î¸â‚,Î¸â‚‚]â¸©, and the
classification function â¸¨fâ¸© is:
Mâ… fâ¸¤ð›‰â¸¥(ð±) â‰œ Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ Mâ†

Here is the mean squared error loss function over scalars:
Mâ… L(a,b) â‰œ (a - b)Â² Mâ†
and the loss of our function â¸¨fâ¸© {w.r.t.} a training example â¸¨yâ¸© is:
Mâ… Lâ¸¤fâ¸¤ð›‰â¸¥â¸¥(ð±,y) AË[t]rcl
                Aâ… â§¼â‰œâ§½ (fâ¸¤ð›‰â¸¥(ð±) - y)Â² 
                Aâƒ â§¼=â§½ (Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ - y)Â²
                Aâƒ â§¼=â§½ Î¸â‚Â²xâ‚Â² + Î¸â‚‚Â²xâ‚‚Â² + 2Î¸â‚Î¸â‚‚xâ‚xâ‚‚ - 2Î¸â‚xâ‚y - 2Î¸â‚‚xâ‚‚y + yÂ²
                Aâ†
Mâ†
Here is the gradient of â¸¨Lâ¸¤fâ¸¤ð›‰â¸¥â¸¥(ð±,y)â¸© {w.r.t.} â¸¨ð›‰â¸©:
Mâ… âˆ‡â¸¤ð›‰â¸¥Lâ¸¤fâ¸¤ð›‰â¸¥â¸¥(ð±,y) AË[t]rcl
                    Aâ… â§¼â‰œâ§½ â€˜[ \frac{âˆ‚(Lâ¸¤fâ¸¤ð›‰â¸¥â¸¥(ð±,y))}{âˆ‚Î¸â‚} , \frac{âˆ‚(Lâ¸¤fâ¸¤ð›‰â¸¥â¸¥(ð±,y))}{âˆ‚Î¸â‚‚} â€™]
                    Aâƒ â§¼=â§½ â€˜[ 2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y , 2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y â€™]
                    Aâ†
Mâ†
The Â«immediate sensitivityÂ» is defined as:
Mâ… Â«ðˆð’Â»â¸¤ð±,ð›‰â¸¥(f) AË[t]rcl
                  Aâ… â§¼â‰œâ§½ â€– âˆ‡â¸¤ð±â¸¥ â€– âˆ‡â¸¤ð›‰â¸¥ Lâ¸¤fâ¸¤ð›‰â¸¥â¸¥(ð±,y) â€–â‚‚ â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ð±â¸¥ â€– â€˜[ 2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y , 2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y â€™] â€–â‚‚ â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ð±â¸¥ âˆš{(2Î¸â‚xâ‚Â²+ 2Î¸â‚‚xâ‚xâ‚‚ - 2xâ‚y)Â² + (2Î¸â‚‚xâ‚‚Â²+ 2Î¸â‚xâ‚xâ‚‚ - 2xâ‚‚y)Â²} â€–â‚‚
                  Aâƒ â§¼=â§½ â€– âˆ‡â¸¤ð±â¸¥ 2âˆš{(xâ‚Â² + xâ‚‚Â²)(Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ - y)Â²} â€–â‚‚
                  Aâ†
Mâ†
% We want to know â¸¨Î²â¸© such that the above quantity is â¸¨Î²â¸©-smooth.
% The derivative of the loss is:
% Mâ… \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
%                              Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€ - y)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)(\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚y}{âˆ‚Î¸áµ€})
%                              Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
%                              Aâ†
% Mâƒ
% Mâƒ \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
%                              Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â² - 2xyÎ¸áµ€ - yÂ²)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(2xyÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(yÂ²)}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸}
%                              Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
%                              Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€ - 2xy
%                              Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
%                              Aâƒ â§¼=â§½ ... 2xáµ€(xÎ¸áµ€ - y)
%                              Aâ†
% Mâ†

\section{A Tiny Model (Vectorized)}

A single layer neural net implements essentially just a single multiplication.

We have an input vector â¸¨xâ¸©, a weights vector â¸¨Î¸â¸©, and the
classification function â¸¨fâ¸© is:
Mâ… fâ¸¤Î¸â¸¥(x) â‰œ xÎ¸áµ€ Mâ†

Here is the mean squared error loss function over scalars:
Mâ… L(a,b) â‰œ (a - b)Â² Mâ†
and the loss of our function â¸¨fâ¸© {w.r.t.} a training example â¸¨yâ¸© is:
Mâ… Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y) AË[t]rcl
                Aâ… â§¼â‰œâ§½ (fâ¸¤Î¸â¸¥(x) - y)Â² 
                Aâƒ â§¼=â§½ (xÎ¸áµ€ - y)Â²
                Aâ†
Mâ†
The Â«immediate sensitivityÂ» is defined as:
Mâ… â€– âˆ‡â‚“ â€– âˆ‡â¸¤Î¸â¸¥ Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y) â€–â‚‚ â€–â‚‚
Mâ†
We want to know â¸¨Î²â¸© such that the above quantity is â¸¨Î²â¸©-smooth.
The derivative of the loss is:
Mâ… \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
                             Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€ - y)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)(\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚y}{âˆ‚Î¸áµ€})
                             Aâƒ â§¼=â§½ 2(xÎ¸áµ€ - y)\frac{âˆ‚(xÎ¸áµ€)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
                             Aâ†
Mâƒ
Mâƒ \frac{âˆ‚Lâ¸¤fâ¸¤Î¸â¸¥â¸¥(x,y)}{âˆ‚Î¸áµ€} AË[t]rcl
                             Aâ… â§¼â‰œâ§½ \frac{âˆ‚(xÎ¸áµ€ - y)Â²}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â² - 2xyÎ¸áµ€ - yÂ²)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(2xyÎ¸áµ€)}{âˆ‚Î¸áµ€} - \frac{âˆ‚(yÂ²)}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ \frac{âˆ‚((xÎ¸áµ€)Â²)}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸}
                             Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€} - 2xy\frac{âˆ‚Î¸áµ€}{âˆ‚Î¸áµ€}
                             Aâƒ â§¼=â§½ 2xÂ²Î¸áµ€ - 2xy
                             Aâƒ â§¼=â§½ 2x(xÎ¸áµ€ - y)
                             Aâƒ â§¼=â§½ ... 2xáµ€(xÎ¸áµ€ - y)
                             Aâ†
Mâ†

\section{New}

% Need: IS is â¸¨Î²â¸©-smooth.

% Know: IS(x)
% Need to know: IS is â¸¨Î²â¸©-smooth
% Need to know: â¸¨ | IS(x) - IS(x') | â‰¤ ???â¸©

% If IS is â¸¨Î²â¸©-smooth then â¸¨| IS(x) - IS(x') | â‰¤ Î²â¸©

% We care about: local sensitivity of gradient G

% â€– G(x) - G(x') â€–â‚‚ â‰¤ IS(x) + Î²

% if G' is Î²-smooth then:

% â€–G'(x) - G'(x')â€– â‰¤ Î² â€–x - x'â€–        (defn of smoothness)
% â€–G'(x) - G'(x')â€– â‰¤ Î²                 (â€–x - x'â€– â‰¤ 1 by assumption)


% LSâ¸¤Gâ¸¥(x) = maxâ¸¤x' . d(x, x') â‰¤ 1â¸¥ â€– G(x) - G(x') â€–

% MVT says:
% âˆ€ x, x'. âˆƒ x''. x â‰¤ x'' â‰¤ x' âˆ§ â€–G(x) - G(x')â€– / â€–x - x'â€– = G'(x'')

%    â€–G(x) - G(x')â€– / â€–x-x'â€– = G'(x'') for some x''           (mean value theorem)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– = G'(x) + â€–G'(x) - G'(x'')â€–      (arithmetic)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– â‰¤ G'(x) + Î² â€–x - x''â€–            (def. of Î²-smoothness)
% => â€–G(x) - G(x')â€– / â€–x-x'â€– â‰¤ G'(x) + Î²                      (def. absolute value)
% => â€–G(x) - G(x')â€–          â‰¤ G'(x) + Î²                      (inequality)
% => LSâ¸¤Gâ¸¥(x)                â‰¤ G'(x) + Î²                      (def. of LS)



% => G(x) - G(x') = G'(x) + â€–G'(x) - G'(x'')â€– 
% => G(x) - G(x') â‰¤ G'(x) + Î²                          (by above)
% => LSâ¸¤Gâ¸¥(x)     â‰¤ G'(x) + Î²                          (def of LS)






\end{document}
\endinput
